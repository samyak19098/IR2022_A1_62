{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# from substitutions import appos\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import string\n",
    "import numpy as np\n",
    "import contractions\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../Humor,Hist,Media,Food'\n",
    "file_names = os.listdir(data_dir)\n",
    "file_paths = [(data_dir + '/' + fname) for fname in file_names]\n",
    "docID_to_doc_mapping = {}\n",
    "for i in range(len(file_names)):\n",
    "    docID_to_doc_mapping[i] = file_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_files(fpaths):\n",
    "    '''\n",
    "        Reads the files and pre process every file's text to form word tokens for every file.\n",
    "        Returns a 2-D list containing word tokens for every file\n",
    "    '''\n",
    "    file_tokens = []\n",
    "    for fpath in fpaths:\n",
    "        f = open(fpath, 'r', encoding='utf-8', errors='replace') #open the file\n",
    "        ftxt_unprocessed = f.read() #read the text of the file\n",
    "        ftoks = preprocess_file(ftxt_unprocessed) #preprocessing the text to form word tokens\n",
    "        file_tokens.append(ftoks)\n",
    "    return file_tokens\n",
    "\n",
    "def check_alnum(tok):\n",
    "    '''\n",
    "        Remove non-alphanumeric characters from a string\n",
    "    '''\n",
    "    tok = ''.join(x for x in tok if x.isalnum() == True)\n",
    "    return tok\n",
    "def remove_punct(tok):\n",
    "    '''\n",
    "        Remove the punctuation in token\n",
    "    '''\n",
    "    punctuations = string.punctuation\n",
    "    tok = ''.join(x for x in tok if x not in punctuations)\n",
    "    return tok\n",
    "\n",
    "def remove_space(tok):\n",
    "    '''\n",
    "        Remove the spaces in token\n",
    "    '''\n",
    "    tok = ''.join(x for x in tok if x != ' ')\n",
    "    return tok\n",
    "\n",
    "def preprocess_file(file_text):\n",
    "    '''\n",
    "        Preprocess the file text and converting to word tokens\n",
    "        Input: string File text\n",
    "        Returns file_tokens, word tokens for the pre processed text\n",
    "    '''\n",
    "    #converting text to lowercase\n",
    "    text = file_text.lower()\n",
    "    \n",
    "    # print(f\"Original tokens : \\n{word_tokenize(text)}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "\n",
    "    #Fixing the contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    #omitting all non-alphanumeric characters\n",
    "    # text = ''.join([x if (x.isalnum() == True or x == \" \") else \" \" for x in text ])\n",
    "\n",
    "    #Performing word tokenization\n",
    "    all_toks = word_tokenize(text)\n",
    "    all_unique_toks = set(all_toks)\n",
    "\n",
    "    #Omitting all the non-alphanumeric characters in tokens\n",
    "    all_unique_toks = [check_alnum(x) for x in all_unique_toks]\n",
    "\n",
    "    # print(f\"All unique tokens :\\n{all_unique_toks}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "\n",
    "    #removing stopwords\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    file_toks = [tok for tok in all_unique_toks if tok not in stop_words]\n",
    "    # print(f\"After removing stopwords :\\n{file_toks}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "\n",
    "    #removing punctations if any remain\n",
    "    toks_no_punct = []\n",
    "    for tok in file_toks:\n",
    "        ctok = remove_punct(tok)\n",
    "        # print(f\"-> before removal : {tok} || After removal : {ctok}\")\n",
    "        if(ctok != \"\"):\n",
    "            toks_no_punct.append(ctok)\n",
    "    # print(f\"After removing puncts :\\n{toks_no_punct}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "\n",
    "    #removing spaces in any remain & check if it is a number\n",
    "    cleaned_toks = []\n",
    "    for tok in toks_no_punct:\n",
    "        ctok = remove_space(tok)\n",
    "        # print(f\"-> before removal : {tok} || After removal : {ctok}\")\n",
    "        if(ctok != \"\"):\n",
    "            cleaned_toks.append(ctok)\n",
    "    cleaned_toks = list(set(cleaned_toks))\n",
    "    # print(f\"After removing spaces and removing numbers :\\n{cleaned_toks}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    #stemming tokens\n",
    "    # stemmer = PorterStemmer()\n",
    "    # stemmed_toks = set()\n",
    "\n",
    "    # for tok in cleaned_toks:\n",
    "    #     stok = stemmer.stem(tok)\n",
    "    #     if(stok != \"\" or (stok not in stop_words)): #consider non-blank tokens and the ones which have length greater than 1\n",
    "    #         stemmed_toks.add(stok)\n",
    "    # print(f\"After stemming and removing single letters :\\n{stemmed_toks}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    final_tokens = [tok for tok in cleaned_toks]\n",
    "\n",
    "    return final_tokens\n",
    "\n",
    "def preprocess_query(query_text):\n",
    "    '''\n",
    "        Performs the pre processing of the query text\n",
    "        Input: A string representing query text\n",
    "        Output: Query word tokens\n",
    "    '''\n",
    "    text = query_text.lower()\n",
    "    \n",
    "    # print(f\"Original Query Tokens : \\n{word_tokenize(text)}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    \n",
    "    #Fixing the contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    #omitting all non-alphanumeric characters\n",
    "    # text = ''.join([x if (x.isalnum() == True or x == \" \") else \" \" for x in text ])\n",
    "\n",
    "    #word tokenization\n",
    "    all_toks = word_tokenize(text)\n",
    "    all_unique_toks = []\n",
    "    for tok in all_toks:\n",
    "        if(tok not in all_unique_toks):\n",
    "            all_unique_toks.append(tok)\n",
    "\n",
    "    all_unique_toks = [check_alnum(x) for x in all_unique_toks]\n",
    "\n",
    "    # print(f\"All unique tokens :\\n{all_unique_toks}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    #removing stopwords\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    file_toks = [tok for tok in all_unique_toks if tok not in stop_words]\n",
    "    # print(f\"After removing stopwords :\\n{file_toks}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    #removing punctations if any remain\n",
    "    toks_no_punct = []\n",
    "    for tok in file_toks:\n",
    "        ctok = remove_punct(tok)\n",
    "        # print(f\"-> before removal : {tok} || After removal : {ctok}\")\n",
    "        if(ctok != \"\"):\n",
    "            toks_no_punct.append(ctok)\n",
    "    # print(f\"After removing puncts :\\n{toks_no_punct}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "\n",
    "    #removing spaces in any remain & check if it is a number\n",
    "    cleaned_toks = []\n",
    "    for tok in toks_no_punct:\n",
    "        ctok = remove_space(tok)\n",
    "        # print(f\"-> before removal : {tok} || After removal : {ctok}\")\n",
    "        if(ctok != \"\"):\n",
    "            cleaned_toks.append(ctok)\n",
    "    \n",
    "    final_tokens = []\n",
    "    for tok in cleaned_toks:\n",
    "        if(tok not in final_tokens):\n",
    "            final_tokens.append(tok)\n",
    "    # cleaned_toks = list(set(cleaned_toks))\n",
    "    # print(f\"After removing spaces and removing numbers :\\n{cleaned_toks}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    #stemming tokens\n",
    "    # stemmer = PorterStemmer()\n",
    "    # stemmed_toks = set()\n",
    "\n",
    "    # for tok in cleaned_toks:\n",
    "    #     stok = stemmer.stem(tok)\n",
    "    #     if(stok != \"\" and len(stok) > 1 and (stok not in stop_words)): #consider non-blank tokens and the ones which have length greater than 1\n",
    "    #         stemmed_toks.add(stok)\n",
    "    # print(f\"After stemming and removing single letters :\\n{stemmed_toks}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    # final_tokens = [tok for tok in cleaned_toks]\n",
    "\n",
    "    return final_tokens\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the files to get file tokens for each and every file\n",
    "file_toks = read_files(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index(file_toks):\n",
    "    ''' \n",
    "        Create the inverted index based on the file tokens\n",
    "        Input: A 2 dimensional list containg word tokens for every file\n",
    "    '''\n",
    "    inv_index_postings = {}\n",
    "    inv_index_frequency = {}\n",
    "    for i in range(len(file_toks)):\n",
    "        for tok in file_toks[i]:\n",
    "            if(tok not in inv_index_postings.keys()):\n",
    "                inv_index_postings[tok] = [i]\n",
    "                inv_index_frequency[tok] = 1\n",
    "            else:\n",
    "                inv_index_postings[tok].append(i)\n",
    "                inv_index_frequency[tok] += 1\n",
    "    inv_index_postings = dict(sorted(inv_index_postings.items()))\n",
    "    terms_list = inv_index_postings.keys()\n",
    "    for word in terms_list:\n",
    "        inv_index_postings[word].sort()\n",
    "    return inv_index_postings, inv_index_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocsFromID(docID_to_docs, doc_IDs):\n",
    "    '''\n",
    "        Given a list of document IDs, it outputs the document names corresponding to thos IDs.\n",
    "        Input: docID_to_docs (mapping between docID -> doc_name), docIDs - list of input document IDs\n",
    "        Returns: doc_names - list of doc_names corresponding to document IDs in doc_IDs\n",
    "    '''\n",
    "    doc_names = []\n",
    "    for doc_ID in doc_IDs:\n",
    "        doc_names.append(docID_to_docs[doc_ID])\n",
    "    return doc_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive_postings(inv_index_postings, term):\n",
    "    '''\n",
    "        Given a term, retreive its posting list.\n",
    "        Input: inv_index_postings - inverted index postings, term\n",
    "        Returns: [] if term not in index, posting list for the term otherwise\n",
    "    '''\n",
    "    terms_list = inv_index_postings.keys()\n",
    "    if(term not in terms_list):\n",
    "        return []\n",
    "    else:\n",
    "        posting = inv_index_postings[term]\n",
    "        return posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive_frequency(inv_index_frequency, term):\n",
    "    terms_list = inv_index_frequency.keys()\n",
    "    if(term not in terms_list):\n",
    "        return 0\n",
    "    else:\n",
    "        freq = inv_index_frequency[term]\n",
    "        return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive_term_info(inv_index_postings, inv_index_frequency, term):\n",
    "    terms_list = inv_index_postings.keys()\n",
    "    if(term not in terms_list):\n",
    "        return [], 0\n",
    "    else:\n",
    "        posting = inv_index_postings[term]\n",
    "        freq = inv_index_frequency[term]\n",
    "        return posting, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_AND(posting1, posting2, verbose=False):\n",
    "\n",
    "\n",
    "    if(posting1 == [] or posting2 == []):\n",
    "        return 0, 0, []\n",
    "\n",
    "    ptr1 = 0\n",
    "    ptr2 = 0\n",
    "    answer_docID = []\n",
    "\n",
    "    num_comparisons = 0\n",
    "\n",
    "    while(ptr1 < len(posting1) and ptr2 < len(posting2)):\n",
    "        num_comparisons += 1\n",
    "        # print(f\"1 : {posting1[ptr1]} , 2: {posting2[ptr2]}\")\n",
    "\n",
    "        if(posting1[ptr1] == posting2[ptr2]):\n",
    "            answer_docID.append(posting1[ptr1])\n",
    "            ptr1 += 1\n",
    "            ptr2 += 1\n",
    "        elif(posting1[ptr1] < posting2[ptr2]):\n",
    "            ptr1 += 1\n",
    "        else:\n",
    "            ptr2 += 1\n",
    "\n",
    "    num_docs_retreived = len(answer_docID)\n",
    "    doc_names_retreived = getDocsFromID(docID_to_doc_mapping, answer_docID)\n",
    "\n",
    "    if(verbose==True):\n",
    "        print(f\"AND Query: \\nNo. of documents retreived: {num_docs_retreived}\\nMinimum number of comparisons: {num_comparisons}\\nNames of retreived documents: {doc_names_retreived}\")\n",
    "    \n",
    "    # temp_verification = list(set(posting1) & set(posting2))\n",
    "    # print(f\"Verification: {check_equal_arrays(temp_verification, answer_docID)}\")\n",
    "\n",
    "    return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID\n",
    "\n",
    "def query_OR(posting1, posting2, verbose=False):\n",
    "    \n",
    "\n",
    "    if((posting1 == []) and (posting2 == [])):\n",
    "        return 0, 0, []\n",
    "    elif((posting1 == []) and (posting2 != [])):\n",
    "        ans_docs = posting2\n",
    "        return len(ans_docs), 0, ans_docs\n",
    "    elif((posting1 != []) and (posting2 == [])):\n",
    "        ans_docs = posting1\n",
    "        return len(ans_docs), 0, ans_docs\n",
    "    else:\n",
    "        ptr1 = 0\n",
    "        ptr2 = 0\n",
    "        answer_docID = []\n",
    "\n",
    "        num_comparisons = 0\n",
    "        \n",
    "        while(ptr1 < len(posting1) and ptr2 < len(posting2)):\n",
    "            num_comparisons += 1\n",
    "            # print(f\"1 : {posting1[ptr1]} , 2: {posting2[ptr2]}\")\n",
    "\n",
    "            if(posting1[ptr1] == posting2[ptr2]):\n",
    "                answer_docID.append(posting1[ptr1])\n",
    "                ptr1 += 1\n",
    "                ptr2 += 1\n",
    "            elif(posting1[ptr1] < posting2[ptr2]):\n",
    "                answer_docID.append(posting1[ptr1])\n",
    "                ptr1 += 1\n",
    "            else:\n",
    "                answer_docID.append(posting2[ptr2])\n",
    "                ptr2 += 1\n",
    "        while(ptr1 < len(posting1)):\n",
    "            answer_docID.append(posting1[ptr1])\n",
    "            ptr1 += 1\n",
    "        while(ptr2 < len(posting2)):\n",
    "            answer_docID.append(posting2[ptr2])\n",
    "            ptr2 += 1\n",
    "\n",
    "\n",
    "        num_docs_retreived = len(answer_docID)\n",
    "        doc_names_retreived = getDocsFromID(docID_to_doc_mapping, answer_docID)\n",
    "\n",
    "        if(verbose==True):\n",
    "            print(f\"Query OR\\nNo. of documents retreived: {num_docs_retreived}\\nMinimum number of comparisons: {num_comparisons}\\nNames of retreived documents: {doc_names_retreived}\")\n",
    "        \n",
    "        # temp_verification = list(set(posting1) | set(posting2))\n",
    "        # print(f\"Verification: {check_equal_arrays(temp_verification, answer_docID)}\")\n",
    "        return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID\n",
    "\n",
    "def perform_NOT(posting):\n",
    "\n",
    "    all_docIDs = [docID for docID in range(len(file_names))]\n",
    "    if(posting == []):\n",
    "        return all_docIDs\n",
    "    for docID in posting:\n",
    "        # print(f\"Removing docID : {docID}\")\n",
    "        all_docIDs.remove(docID)\n",
    "\n",
    "    return all_docIDs\n",
    "\n",
    "def query_AND_NOT(posting1, posting2, verbose=False):\n",
    "\n",
    "    if((posting1 == [])):\n",
    "        return 0, 0, []\n",
    "    \n",
    "    posting2_NOT = perform_NOT(posting2)\n",
    "    ptr1 = 0\n",
    "    ptr2 = 0\n",
    "    answer_docID = []\n",
    "\n",
    "    num_comparisons = 0\n",
    "\n",
    "    while(ptr1 < len(posting1) and ptr2 < len(posting2_NOT)):\n",
    "        num_comparisons += 1\n",
    "        # print(f\"1 : {posting1[ptr1]} , 2: {posting2[ptr2]}\")\n",
    "        if(posting1[ptr1] == posting2_NOT[ptr2]):\n",
    "            answer_docID.append(posting1[ptr1])\n",
    "            ptr1 += 1\n",
    "            ptr2 += 1\n",
    "        elif(posting1[ptr1] < posting2_NOT[ptr2]):\n",
    "            ptr1 += 1\n",
    "        else:\n",
    "            ptr2 += 1\n",
    "\n",
    "    num_docs_retreived = len(answer_docID)\n",
    "    doc_names_retreived = getDocsFromID(docID_to_doc_mapping, answer_docID)\n",
    "\n",
    "    if(verbose==True):\n",
    "        print(f\"Query AND NOT\\nNo. of documents retreived: {num_docs_retreived}\\nMinimum number of comparisons: {num_comparisons}\\nNames of retreived documents: {doc_names_retreived}\")\n",
    "    \n",
    "    # temp_verification = list(set(posting1) | set(posting2))\n",
    "    # print(f\"Verification: {check_equal_arrays(temp_verification, answer_docID)}\")\n",
    "    return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID\n",
    "\n",
    "def query_OR_NOT(posting1, posting2, verbose=False):\n",
    "    \n",
    "    if((posting1 == []) and (posting2 == [])):\n",
    "        ans_docs = perform_NOT(posting2)\n",
    "        return len(ans_docs), 0, ans_docs\n",
    "    elif((posting1 == []) and (posting2 != [])):\n",
    "        ans_docs = perform_NOT(posting2)\n",
    "        return len(ans_docs), 0, ans_docs\n",
    "    elif((posting1 != []) and (posting2 == [])):\n",
    "        ans_docs = posting1\n",
    "        return len(ans_docs), 0, ans_docs\n",
    "    else:\n",
    "        \n",
    "        posting2_NOT = perform_NOT(posting2)\n",
    "        ptr1 = 0\n",
    "        ptr2 = 0\n",
    "        answer_docID = []\n",
    "\n",
    "        num_comparisons = 0\n",
    "\n",
    "        while(ptr1 < len(posting1) and ptr2 < len(posting2_NOT)):\n",
    "            num_comparisons += 1\n",
    "            # print(f\"1 : {posting1[ptr1]} , 2: {posting2[ptr2]}\")\n",
    "            if(posting1[ptr1] == posting2_NOT[ptr2]):\n",
    "                answer_docID.append(posting1[ptr1])\n",
    "                ptr1 += 1\n",
    "                ptr2 += 1\n",
    "            elif(posting1[ptr1] < posting2_NOT[ptr2]):\n",
    "                answer_docID.append(posting1[ptr1])\n",
    "                ptr1 += 1\n",
    "            else:\n",
    "                answer_docID.append(posting2_NOT[ptr2])\n",
    "                ptr2 += 1\n",
    "        while(ptr1 < len(posting1)):\n",
    "            answer_docID.append(posting1[ptr1])\n",
    "            ptr1 += 1\n",
    "        while(ptr2 < len(posting2_NOT)):\n",
    "            answer_docID.append(posting2_NOT[ptr2])\n",
    "            ptr2 += 1\n",
    "\n",
    "\n",
    "        num_docs_retreived = len(answer_docID)\n",
    "        doc_names_retreived = getDocsFromID(docID_to_doc_mapping, answer_docID)\n",
    "\n",
    "        if(verbose==True):\n",
    "            print(f\"Query OR NOT\\nNo. of documents retreived: {num_docs_retreived}\\nMinimum number of comparisons: {num_comparisons}\\nNames of retreived documents: {doc_names_retreived}\")\n",
    "        \n",
    "        return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ops(ops_string):\n",
    "    ops = ops_string.split(\",\")\n",
    "    ops = [op.strip() for op in ops]\n",
    "    for op in ops:\n",
    "        if(not(op == \"AND\" or op == \"OR\" or op == \"OR NOT\" or op == \"AND NOT\")):\n",
    "            return []\n",
    "    return ops\n",
    "\n",
    "def perform_query(posting1, posting2, op):\n",
    "    if(op == \"AND\"):\n",
    "        num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID = query_AND(posting1, posting2)\n",
    "        return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID\n",
    "    elif(op == \"OR\"):\n",
    "        num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID = query_OR(posting1, posting2)\n",
    "        return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID\n",
    "    elif(op == \"AND NOT\"):\n",
    "        num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID = query_AND_NOT(posting1, posting2)\n",
    "        return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID\n",
    "    elif(op == \"OR NOT\"):\n",
    "        num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID = query_OR_NOT(posting1, posting2)\n",
    "        return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID\n",
    "    else:\n",
    "        print(\"INVALID OPERATOR\")\n",
    "        return []\n",
    "\n",
    "def process_query(inv_index, query_tokens, ops):\n",
    "    # query_tokens = preprocess_query(query_text)\n",
    "    print(f\"Query tokens : {query_tokens}\\n\")\n",
    "    print(f\"Operators : \")\n",
    "    n_qtoks = len(query_tokens)\n",
    "    n_ops = n_qtoks - 1\n",
    "    # query_operato = copy.deepcopy(ops)\n",
    "    if(len(ops) > n_ops):\n",
    "        print(f\"Insufficient tokens in the query\\n\")\n",
    "        return -1, -1, []\n",
    "    elif(len(ops) < n_ops):\n",
    "        print(f\"Insufficient operators\")\n",
    "        return -1, -1, []\n",
    "    else:\n",
    "        total_comparisons = 0\n",
    "        if(n_ops == 1):\n",
    "            p1 = retreive_postings(inv_index, query_tokens[0])\n",
    "            p2 = retreive_postings(inv_index,query_tokens[1])\n",
    "            num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID = perform_query(p1, p2, ops[0])\n",
    "            return num_docs_retreived, num_comparisons, doc_names_retreived\n",
    "        else:\n",
    "            p1 = retreive_postings(inv_index, query_tokens[0])\n",
    "            p2 = retreive_postings(inv_index, query_tokens[1])\n",
    "            ndocs, total_comparisons, doc_names, ans = perform_query(p1, p2, ops[0])\n",
    "            qtok_ptr = 2\n",
    "            for i in range(1, len(ops)):\n",
    "                posting_tok = retreive_postings(inv_index, query_tokens[qtok_ptr])\n",
    "                ndocs, cmps, doc_names, ans = perform_query(ans, posting_tok, ops[i])\n",
    "                total_comparisons += cmps\n",
    "                qtok_ptr += 1\n",
    "            num_docs_retreived = len(ans)\n",
    "            doc_names_retreived = getDocsFromID(docID_to_doc_mapping, ans)\n",
    "            return num_docs_retreived, total_comparisons, doc_names_retreived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index_postings, inverted_index_frequency = create_inverted_index(file_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input query sequence : happy lion a bad boy\n",
      "\n",
      "Input operator sequence : [OR, OR NOT, AND]\n",
      "\n",
      "Input operator (array) : ['OR', 'OR NOT', 'AND']\n",
      "\n",
      "Query tokens : ['happy', 'lion', 'bad', 'boy']\n",
      "\n",
      "Query tokens : ['happy', 'lion', 'bad', 'boy']\n",
      "\n",
      "Operators : \n",
      "1133\n",
      "[1, 7, 9, 14, 15, 18, 21, 22, 26, 27, 28, 35, 38, 52, 56, 60, 69, 70, 72, 83, 84, 91, 92, 93, 95, 99, 109, 112, 113, 117, 119, 122, 127, 128, 140, 150, 153, 161, 167, 169, 173, 174, 175, 180, 183, 184, 212, 213, 218, 219, 222, 223, 224, 228, 230, 231, 239, 240, 241, 243, 245, 246, 248, 249, 252, 256, 260, 264, 266, 268, 270, 274, 277, 278, 280, 285, 286, 287, 288, 289, 291, 295, 299, 300, 305, 306, 307, 308, 315, 318, 322, 323, 324, 326, 328, 335, 338, 342, 344, 345, 346, 348, 352, 353, 354, 360, 362, 367, 370, 376, 377, 378, 379, 383, 390, 392, 402, 407, 414, 418, 420, 424, 428, 430, 438, 441, 445, 449, 450, 451, 454, 471, 476, 477, 484, 492, 495, 501, 511, 519, 520, 527, 530, 533, 537, 543, 547, 548, 549, 569, 572, 574, 575, 579, 584, 590, 592, 593, 599, 602, 604, 605, 609, 610, 611, 616, 617, 618, 630, 635, 641, 643, 648, 649, 655, 656, 660, 663, 665, 666, 667, 669, 672, 675, 682, 683, 686, 687, 688, 689, 691, 695, 705, 706, 707, 708, 709, 712, 713, 718, 721, 722, 724, 726, 749, 750, 752, 753, 762, 765, 767, 774, 777, 787, 788, 793, 794, 803, 809, 812, 814, 815, 817, 835, 837, 842, 844, 846, 850, 851, 853, 854, 857, 868, 887, 889, 895, 905, 906, 907, 908, 910, 911, 916, 924, 925, 928, 937, 941, 949, 980, 981, 984, 985, 989, 996, 999, 1001, 1016, 1017, 1018, 1028, 1029, 1034, 1035, 1039, 1041, 1044, 1047, 1054, 1056, 1058, 1063, 1067, 1068, 1074, 1093, 1104, 1106, 1109, 1110, 1111, 1112, 1113, 1118, 1120, 1127]\n",
      "Query : happy OR lion OR NOT bad AND boy \n",
      "\n",
      "\n",
      "\n",
      "No. of documents retreived: 121\n",
      "Number of comparisons: 2061\n",
      "Names of retreived documents: ['aggie.txt', 'alabama.txt', 'allfam.epi', 'amazing.epi', 'barney.txt', 'bbh_intv.txt', 'bitchcar.hum', 'bitnet.txt', 'bnbeg2.4.txt', 'bnbguide.txt', 'bnb_quot.txt', 'boneles2.txt', 'c0dez.txt', 'calvin.txt', 'cast.lis', 'chinesec.hum', 'cokeform.txt', 'confucius_say.txt', 'cookie.1', 'cowexplo.hum', 'crzycred.lst', 'dead3.txt', 'dead5.txt', 'dirtword.txt', 'drinks.txt', 'dromes.txt', 'dym', 'episimp2.txt', 'epi_.txt', 'epi_bnb.txt', 'epi_merm.txt', 'epi_rns.txt', 'epi_tton.txt', 'filmgoof.txt', 'films_gl.txt', 'forsooth.hum', 'fuck!.txt', 'fwksfun.hum', 'gd_alf.txt', 'gd_flybd.txt', 'gd_frasr.txt', 'gd_gal.txt', 'gd_liqtv.txt', 'gd_sgrnd.txt', 'gd_tznew.txt', 'golnar.txt', 'hitchcok.txt', 'homermmm.txt', 'humor9.txt', 'idaho.txt', 'inquirer.txt', 'ivan.hum', 'japantv.txt', 'jason.fun', 'jokes', 'kashrut.txt', 'legal.hum', 'letter_f.sch', 'llamas.txt', 'lost.txt', 'lotsa.jok', 'madscrib.hum', 'manners.txt', 'mead.rcp', 'melodram.hum', 'merry.txt', 'mlverb.hum', 'moore.txt', 'moose.txt', 'msorrow', 'normal.boy', 'normalboy.txt', 'normquot.txt', 'nukewar.txt', 'onetotwo.hum', 'oracle.jok', 'pepsideg.txt', 'phorse.hum', 'phunatdi.ana', 'polemom.txt', 'popconc.hum', 'prac1.jok', 'prac2.jok', 'prac3.jok', 'prac4.jok', 'practica.txt', 'pro-fact.hum', 'progrs.gph', 'pukeprom.jok', 'pun.txt', 'quux_p.oem', 'rns_bcl.txt', 'rns_bwl.txt', 'rns_ency.txt', 'scratchy.txt', 'sigs.txt', 'soleleer.hum', 'sorority.gir', 'strine.txt', 'stuf10.txt', 'terbear.txt', 'texican.dic', 'texican.lex', 'tfepisod.hum', 'tfpoems.hum', 'top10.txt', 'top10st2.txt', 'tpquotes.txt', 'trekwes.hum', 'trukdeth.txt', 'twilight.txt', 'twinkies.jok', 't_zone.jok', 'urban.txt', 'variety1.asc', 'welfare', 'welfare.txt', 'wetdream.hum', 'why-me.hum', 'wrdnws8.txt', 'xibovac.txt']\n"
     ]
    }
   ],
   "source": [
    "input_query_sequence = input(\"Enter the space-separated query terms : \")\n",
    "input_operator_sequence = input(\"Enter the comma-separated operator sequence enclosed in '[]' : \")\n",
    "print(f\"Input query sequence : {input_query_sequence}\\n\")\n",
    "print(f\"Input operator sequence : {input_operator_sequence}\\n\")\n",
    "input_operators = process_ops(input_operator_sequence[1:len(input_operator_sequence) - 1])\n",
    "query_tokens = preprocess_query(input_query_sequence)\n",
    "print(f\"Input operator (array) : {input_operators}\\n\")\n",
    "print(f\"Query tokens : {query_tokens}\\n\")\n",
    "query_result_ndocs, query_result_total_cmps, query_result_doc_names = process_query(inverted_index_postings, query_tokens, input_operators)\n",
    "\n",
    "if(query_result_ndocs != -1):\n",
    "    query_generated = query_tokens[0] + \" \" + input_operators[0] + \" \" + query_tokens[1] + \" \"\n",
    "    ptr_qtoks = 2\n",
    "    for i in range(1, len(input_operators)):\n",
    "        query_generated += input_operators[i] + \" \"\n",
    "        query_generated += query_tokens[ptr_qtoks] + \" \"\n",
    "        ptr_qtoks += 1\n",
    "    print(f\"Query : {query_generated}\\n\\n\")\n",
    "    print(f\"\\nNo. of documents retreived: {query_result_ndocs}\\nNumber of comparisons: {query_result_total_cmps}\\nNames of retreived documents: {query_result_doc_names}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2370e07001db70a9c24f7e21173c51fbc4321340913a02830aed4885459fa0a0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
