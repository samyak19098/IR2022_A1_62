{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# from substitutions import appos\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import string\n",
    "import numpy as np\n",
    "import contractions\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../Humor,Hist,Media,Food'\n",
    "file_names = os.listdir(data_dir)\n",
    "file_paths = [(data_dir + '/' + fname) for fname in file_names]\n",
    "docID_to_doc = {}\n",
    "for i in range(len(file_names)):\n",
    "    docID_to_doc[i] = file_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_files(fpaths):\n",
    "    file_tokens = []\n",
    "    for fpath in fpaths:\n",
    "        f = open(fpath, 'r', encoding='utf-8', errors='replace')\n",
    "        ftxt_unprocessed = f.read()\n",
    "        # print(ftxt_unprocessed)\n",
    "        ftoks = preprocess_file(ftxt_unprocessed)\n",
    "        file_tokens.append(ftoks)\n",
    "    return file_tokens\n",
    "\n",
    "def isValidTok(tok):\n",
    "    if((tok not in string.punctuation) and (tok.isnumeric() == False) and (sum([0 if ch in string.punctuation else 1 for ch in tok]) >= 1)):\n",
    "        return True\n",
    "    return False \n",
    "def check_alnum(tok):\n",
    "    tok = ''.join(x for x in tok if x.isalnum() == True)\n",
    "    return tok\n",
    "def remove_punct(tok):\n",
    "    punctuations = string.punctuation\n",
    "    tok = ''.join(x for x in tok if x not in punctuations)\n",
    "    return tok\n",
    "\n",
    "def remove_space(tok):\n",
    "    tok = ''.join(x for x in tok if x != ' ')\n",
    "    return tok\n",
    "\n",
    "def preprocess_file(file_text):\n",
    "    '''\n",
    "        Preprocess the file text\n",
    "    '''\n",
    "    #converting text to lowercase\n",
    "    text = file_text.lower()\n",
    "    \n",
    "    # print(f\"Original tokens : \\n{word_tokenize(text)}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "\n",
    "    #Fixing the contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    #omitting all non-alphanumeric characters\n",
    "    # text = ''.join([x if (x.isalnum() == True or x == \" \") else \" \" for x in text ])\n",
    "\n",
    "    #word tokenization\n",
    "    all_toks = word_tokenize(text)\n",
    "    all_unique_toks = set(all_toks)\n",
    "\n",
    "    all_unique_toks = [check_alnum(x) for x in all_unique_toks]\n",
    "\n",
    "    # print(f\"All unique tokens :\\n{all_unique_toks}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    #removing stopwords\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    file_toks = [tok for tok in all_unique_toks if tok not in stop_words]\n",
    "    # print(f\"After removing stopwords :\\n{file_toks}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    #removing punctations if any remain\n",
    "    toks_no_punct = []\n",
    "    for tok in file_toks:\n",
    "        ctok = remove_punct(tok)\n",
    "        # print(f\"-> before removal : {tok} || After removal : {ctok}\")\n",
    "        if(ctok != \"\"):\n",
    "            toks_no_punct.append(ctok)\n",
    "    # print(f\"After removing puncts :\\n{toks_no_punct}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "\n",
    "    #removing spaces in any remain & check if it is a number\n",
    "    cleaned_toks = []\n",
    "    for tok in toks_no_punct:\n",
    "        ctok = remove_space(tok)\n",
    "        # print(f\"-> before removal : {tok} || After removal : {ctok}\")\n",
    "        if(ctok != \"\"):\n",
    "            cleaned_toks.append(ctok)\n",
    "    cleaned_toks = list(set(cleaned_toks))\n",
    "    # print(f\"After removing spaces and removing numbers :\\n{cleaned_toks}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    #stemming tokens\n",
    "    # stemmer = PorterStemmer()\n",
    "    # stemmed_toks = set()\n",
    "\n",
    "    # for tok in cleaned_toks:\n",
    "    #     stok = stemmer.stem(tok)\n",
    "    #     if(stok != \"\" or (stok not in stop_words)): #consider non-blank tokens and the ones which have length greater than 1\n",
    "    #         stemmed_toks.add(stok)\n",
    "    # print(f\"After stemming and removing single letters :\\n{stemmed_toks}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    final_tokens = [tok for tok in cleaned_toks]\n",
    "\n",
    "    return final_tokens\n",
    "\n",
    "def preprocess_query(query_text):\n",
    "\n",
    "    text = query_text.lower()\n",
    "    \n",
    "    # print(f\"Original Query Tokens : \\n{word_tokenize(text)}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    \n",
    "    #Fixing the contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    #omitting all non-alphanumeric characters\n",
    "    # text = ''.join([x if (x.isalnum() == True or x == \" \") else \" \" for x in text ])\n",
    "\n",
    "    #word tokenization\n",
    "    all_toks = word_tokenize(text)\n",
    "    all_unique_toks = []\n",
    "    for tok in all_toks:\n",
    "        if(tok not in all_unique_toks):\n",
    "            all_unique_toks.append(tok)\n",
    "\n",
    "    all_unique_toks = [check_alnum(x) for x in all_unique_toks]\n",
    "\n",
    "    # print(f\"All unique tokens :\\n{all_unique_toks}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    #removing stopwords\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    file_toks = [tok for tok in all_unique_toks if tok not in stop_words]\n",
    "    # print(f\"After removing stopwords :\\n{file_toks}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    #removing punctations if any remain\n",
    "    toks_no_punct = []\n",
    "    for tok in file_toks:\n",
    "        ctok = remove_punct(tok)\n",
    "        # print(f\"-> before removal : {tok} || After removal : {ctok}\")\n",
    "        if(ctok != \"\"):\n",
    "            toks_no_punct.append(ctok)\n",
    "    # print(f\"After removing puncts :\\n{toks_no_punct}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "\n",
    "    #removing spaces in any remain & check if it is a number\n",
    "    cleaned_toks = []\n",
    "    for tok in toks_no_punct:\n",
    "        ctok = remove_space(tok)\n",
    "        # print(f\"-> before removal : {tok} || After removal : {ctok}\")\n",
    "        if(ctok != \"\"):\n",
    "            cleaned_toks.append(ctok)\n",
    "    \n",
    "    final_tokens = []\n",
    "    for tok in cleaned_toks:\n",
    "        if(tok not in final_tokens):\n",
    "            final_tokens.append(tok)\n",
    "    # cleaned_toks = list(set(cleaned_toks))\n",
    "    # print(f\"After removing spaces and removing numbers :\\n{cleaned_toks}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    #stemming tokens\n",
    "    # stemmer = PorterStemmer()\n",
    "    # stemmed_toks = set()\n",
    "\n",
    "    # for tok in cleaned_toks:\n",
    "    #     stok = stemmer.stem(tok)\n",
    "    #     if(stok != \"\" and len(stok) > 1 and (stok not in stop_words)): #consider non-blank tokens and the ones which have length greater than 1\n",
    "    #         stemmed_toks.add(stok)\n",
    "    # print(f\"After stemming and removing single letters :\\n{stemmed_toks}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    # final_tokens = [tok for tok in cleaned_toks]\n",
    "\n",
    "    return final_tokens\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_toks = read_files(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index(file_toks):\n",
    "    inv_index_postings = {}\n",
    "    inv_index_frequency = {}\n",
    "    for i in range(len(file_toks)):\n",
    "        for tok in file_toks[i]:\n",
    "            if(tok not in inv_index_postings.keys()):\n",
    "                inv_index_postings[tok] = [i]\n",
    "                inv_index_frequency[tok] = 1\n",
    "            else:\n",
    "                inv_index_postings[tok].append(i)\n",
    "                inv_index_frequency[tok] += 1\n",
    "    inv_index_postings = dict(sorted(inv_index_postings.items()))\n",
    "    terms_list = inv_index_postings.keys()\n",
    "    for word in terms_list:\n",
    "        inv_index_postings[word].sort()\n",
    "    return inv_index_postings, inv_index_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocsFromID(file_names, doc_IDs):\n",
    "    doc_names = []\n",
    "    for doc_ID in doc_IDs:\n",
    "        doc_names.append(file_names[doc_ID])\n",
    "    return doc_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_equal_arrays(arr1, arr2):\n",
    "\n",
    "    if(len(arr1) != len(arr2)):\n",
    "        return False\n",
    "    \n",
    "    arr1 = sorted(arr1)\n",
    "    arr2 = sorted(arr2)\n",
    "    for i in range(len(arr1)):\n",
    "        if(arr1[i] != arr2[i]):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive_postings(inv_index_postings, term):\n",
    "    terms_list = inv_index_postings.keys()\n",
    "    if(term not in terms_list):\n",
    "        return []\n",
    "    else:\n",
    "        posting = inv_index_postings[term]\n",
    "        return posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive_frequency(inv_index_frequency, term):\n",
    "    terms_list = inv_index_frequency.keys()\n",
    "    if(term not in terms_list):\n",
    "        return 0\n",
    "    else:\n",
    "        freq = inv_index_frequency[term]\n",
    "        return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive_term_info(inv_index_postings, inv_index_frequency, term):\n",
    "    terms_list = inv_index_postings.keys()\n",
    "    if(term not in terms_list):\n",
    "        return [], 0\n",
    "    else:\n",
    "        posting = inv_index_postings[term]\n",
    "        freq = inv_index_frequency[term]\n",
    "        return posting, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_AND(posting1, posting2, verbose=False):\n",
    "\n",
    "\n",
    "    if(posting1 == [] or posting2 == []):\n",
    "        return 0, 0, []\n",
    "\n",
    "    ptr1 = 0\n",
    "    ptr2 = 0\n",
    "    answer_docID = []\n",
    "\n",
    "    num_comparisons = 0\n",
    "\n",
    "    while(ptr1 < len(posting1) and ptr2 < len(posting2)):\n",
    "        num_comparisons += 1\n",
    "        # print(f\"1 : {posting1[ptr1]} , 2: {posting2[ptr2]}\")\n",
    "\n",
    "        if(posting1[ptr1] == posting2[ptr2]):\n",
    "            answer_docID.append(posting1[ptr1])\n",
    "            ptr1 += 1\n",
    "            ptr2 += 1\n",
    "        elif(posting1[ptr1] < posting2[ptr2]):\n",
    "            ptr1 += 1\n",
    "        else:\n",
    "            ptr2 += 1\n",
    "\n",
    "    num_docs_retreived = len(answer_docID)\n",
    "    doc_names_retreived = getDocsFromID(file_names, answer_docID)\n",
    "\n",
    "    if(verbose==True):\n",
    "        print(f\"AND Query: \\nNo. of documents retreived: {num_docs_retreived}\\nMinimum number of comparisons: {num_comparisons}\\nNames of retreived documents: {doc_names_retreived}\")\n",
    "    \n",
    "    # temp_verification = list(set(posting1) & set(posting2))\n",
    "    # print(f\"Verification: {check_equal_arrays(temp_verification, answer_docID)}\")\n",
    "\n",
    "    return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID\n",
    "\n",
    "def query_OR(posting1, posting2, verbose=False):\n",
    "    \n",
    "\n",
    "    if((posting1 == []) and (posting2 == [])):\n",
    "        return 0, 0, []\n",
    "    elif((posting1 == []) and (posting2 != [])):\n",
    "        ans_docs = posting2\n",
    "        return len(ans_docs), 0, ans_docs\n",
    "    elif((posting1 != []) and (posting2 == [])):\n",
    "        ans_docs = posting1\n",
    "        return len(ans_docs), 0, ans_docs\n",
    "    else:\n",
    "        ptr1 = 0\n",
    "        ptr2 = 0\n",
    "        answer_docID = []\n",
    "\n",
    "        num_comparisons = 0\n",
    "        \n",
    "        while(ptr1 < len(posting1) and ptr2 < len(posting2)):\n",
    "            num_comparisons += 1\n",
    "            # print(f\"1 : {posting1[ptr1]} , 2: {posting2[ptr2]}\")\n",
    "\n",
    "            if(posting1[ptr1] == posting2[ptr2]):\n",
    "                answer_docID.append(posting1[ptr1])\n",
    "                ptr1 += 1\n",
    "                ptr2 += 1\n",
    "            elif(posting1[ptr1] < posting2[ptr2]):\n",
    "                answer_docID.append(posting1[ptr1])\n",
    "                ptr1 += 1\n",
    "            else:\n",
    "                answer_docID.append(posting2[ptr2])\n",
    "                ptr2 += 1\n",
    "        while(ptr1 < len(posting1)):\n",
    "            answer_docID.append(posting1[ptr1])\n",
    "            ptr1 += 1\n",
    "        while(ptr2 < len(posting2)):\n",
    "            answer_docID.append(posting2[ptr2])\n",
    "            ptr2 += 1\n",
    "\n",
    "\n",
    "        num_docs_retreived = len(answer_docID)\n",
    "        doc_names_retreived = getDocsFromID(file_names, answer_docID)\n",
    "\n",
    "        if(verbose==True):\n",
    "            print(f\"Query OR\\nNo. of documents retreived: {num_docs_retreived}\\nMinimum number of comparisons: {num_comparisons}\\nNames of retreived documents: {doc_names_retreived}\")\n",
    "        \n",
    "        # temp_verification = list(set(posting1) | set(posting2))\n",
    "        # print(f\"Verification: {check_equal_arrays(temp_verification, answer_docID)}\")\n",
    "        return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID\n",
    "\n",
    "def perform_NOT(posting):\n",
    "\n",
    "    all_docIDs = [docID for docID in range(len(file_names))]\n",
    "    if(posting == []):\n",
    "        return all_docIDs\n",
    "    print(len(all_docIDs))\n",
    "    print(posting)\n",
    "    for docID in posting:\n",
    "        # print(f\"Removing docID : {docID}\")\n",
    "        all_docIDs.remove(docID)\n",
    "\n",
    "    return all_docIDs\n",
    "\n",
    "def query_AND_NOT(posting1, posting2, verbose=False):\n",
    "\n",
    "    if((posting1 == [])):\n",
    "        return 0, 0, []\n",
    "    \n",
    "    posting2_NOT = perform_NOT(posting2)\n",
    "    ptr1 = 0\n",
    "    ptr2 = 0\n",
    "    answer_docID = []\n",
    "\n",
    "    num_comparisons = 0\n",
    "\n",
    "    while(ptr1 < len(posting1) and ptr2 < len(posting2_NOT)):\n",
    "        num_comparisons += 1\n",
    "        # print(f\"1 : {posting1[ptr1]} , 2: {posting2[ptr2]}\")\n",
    "        if(posting1[ptr1] == posting2_NOT[ptr2]):\n",
    "            answer_docID.append(posting1[ptr1])\n",
    "            ptr1 += 1\n",
    "            ptr2 += 1\n",
    "        elif(posting1[ptr1] < posting2_NOT[ptr2]):\n",
    "            ptr1 += 1\n",
    "        else:\n",
    "            ptr2 += 1\n",
    "\n",
    "    num_docs_retreived = len(answer_docID)\n",
    "    doc_names_retreived = getDocsFromID(file_names, answer_docID)\n",
    "\n",
    "    if(verbose==True):\n",
    "        print(f\"Query AND NOT\\nNo. of documents retreived: {num_docs_retreived}\\nMinimum number of comparisons: {num_comparisons}\\nNames of retreived documents: {doc_names_retreived}\")\n",
    "    \n",
    "    # temp_verification = list(set(posting1) | set(posting2))\n",
    "    # print(f\"Verification: {check_equal_arrays(temp_verification, answer_docID)}\")\n",
    "    return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID\n",
    "\n",
    "def query_OR_NOT(posting1, posting2, verbose=False):\n",
    "    \n",
    "    if((posting1 == []) and (posting2 == [])):\n",
    "        ans_docs = perform_NOT(posting2)\n",
    "        return len(ans_docs), 0, ans_docs\n",
    "    elif((posting1 == []) and (posting2 != [])):\n",
    "        ans_docs = perform_NOT(posting2)\n",
    "        return len(ans_docs), 0, ans_docs\n",
    "    elif((posting1 != []) and (posting2 == [])):\n",
    "        ans_docs = posting1\n",
    "        return len(ans_docs), 0, ans_docs\n",
    "    else:\n",
    "        \n",
    "        posting2_NOT = perform_NOT(posting2)\n",
    "        ptr1 = 0\n",
    "        ptr2 = 0\n",
    "        answer_docID = []\n",
    "\n",
    "        num_comparisons = 0\n",
    "\n",
    "        while(ptr1 < len(posting1) and ptr2 < len(posting2_NOT)):\n",
    "            num_comparisons += 1\n",
    "            # print(f\"1 : {posting1[ptr1]} , 2: {posting2[ptr2]}\")\n",
    "            if(posting1[ptr1] == posting2_NOT[ptr2]):\n",
    "                answer_docID.append(posting1[ptr1])\n",
    "                ptr1 += 1\n",
    "                ptr2 += 1\n",
    "            elif(posting1[ptr1] < posting2_NOT[ptr2]):\n",
    "                answer_docID.append(posting1[ptr1])\n",
    "                ptr1 += 1\n",
    "            else:\n",
    "                answer_docID.append(posting2_NOT[ptr2])\n",
    "                ptr2 += 1\n",
    "        while(ptr1 < len(posting1)):\n",
    "            answer_docID.append(posting1[ptr1])\n",
    "            ptr1 += 1\n",
    "        while(ptr2 < len(posting2_NOT)):\n",
    "            answer_docID.append(posting2_NOT[ptr2])\n",
    "            ptr2 += 1\n",
    "\n",
    "\n",
    "        num_docs_retreived = len(answer_docID)\n",
    "        doc_names_retreived = getDocsFromID(file_names, answer_docID)\n",
    "\n",
    "        if(verbose==True):\n",
    "            print(f\"Query OR NOT\\nNo. of documents retreived: {num_docs_retreived}\\nMinimum number of comparisons: {num_comparisons}\\nNames of retreived documents: {doc_names_retreived}\")\n",
    "        \n",
    "        return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1133\n",
      "[0, 2, 3, 4, 6]\n",
      "Query OR NOT\n",
      "No. of documents retreived: 1132\n",
      "Minimum number of comparisons: 6\n",
      "Names of retreived documents: ['1st_aid.txt', 'a-team', 'abbott.txt', 'aboutada.txt', 'acetab1.txt', 'aclamt.txt', 'acronym.lis', 'acronym.txt', 'acronyms.txt', 'adameve.hum', 'adcopy.hum', 'addrmeri.txt', 'admin.txt', 'adrian_e.faq', 'ads.txt', 'adt_miam.txt', 'advrtize.txt', 'aeonint.txt', 'age.txt', 'aggie.txt', 'aids.txt', 'airlines', 'alabama.txt', 'alcatax.txt', 'alcohol.hum', 'alflog.txt', 'allfam.epi', 'allusion', 'all_grai', 'amazing.epi', 'ambrose.bie', 'amchap2.txt', 'analogy.hum', 'aniherb.txt', 'anime.cli', 'anime.lif', 'anim_lif.txt', 'annoy.fascist', 'anorexia.txt', 'answers', 'anthropo.stu', 'antibiot.txt', 'antimead.bev', 'aphrodis.txt', 'appbred.brd', 'appetiz.rcp', 'applepie.des', 'apsaucke.des', 'apsnet.txt', 'arab.dic', 'arcadian.txt', 'argotdic.txt', 'arnold.txt', 'art-fart.hum', 'arthriti.txt', 'ateam.epi', 'atherosc.txt', 'atombomb.hum', 'att.txt', 'aussie.lng', 'avengers.lis', 'awespinh.sal', 'ayurved.txt', 'a_fish_c.apo', 'a_tv_t-p.com', 'b-2.jok', 'b12.txt', 'back1.txt', 'bad', 'bad-d', 'bad.jok', 'badday.hum', 'bagelope.txt', 'bakebred.txt', 'baklava.des', 'banana01.brd', 'banana02.brd', 'banana03.brd', 'banana04.brd', 'banana05.brd', 'bank.rob', 'barney.cn1', 'barney.txt', 'basehead.txt', 'batrbred.txt', 'bb', 'bbc_vide.cat', 'bbh_intv.txt', 'bbq.txt', 'beapimp.hum', 'beauty.tm', 'beave.hum', 'beer-g', 'beer-gui', 'beer.gam', 'beer.hum', 'beer.txt', 'beerdiag.txt', 'beergame.hum', 'beergame.txt', 'beerjesus.hum', 'beershrm.fis', 'beershrp.fis', 'beerwarn.txt', 'beesherb.txt', 'beginn.ers', 'berryeto.bev', 'bhang.fun', 'bhb.ill', 'bible.txt', 'bigpic1.hum', 'billcat.hum', 'bimg.prn', 'bingbong.hum', 'bitchcar.hum', 'bitnet.txt', 'blackadd', 'blackapp.hum', 'blackhol.hum', 'blake7.lis', 'blaster.hum', 'bless.bc', 'blkbean.txt', 'blkbnsrc.vgn', 'blood.txt', 'blooprs1.asc', 'bmdn01.txt', 'bnbeg2.4.txt', 'bnbguide.txt', 'bnb_quot.txt', 'boarchil.txt', 'boatmemo.jok', 'boe.hum', 'bond-2.txt', 'boneles2.txt', 'booknuti.txt', 'booze.fun', 'booze1.fun', 'booze2.fun', 'bored.txt', 'boston.geog', 'bozo_tv.leg', 'brainect.hum', 'brdpudd.des', 'bread.rcp', 'bread.rec', 'bread.txt', 'breadpud.des', 'bredcake.des', 'brewing', 'browneco.hum', 'brownie.rec', 'brush1.txt', 'btaco.txt', 'btcisfre.hum', 'btscke01.des', 'btscke02.des', 'btscke03.des', 'btscke04.des', 'btscke05.des', 'buffwing.pol', 'bugbreak.hum', 'bugs.txt', 'buldrwho.txt', 'bunacald.fis', 'burrito.mea', 'butcher.txt', 'butstcod.fis', 'butwrong.hum', 'buzzword.hum', 'bw-phwan.hat', 'bw-summe.hat', 'bw.txt', 'byfb.txt', 'c0dez.txt', 'cabbage.txt', 'caesardr.sal', 'cake.rec', 'calamus.hrb', 'calculus.txt', 'calif.hum', 'calvin.txt', 'cancer.rat', 'candy.txt', 'candybar.fun', 'capital.txt', 'caramels.des', 'carowner.txt', 'cars.txt', 'cartoon.law', 'cartoon.laws', 'cartoon_.txt', 'cartoon_laws.txt', 'cartwb.son', 'cast.lis', 'catballs.hum', 'catin.hat', 'catranch.hum', 'catstory.txt', 'cbmatic.hum', 'cereal.txt', 'cform2.txt', 'cgs_lst.txt', 'chainltr.txt', 'change.hum', 'charity.hum', 'cheapfar.hum', 'cheapin.la', 'chickenheadbbs.txt', 'chickens.jok', 'chickens.txt', 'childhoo.jok', 'childrenbooks.txt', 'chili.txt', 'chinese.txt', 'chinesec.hum', 'choco-ch.ips', 'christop.int', 'chung.iv', 'chunnel.txt', 'church.sto', 'clancy.txt', 'classicm.hum', 'climbing.let', 'cmu.share', 'co-car.jok', 'cockney.alp', 'coffee.faq', 'coffee.txt', 'coffeebeerwomen.txt', 'cogdis.txt', 'coke.fun', 'coke.txt', 'coke1', 'cokeform.txt', 'coke_fan.naz', 'coladrik.fun', 'coladrik.txt', 'cold.fus', 'coldfake.hum', 'collected_quotes.txt', 'college.hum', 'college.sla', 'college.txt', 'comic_st.gui', 'commutin.jok', 'commword.hum', 'computer.txt', 'comrevi1.hum', 'conan.txt', 'confucius_say.txt', 'consp.txt', 'contract.moo', 'cookberk', 'cookbkly.how', 'cookie.1', 'cooking.fun', 'cooking.jok', 'coollngo2.txt', 'cooplaws', 'cops.txt', 'corporat.txt', 'court.quips', 'cowexplo.hum', 'coyote.txt', 'crazy.txt', 'critic.txt', 'crzycred.lst', 'cuchy.hum', 'cucumber.jok', 'cucumber.txt', 'cuisine.txt', 'cultmov.faq', 'curiousgeorgie.txt', 'curry.hrb', 'curry.txt', 'curse.txt', 'cybrtrsh.txt', 'd-ned.hum', 'dalive', 'damiana.hrb', 'dandwine.bev', 'dark.suc', 'dead-r', 'dead2.txt', 'dead3.txt', 'dead4.txt', 'dead5.txt', 'deadlysins.txt', 'deathhem.txt', 'deep.txt', 'defectiv.hum', 'desk.txt', 'deterior.hum', 'devils.jok', 'diesmurf.txt', 'diet.txt', 'dieter.txt', 'dingding.hum', 'dining.out', 'dirtword.txt', 'disaster.hum', 'disclmr.txt', 'disclym.txt', 'doc-says.txt', 'docdict.txt', 'docspeak.txt', 'doggun.sto', 'donut.txt', 'dover.poem', 'draxamus.txt', 'drinker.txt', 'drinking.tro', 'drinkrul.jok', 'drinks.gui', 'drinks.txt', 'drive.txt', 'dromes.txt', 'druggame.hum', 'drugshum.hum', 'drunk.txt', 'dthought.txt', 'dubltalk.jok', 'dym', 'eandb.drx', 'earp', 'eatme.txt', 'econridl.fun', 'egg-bred.txt', 'egglentl.vgn', 'eggroll1.mea', 'electric.txt', 'element.jok', 'elephant.fun', 'elevator.fun', 'empeval.txt', 'engineer.hum', 'english', 'english.txt', 'engmuffn.txt', 'engrhyme.txt', 'enlightenment.txt', 'enquire.hum', 'epikarat.txt', 'epiquest.txt', 'episimp2.txt', 'epitaph', 'epi_.txt', 'epi_bnb.txt', 'epi_merm.txt', 'epi_rns.txt', 'epi_tton.txt', 'eskimo.nel', 'exam.50', 'excuse.txt', 'excuse30.txt', 'excuses.txt', 'exidy.txt', 'exylic.txt', 'facedeth.txt', 'failure.txt', 'fajitas.rcp', 'farsi.phrase', 'farsi.txt', 'fartinfo.txt', 'fartting.txt', 'fascist.txt', 'fbipizza.txt', 'fearcola.hum', 'fed.txt', 'fegg!int.txt', 'feggaqui.txt', 'feggmagi.txt', 'feista01.dip', 'female.jok', 'fiber.txt', 'figure_1.txt', 'filmgoof.txt', 'films_gl.txt', 'final-ex.txt', 'finalexm.hum', 'firecamp.txt', 'fireplacein.txt', 'firstaid.inf', 'firstaid.txt', 'fish.rec', 'flattax.hum', 'flowchrt', 'flowchrt.txt', 'flux_fix.txt', 'focaccia.brd', 'food', 'foodtips', 'footfun.hum', 'forsooth.hum', 'free-cof.fee', 'freshman.hum', 'freudonseuss.txt', 'frogeye1.sal', 'from.hum', 'fuck!.txt', 'fuckyou2.txt', 'fudge.txt', 'fusion.gal', 'fusion.sup', 'fwksfun.hum', 'f_tang.txt', 'gack!.txt', 'gaiahuma', 'gameshow.txt', 'ganamembers.txt', 'garlpast.vgn', 'gas.txt', 'gd_alf.txt', 'gd_drwho.txt', 'gd_flybd.txt', 'gd_frasr.txt', 'gd_gal.txt', 'gd_guide.txt', 'gd_hhead.txt', 'gd_liqtv.txt', 'gd_maxhd.txt', 'gd_ol.txt', 'gd_ql.txt', 'gd_sgrnd.txt', 'gd_tznew.txt', 'german.aut', 'get.drunk.cheap', 'ghostfun.hum', 'ghostsch.hum', 'gingbeer.txt', 'girlspeak.txt', 'godmonth.txt', 'goforth.hum', 'gohome.hum', 'goldwatr.txt', 'golnar.txt', 'good.txt', 'gotukola.hrb', 'gown.txt', 'grail.txt', 'grammar.jok', 'greenchi.txt', 'grommet.hum', 'grospoem.txt', 'growth.txt', 'gumbo.txt', 'hack', 'hack7.txt', 'hackingcracking.txt', 'hackmorality.txt', 'hacktest.txt', 'hamburge.nam', 'hammock.hum', 'hangover.txt', 'happyhack.txt', 'harmful.hum', 'hate.hum', 'hbo_spec.rev', 'headlnrs', 'hecomes.jok', 'hedgehog.txt', 'height.txt', 'hell.jok', 'hell.txt', 'herb!.hum', 'hermsys.txt', 'heroic.txt', 'hi.tec', 'hierarch.txt', 'highland.epi', 'hilbilly.wri', 'history2.oop', 'hitchcoc.app', 'hitchcok.txt', 'hitler.59', 'hitler.txt', 'hitlerap.txt', 'homebrew.txt', 'homermmm.txt', 'hoonsrc.txt', 'hoosier.txt', 'hop.faq', 'horflick.txt', 'horoscop.jok', 'horoscop.txt', 'horoscope.txt', 'hotel.txt', 'hotnnot.hum', 'hotpeper.txt', 'how.bugs.breakd', 'how2bgod.txt', 'how2dotv.txt', 'howlong.hum', 'how_to_i.pro', 'hstlrtxt.txt', 'htswfren.txt', 'hum2', 'humatra.txt', 'humatran.jok', 'humor9.txt', 'humpty.dumpty', 'iced.tea', 'icm.hum', 'idaho.txt', 'idr2.txt', 'imbecile.txt', 'imprrisk.hum', 'impurmat.hum', 'incarhel.hum', 'indgrdn.txt', 'initials.rid', 'inlaws1.txt', 'inquirer.txt', 'ins1', 'insanity.hum', 'insect1.txt', 'insult', 'insult.lst', 'insults1.txt', 'insuranc.sty', 'insure.hum', 'interv.hum', 'investi.hum', 'iqtest', 'iremember', 'is_story.txt', 'italoink.txt', 'ivan.hum', 'jac&tuu.hum', 'jalapast.dip', 'jambalay.pol', 'japantv.txt', 'japice.bev', 'japrap.hum', 'jargon.phd', 'jason.fun', 'jawgumbo.fis', 'jawsalad.fis', 'jayjay.txt', 'jc-elvis.inf', 'jeffie.heh', 'jerky.rcp', 'jimhood.txt', 'johann', 'jokeju07.txt', 'jokes', 'jokes.txt', 'jokes1.txt', 'jon.txt', 'jrrt.riddle', 'jungjuic.bev', 'just2', 'justify', 'kaboom.hum', 'kanalx.txt', 'kashrut.txt', 'kid2', 'kid_diet.txt', 'killer.hum', 'killself.hum', 'kilroy', 'kilsmur.hum', 'kloo.txt', 'koans.txt', 'labels.txt', 'lampoon.jok', 'languag.jok', 'lansing.txt', 'law.sch', 'lawhunt.txt', 'laws.txt', 'lawskool.txt', 'lawsuniv.hum', 'lawyer.jok', 'lawyers.txt', 'lazarus.txt', 'la_times.hun', 'lbinter.hum', 'leech.txt', 'legal.hum', 'let.go', 'letgosh.txt', 'letter.txt', 'letterbx.txt', 'letter_f.sch', 'libraway.txt', 'liceprof.sty', 'lif&love.hum', 'lifeimag.hum', 'lifeinfo.hum', 'lifeonledge.txt', 'limerick.jok', 'lines.jok', 'lion.jok', 'lion.txt', 'lions.cat', 'lipkovits.txt', 'livnware.hum', 'llamas.txt', 'lll.hum', 'llong.hum', 'lobquad.hum', 'looser.hum', 'losers84.hum', 'losers86.hum', 'lost.txt', 'lotsa.jok', 'lozers', 'lozerzon.hum', 'lozeuser.hum', 'lp-assoc.txt', 'lucky.cha', 'ludeinfo.hum', 'ludeinfo.txt', 'luggage.hum', 'luvstory.txt', 'luzerzo2.hum', 'm0dzmen.hum', 'macsfarm.old', 'madhattr.jok', 'madscrib.hum', 'maecenas.hum', 'mailfrag.hum', 'makebeer.hum', 'making_y.wel', 'malechem.txt', 'manager.txt', 'manilla.hum', 'manners.txt', 'manspace.hum', 'margos.txt', 'marines.hum', 'marriage.hum', 'mash.hum', 'math.1', 'math.2', 'math.far', 'maxheadr', 'mcd.txt', 'mead.rcp', 'meat2.txt', 'meinkamp.hum', 'mel.txt', 'melodram.hum', 'memo.hum', 'memory.hum', 'men&wome.txt', 'mensroom.jok', 'merry.txt', 'miamadvi.hum', 'miami.hum', 'miamimth.txt', 'middle.age', 'mindvox', 'minn.txt', 'miranda.hum', 'misc.1', 'misery.hum', 'missdish', 'missheav.hum', 'mitch.txt', 'mlverb.hum', 'modemwld.txt', 'modest.hum', 'modstup', 'mog-history', 'montoys.txt', 'montpyth.hum', 'moonshin', 'moore.txt', 'moose.txt', 'moslem.txt', 'mothers.txt', 'motrbike.jok', 'mov_rail.txt', 'mowers.txt', 'mr.rogers', 'mrscienc.hum', 'mrsfield', 'msfields.txt', 'msorrow', 'mtm.hum', 'mtv.asc', 'mundane.v2', 'murph.jok', 'murphy.txt', 'murphys.txt', 'murphy_l.txt', 'mutate.hum', 'mydaywss.hum', 'myheart.hum', 'naivewiz.hum', 'namaste.txt', 'nameisreo.txt', 'namm', 'nasaglenn.txt', 'necropls.txt', 'netmask.txt', 'netnews.10', 'newcoke.txt', 'newconst.hum', 'newmex.hum', 'news.hum', 'nigel.1', 'nigel.10', 'nigel.2', 'nigel.3', 'nigel.4', 'nigel.5', 'nigel.6', 'nigel.7', 'nigel10.txt', 'nihgel_8.9', 'nintendo.jok', 'normal.boy', 'normalboy.txt', 'normquot.txt', 'nosuch_nasfic', 'novel.hum', 'nuke.hum', 'nukeplay.hum', 'nukewar.jok', 'nukewar.txt', 'nukwaste', 'number', 'number.killer', 'number_k.ill', 'nurds.hum', 'nysucks.hum', 'nzdrinks.txt', 'o-ttalk.hum', 'oakwood.txt', 'oam-001.txt', 'oam.nfo', 'oasis', 'oatbran.rec', 'oculis.rcp', 'odd_to.obs', 'odearakk.hum', 'office.txt', 'ohandre.hum', 'oilgluts.hum', 'old.txt', 'oldeng.hum', 'oldtime.sng', 'oldtime.txt', 'oliver.txt', 'oliver02.txt', 'onan.txt', 'one.par', 'onetoone.hum', 'onetotwo.hum', 'ookpik.hum', 'opinion.hum', 'oracle.jok', 'oranchic.pol', 'orgfrost.bev', 'ourfathr.txt', 'outawork.erl', 'outlimit.txt', 'oxymoron.jok', 'oxymoron.txt', 'ozarks.hum', 'p-law.hum', 'packard.txt', 'paddingurpapers.txt', 'parabl.hum', 'parades.hum', 'parsnip.txt', 'passage.hum', 'passenge.sim', 'pasta001.sal', 'pat.txt', 'pbcookie.des', 'peanuts.txt', 'peatchp.hum', 'pecker.txt', 'penisprt.txt', 'penndtch', 'pepper.txt', 'pepsideg.txt', 'petshop', 'phony.hum', 'phorse.hum', 'phunatdi.ana', 'phxbbs-m.txt', 'pickup.lin', 'pickup.txt', 'pipespec.txt', 'pizzawho.hum', 'planeget.hum', 'planetzero.txt', 'poets.hum', 'pol-corr.txt', 'polemom.txt', 'poli.tics', 'policpig.hum', 'poli_t.ics', 'poll2res.hum', 'polly.txt', 'polly_.new', 'poopie.txt', 'popconc.hum', 'popmach', 'popmusi.hum', 'post.nuc', 'pot.txt', 'potty.txt', 'pournell.spo', 'ppbeer.txt', 'prac1.jok', 'prac2.jok', 'prac3.jok', 'prac4.jok', 'pracjoke.txt', 'practica.txt', 'prawblim.hum', 'prayer.hum', 'primes.jok', 'princess.brd', 'pro-fact.hum', 'problem.txt', 'progrs.gph', 'proof.met', 'prooftec.txt', 'proposal.jok', 'proudlyserve.txt', 'prover.wisom', 'prover_w.iso', 'psalm.reagan', 'psalm23.txt', 'psalm_nixon', 'psalm_re.aga', 'psilaine.hum', 'psycho.txt', 'psych_pr.quo', 'pukeprom.jok', 'pun.txt', 'pure.mat', 'puzzle.spo', 'puzzles.jok', 'python_s.ong', 'q.pun', 'qttofu.vgn', 'quack26.txt', 'quantity.001', 'quantum.jok', 'quantum.phy', 'quest.hum', 'quick.jok', 'quotes.bug', 'quotes.jok', 'quotes.txt', 'quux_p.oem', 'rabbit.txt', 'racist.net', 'radexposed.txt', 'radiolaf.hum', 'rapmastr.hum', 'ratings.hum', 'ratspit.hum', 'raven.hum', 'readme.bat', 'reagan.hum', 'realest.txt', 'reasons.txt', 'rec.por', 'recepies.fun', 'recip1.txt', 'recipe.001', 'recipe.002', 'recipe.003', 'recipe.004', 'recipe.005', 'recipe.006', 'recipe.007', 'recipe.008', 'recipe.009', 'recipe.010', 'recipe.011', 'recipe.012', 'reconcil.hum', 'record_.gap', 'red-neck.jks', 'reddwarf.sng', 'reddye.hum', 'rednecks.txt', 'reeves.txt', 'relative.ada', 'religion.txt', 'renored.txt', 'renorthr.txt', 'rent-a_cat', 'rentals.hum', 'repair.hum', 'report.hum', 'research.hum', 'residncy.jok', 'resolutn.txt', 'resrch_p.hra', 'resrch_phrase', 'revolt.dj', 'richbred.txt', 'rinaldo.jok', 'rinaldos.law', 'rinaldos.txt', 'ripoffpc.hum', 'rns_bcl.txt', 'rns_bwl.txt', 'rns_ency.txt', 'roach.asc', 'roadpizz.txt', 'robot.tes', 'rocking.hum', 'rockmus.hum', 'sanshop.txt', 'saveface.hum', 'sawyer.txt', 'scam.txt', 'scratchy.txt', 'seafood.txt', 'seeds42.txt', 'sf-zine.pub', 'sfmovie.txt', 'shameonu.hum', 'shooters.txt', 'shorties.jok', 'shrink.news', 'shuimai.txt', 'shuttleb.hum', 'signatur.jok', 'sigs.txt', 'silverclaws.txt', 'simp.txt', 'sinksub.txt', 'skincat', 'skippy.hum', 'skippy.txt', 'slogans.txt', 'smackjok.hum', 'smartass.txt', 'smiley.txt', 'smokers.txt', 'smurf-03.txt', 'smurfkil.hum', 'smurfs.cc', 'smurf_co.txt', 'snapple.rum', 'snipe.txt', 'soccer.txt', 'socecon.hum', 'social.hum', 'socks.drx', 'solders.hum', 'soleleer.hum', 'solviets.hum', 'some_hu.mor', 'soporifi.abs', 'sorority.gir', 'spacever.hum', 'speling.msk', 'spelin_r.ifo', 'spider.hum', 'spoonlis.txt', 'spydust.hum', 'squids.gph', 'staff.txt', 'stagline.txt', 'standard.hum', 'startrek.txt', 'stereo.txt', 'steroid.txt', 'stone.hum', 'strattma.txt', 'stressman.txt', 'strine.txt', 'strsdiet.txt', 'studentb.txt', 'stuf10.txt', 'stuf11.txt', 'st_silic.txt', 'subb_lis.txt', 'subrdead.hum', 'suicide2.txt', 'sungenu.hum', 'supermar.rul', 'swearfrn.hum', 'sw_err.txt', 'symbol.hum', 'sysadmin.txt', 'sysman.txt', 't-10.hum', 't-shirt.hum', 'takenote.jok', 'talebeat.hum', 'talkbizr.txt', 'taping.hum', 'tarot.txt', 'teens.txt', 'teevee.hum', 'telecom.q', 'televisi.hum', 'televisi.txt', 'temphell.jok', 'terbear.txt', 'termpoem.txt', 'terms.hum', \"terrmcd'.hum\", 'terrnieg.hum', 'test.hum', 'test.jok', 'test2.jok', 'testchri.txt', 'texbeef.txt', 'texican.dic', 'texican.lex', 'textgrap.hum', 'tfepisod.hum', 'tfpoems.hum', 'thecube.hum', 'thermite.ana', 'thesis.beh', 'the_ant.txt', 'the_math.hel', 'thievco.txt', 'three.txt', 'throwawa.hum', 'tickmoon.hum', 'timetr.hum', 'tnd.1', 'top10.elf', 'top10.txt', 'top10st1.txt', 'top10st2.txt', 'topten.hum', 'toxcwast.hum', 'tpquote2.txt', 'tpquotes.txt', 'transp.txt', 'trekfume.txt', 'trekwes.hum', 'tribble.hum', 'trukdeth.txt', 'truthlsd.hum', 'truths.hum', 'tshirts.jok', 'tuflife.txt', 'tuna.lab', 'turbo.hum', 'turing.shr', 'turkey.fun', 'twilight.txt', 'twinkie.txt', 'twinkies.jok', 'twinpeak.txt', 't_zone.jok', 'ukunderg.txt', 'un.happy', 'units.mea', 'univ.odd', 'unochili.txt', 'urban.txt', 'vaguemag.90s', 'valujet.txt', 'variety1.asc', 'variety2.asc', 'variety3.asc', 'various.txt', 'vegan.rcp', 'vegkill.txt', 'venganza.txt', 'venison.txt', 'voltron.hum', 'vonthomp', 'wacky.ani', 'wagit.txt', 'wagon.hum', 'waitress.txt', 'washroom.txt', 'watchlip.hum', 'wedding.hum', 'weight.txt', 'weights.hum', 'welfare', 'welfare.txt', 'wetdream.hum', 'whatbbs', 'whatthe.hum', 'whitbred.txt', 'who.txt', 'whoon1st.hum', 'whoops.hum', 'why-me.hum', 'widows', 'wimptest.txt', 'wisconsi.txt', 'wisdom', 'wkrp.epi', 'women.jok', 'wonton.txt', 'wood', 'woodbine.txt', 'woodbugs.txt', 'woods.txt', 'woodsmok.txt', 'woolly_m.amm', 'word.hum', 'worldend.hum', 'wrdnws1.txt', 'wrdnws2.txt', 'wrdnws3.txt', 'wrdnws4.txt', 'wrdnws5.txt', 'wrdnws6.txt', 'wrdnws7.txt', 'wrdnws8.txt', 'wrdnws9.txt', 'x-drinks.txt', 'xibovac.txt', 'xtermin8.hum', 'y.txt', 'yjohncse.hum', 'yogisays.txt', 'yogurt.asc', 'yuban.txt', 'yuppies.hum', 'zen.txt', 'zgtoilet.txt', 'zodiac.hum', 'zucantom.sal', 'zuccmush.sal']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1132,\n",
       " 6,\n",
       " ['1st_aid.txt',\n",
       "  'a-team',\n",
       "  'abbott.txt',\n",
       "  'aboutada.txt',\n",
       "  'acetab1.txt',\n",
       "  'aclamt.txt',\n",
       "  'acronym.lis',\n",
       "  'acronym.txt',\n",
       "  'acronyms.txt',\n",
       "  'adameve.hum',\n",
       "  'adcopy.hum',\n",
       "  'addrmeri.txt',\n",
       "  'admin.txt',\n",
       "  'adrian_e.faq',\n",
       "  'ads.txt',\n",
       "  'adt_miam.txt',\n",
       "  'advrtize.txt',\n",
       "  'aeonint.txt',\n",
       "  'age.txt',\n",
       "  'aggie.txt',\n",
       "  'aids.txt',\n",
       "  'airlines',\n",
       "  'alabama.txt',\n",
       "  'alcatax.txt',\n",
       "  'alcohol.hum',\n",
       "  'alflog.txt',\n",
       "  'allfam.epi',\n",
       "  'allusion',\n",
       "  'all_grai',\n",
       "  'amazing.epi',\n",
       "  'ambrose.bie',\n",
       "  'amchap2.txt',\n",
       "  'analogy.hum',\n",
       "  'aniherb.txt',\n",
       "  'anime.cli',\n",
       "  'anime.lif',\n",
       "  'anim_lif.txt',\n",
       "  'annoy.fascist',\n",
       "  'anorexia.txt',\n",
       "  'answers',\n",
       "  'anthropo.stu',\n",
       "  'antibiot.txt',\n",
       "  'antimead.bev',\n",
       "  'aphrodis.txt',\n",
       "  'appbred.brd',\n",
       "  'appetiz.rcp',\n",
       "  'applepie.des',\n",
       "  'apsaucke.des',\n",
       "  'apsnet.txt',\n",
       "  'arab.dic',\n",
       "  'arcadian.txt',\n",
       "  'argotdic.txt',\n",
       "  'arnold.txt',\n",
       "  'art-fart.hum',\n",
       "  'arthriti.txt',\n",
       "  'ateam.epi',\n",
       "  'atherosc.txt',\n",
       "  'atombomb.hum',\n",
       "  'att.txt',\n",
       "  'aussie.lng',\n",
       "  'avengers.lis',\n",
       "  'awespinh.sal',\n",
       "  'ayurved.txt',\n",
       "  'a_fish_c.apo',\n",
       "  'a_tv_t-p.com',\n",
       "  'b-2.jok',\n",
       "  'b12.txt',\n",
       "  'back1.txt',\n",
       "  'bad',\n",
       "  'bad-d',\n",
       "  'bad.jok',\n",
       "  'badday.hum',\n",
       "  'bagelope.txt',\n",
       "  'bakebred.txt',\n",
       "  'baklava.des',\n",
       "  'banana01.brd',\n",
       "  'banana02.brd',\n",
       "  'banana03.brd',\n",
       "  'banana04.brd',\n",
       "  'banana05.brd',\n",
       "  'bank.rob',\n",
       "  'barney.cn1',\n",
       "  'barney.txt',\n",
       "  'basehead.txt',\n",
       "  'batrbred.txt',\n",
       "  'bb',\n",
       "  'bbc_vide.cat',\n",
       "  'bbh_intv.txt',\n",
       "  'bbq.txt',\n",
       "  'beapimp.hum',\n",
       "  'beauty.tm',\n",
       "  'beave.hum',\n",
       "  'beer-g',\n",
       "  'beer-gui',\n",
       "  'beer.gam',\n",
       "  'beer.hum',\n",
       "  'beer.txt',\n",
       "  'beerdiag.txt',\n",
       "  'beergame.hum',\n",
       "  'beergame.txt',\n",
       "  'beerjesus.hum',\n",
       "  'beershrm.fis',\n",
       "  'beershrp.fis',\n",
       "  'beerwarn.txt',\n",
       "  'beesherb.txt',\n",
       "  'beginn.ers',\n",
       "  'berryeto.bev',\n",
       "  'bhang.fun',\n",
       "  'bhb.ill',\n",
       "  'bible.txt',\n",
       "  'bigpic1.hum',\n",
       "  'billcat.hum',\n",
       "  'bimg.prn',\n",
       "  'bingbong.hum',\n",
       "  'bitchcar.hum',\n",
       "  'bitnet.txt',\n",
       "  'blackadd',\n",
       "  'blackapp.hum',\n",
       "  'blackhol.hum',\n",
       "  'blake7.lis',\n",
       "  'blaster.hum',\n",
       "  'bless.bc',\n",
       "  'blkbean.txt',\n",
       "  'blkbnsrc.vgn',\n",
       "  'blood.txt',\n",
       "  'blooprs1.asc',\n",
       "  'bmdn01.txt',\n",
       "  'bnbeg2.4.txt',\n",
       "  'bnbguide.txt',\n",
       "  'bnb_quot.txt',\n",
       "  'boarchil.txt',\n",
       "  'boatmemo.jok',\n",
       "  'boe.hum',\n",
       "  'bond-2.txt',\n",
       "  'boneles2.txt',\n",
       "  'booknuti.txt',\n",
       "  'booze.fun',\n",
       "  'booze1.fun',\n",
       "  'booze2.fun',\n",
       "  'bored.txt',\n",
       "  'boston.geog',\n",
       "  'bozo_tv.leg',\n",
       "  'brainect.hum',\n",
       "  'brdpudd.des',\n",
       "  'bread.rcp',\n",
       "  'bread.rec',\n",
       "  'bread.txt',\n",
       "  'breadpud.des',\n",
       "  'bredcake.des',\n",
       "  'brewing',\n",
       "  'browneco.hum',\n",
       "  'brownie.rec',\n",
       "  'brush1.txt',\n",
       "  'btaco.txt',\n",
       "  'btcisfre.hum',\n",
       "  'btscke01.des',\n",
       "  'btscke02.des',\n",
       "  'btscke03.des',\n",
       "  'btscke04.des',\n",
       "  'btscke05.des',\n",
       "  'buffwing.pol',\n",
       "  'bugbreak.hum',\n",
       "  'bugs.txt',\n",
       "  'buldrwho.txt',\n",
       "  'bunacald.fis',\n",
       "  'burrito.mea',\n",
       "  'butcher.txt',\n",
       "  'butstcod.fis',\n",
       "  'butwrong.hum',\n",
       "  'buzzword.hum',\n",
       "  'bw-phwan.hat',\n",
       "  'bw-summe.hat',\n",
       "  'bw.txt',\n",
       "  'byfb.txt',\n",
       "  'c0dez.txt',\n",
       "  'cabbage.txt',\n",
       "  'caesardr.sal',\n",
       "  'cake.rec',\n",
       "  'calamus.hrb',\n",
       "  'calculus.txt',\n",
       "  'calif.hum',\n",
       "  'calvin.txt',\n",
       "  'cancer.rat',\n",
       "  'candy.txt',\n",
       "  'candybar.fun',\n",
       "  'capital.txt',\n",
       "  'caramels.des',\n",
       "  'carowner.txt',\n",
       "  'cars.txt',\n",
       "  'cartoon.law',\n",
       "  'cartoon.laws',\n",
       "  'cartoon_.txt',\n",
       "  'cartoon_laws.txt',\n",
       "  'cartwb.son',\n",
       "  'cast.lis',\n",
       "  'catballs.hum',\n",
       "  'catin.hat',\n",
       "  'catranch.hum',\n",
       "  'catstory.txt',\n",
       "  'cbmatic.hum',\n",
       "  'cereal.txt',\n",
       "  'cform2.txt',\n",
       "  'cgs_lst.txt',\n",
       "  'chainltr.txt',\n",
       "  'change.hum',\n",
       "  'charity.hum',\n",
       "  'cheapfar.hum',\n",
       "  'cheapin.la',\n",
       "  'chickenheadbbs.txt',\n",
       "  'chickens.jok',\n",
       "  'chickens.txt',\n",
       "  'childhoo.jok',\n",
       "  'childrenbooks.txt',\n",
       "  'chili.txt',\n",
       "  'chinese.txt',\n",
       "  'chinesec.hum',\n",
       "  'choco-ch.ips',\n",
       "  'christop.int',\n",
       "  'chung.iv',\n",
       "  'chunnel.txt',\n",
       "  'church.sto',\n",
       "  'clancy.txt',\n",
       "  'classicm.hum',\n",
       "  'climbing.let',\n",
       "  'cmu.share',\n",
       "  'co-car.jok',\n",
       "  'cockney.alp',\n",
       "  'coffee.faq',\n",
       "  'coffee.txt',\n",
       "  'coffeebeerwomen.txt',\n",
       "  'cogdis.txt',\n",
       "  'coke.fun',\n",
       "  'coke.txt',\n",
       "  'coke1',\n",
       "  'cokeform.txt',\n",
       "  'coke_fan.naz',\n",
       "  'coladrik.fun',\n",
       "  'coladrik.txt',\n",
       "  'cold.fus',\n",
       "  'coldfake.hum',\n",
       "  'collected_quotes.txt',\n",
       "  'college.hum',\n",
       "  'college.sla',\n",
       "  'college.txt',\n",
       "  'comic_st.gui',\n",
       "  'commutin.jok',\n",
       "  'commword.hum',\n",
       "  'computer.txt',\n",
       "  'comrevi1.hum',\n",
       "  'conan.txt',\n",
       "  'confucius_say.txt',\n",
       "  'consp.txt',\n",
       "  'contract.moo',\n",
       "  'cookberk',\n",
       "  'cookbkly.how',\n",
       "  'cookie.1',\n",
       "  'cooking.fun',\n",
       "  'cooking.jok',\n",
       "  'coollngo2.txt',\n",
       "  'cooplaws',\n",
       "  'cops.txt',\n",
       "  'corporat.txt',\n",
       "  'court.quips',\n",
       "  'cowexplo.hum',\n",
       "  'coyote.txt',\n",
       "  'crazy.txt',\n",
       "  'critic.txt',\n",
       "  'crzycred.lst',\n",
       "  'cuchy.hum',\n",
       "  'cucumber.jok',\n",
       "  'cucumber.txt',\n",
       "  'cuisine.txt',\n",
       "  'cultmov.faq',\n",
       "  'curiousgeorgie.txt',\n",
       "  'curry.hrb',\n",
       "  'curry.txt',\n",
       "  'curse.txt',\n",
       "  'cybrtrsh.txt',\n",
       "  'd-ned.hum',\n",
       "  'dalive',\n",
       "  'damiana.hrb',\n",
       "  'dandwine.bev',\n",
       "  'dark.suc',\n",
       "  'dead-r',\n",
       "  'dead2.txt',\n",
       "  'dead3.txt',\n",
       "  'dead4.txt',\n",
       "  'dead5.txt',\n",
       "  'deadlysins.txt',\n",
       "  'deathhem.txt',\n",
       "  'deep.txt',\n",
       "  'defectiv.hum',\n",
       "  'desk.txt',\n",
       "  'deterior.hum',\n",
       "  'devils.jok',\n",
       "  'diesmurf.txt',\n",
       "  'diet.txt',\n",
       "  'dieter.txt',\n",
       "  'dingding.hum',\n",
       "  'dining.out',\n",
       "  'dirtword.txt',\n",
       "  'disaster.hum',\n",
       "  'disclmr.txt',\n",
       "  'disclym.txt',\n",
       "  'doc-says.txt',\n",
       "  'docdict.txt',\n",
       "  'docspeak.txt',\n",
       "  'doggun.sto',\n",
       "  'donut.txt',\n",
       "  'dover.poem',\n",
       "  'draxamus.txt',\n",
       "  'drinker.txt',\n",
       "  'drinking.tro',\n",
       "  'drinkrul.jok',\n",
       "  'drinks.gui',\n",
       "  'drinks.txt',\n",
       "  'drive.txt',\n",
       "  'dromes.txt',\n",
       "  'druggame.hum',\n",
       "  'drugshum.hum',\n",
       "  'drunk.txt',\n",
       "  'dthought.txt',\n",
       "  'dubltalk.jok',\n",
       "  'dym',\n",
       "  'eandb.drx',\n",
       "  'earp',\n",
       "  'eatme.txt',\n",
       "  'econridl.fun',\n",
       "  'egg-bred.txt',\n",
       "  'egglentl.vgn',\n",
       "  'eggroll1.mea',\n",
       "  'electric.txt',\n",
       "  'element.jok',\n",
       "  'elephant.fun',\n",
       "  'elevator.fun',\n",
       "  'empeval.txt',\n",
       "  'engineer.hum',\n",
       "  'english',\n",
       "  'english.txt',\n",
       "  'engmuffn.txt',\n",
       "  'engrhyme.txt',\n",
       "  'enlightenment.txt',\n",
       "  'enquire.hum',\n",
       "  'epikarat.txt',\n",
       "  'epiquest.txt',\n",
       "  'episimp2.txt',\n",
       "  'epitaph',\n",
       "  'epi_.txt',\n",
       "  'epi_bnb.txt',\n",
       "  'epi_merm.txt',\n",
       "  'epi_rns.txt',\n",
       "  'epi_tton.txt',\n",
       "  'eskimo.nel',\n",
       "  'exam.50',\n",
       "  'excuse.txt',\n",
       "  'excuse30.txt',\n",
       "  'excuses.txt',\n",
       "  'exidy.txt',\n",
       "  'exylic.txt',\n",
       "  'facedeth.txt',\n",
       "  'failure.txt',\n",
       "  'fajitas.rcp',\n",
       "  'farsi.phrase',\n",
       "  'farsi.txt',\n",
       "  'fartinfo.txt',\n",
       "  'fartting.txt',\n",
       "  'fascist.txt',\n",
       "  'fbipizza.txt',\n",
       "  'fearcola.hum',\n",
       "  'fed.txt',\n",
       "  'fegg!int.txt',\n",
       "  'feggaqui.txt',\n",
       "  'feggmagi.txt',\n",
       "  'feista01.dip',\n",
       "  'female.jok',\n",
       "  'fiber.txt',\n",
       "  'figure_1.txt',\n",
       "  'filmgoof.txt',\n",
       "  'films_gl.txt',\n",
       "  'final-ex.txt',\n",
       "  'finalexm.hum',\n",
       "  'firecamp.txt',\n",
       "  'fireplacein.txt',\n",
       "  'firstaid.inf',\n",
       "  'firstaid.txt',\n",
       "  'fish.rec',\n",
       "  'flattax.hum',\n",
       "  'flowchrt',\n",
       "  'flowchrt.txt',\n",
       "  'flux_fix.txt',\n",
       "  'focaccia.brd',\n",
       "  'food',\n",
       "  'foodtips',\n",
       "  'footfun.hum',\n",
       "  'forsooth.hum',\n",
       "  'free-cof.fee',\n",
       "  'freshman.hum',\n",
       "  'freudonseuss.txt',\n",
       "  'frogeye1.sal',\n",
       "  'from.hum',\n",
       "  'fuck!.txt',\n",
       "  'fuckyou2.txt',\n",
       "  'fudge.txt',\n",
       "  'fusion.gal',\n",
       "  'fusion.sup',\n",
       "  'fwksfun.hum',\n",
       "  'f_tang.txt',\n",
       "  'gack!.txt',\n",
       "  'gaiahuma',\n",
       "  'gameshow.txt',\n",
       "  'ganamembers.txt',\n",
       "  'garlpast.vgn',\n",
       "  'gas.txt',\n",
       "  'gd_alf.txt',\n",
       "  'gd_drwho.txt',\n",
       "  'gd_flybd.txt',\n",
       "  'gd_frasr.txt',\n",
       "  'gd_gal.txt',\n",
       "  'gd_guide.txt',\n",
       "  'gd_hhead.txt',\n",
       "  'gd_liqtv.txt',\n",
       "  'gd_maxhd.txt',\n",
       "  'gd_ol.txt',\n",
       "  'gd_ql.txt',\n",
       "  'gd_sgrnd.txt',\n",
       "  'gd_tznew.txt',\n",
       "  'german.aut',\n",
       "  'get.drunk.cheap',\n",
       "  'ghostfun.hum',\n",
       "  'ghostsch.hum',\n",
       "  'gingbeer.txt',\n",
       "  'girlspeak.txt',\n",
       "  'godmonth.txt',\n",
       "  'goforth.hum',\n",
       "  'gohome.hum',\n",
       "  'goldwatr.txt',\n",
       "  'golnar.txt',\n",
       "  'good.txt',\n",
       "  'gotukola.hrb',\n",
       "  'gown.txt',\n",
       "  'grail.txt',\n",
       "  'grammar.jok',\n",
       "  'greenchi.txt',\n",
       "  'grommet.hum',\n",
       "  'grospoem.txt',\n",
       "  'growth.txt',\n",
       "  'gumbo.txt',\n",
       "  'hack',\n",
       "  'hack7.txt',\n",
       "  'hackingcracking.txt',\n",
       "  'hackmorality.txt',\n",
       "  'hacktest.txt',\n",
       "  'hamburge.nam',\n",
       "  'hammock.hum',\n",
       "  'hangover.txt',\n",
       "  'happyhack.txt',\n",
       "  'harmful.hum',\n",
       "  'hate.hum',\n",
       "  'hbo_spec.rev',\n",
       "  'headlnrs',\n",
       "  'hecomes.jok',\n",
       "  'hedgehog.txt',\n",
       "  'height.txt',\n",
       "  'hell.jok',\n",
       "  'hell.txt',\n",
       "  'herb!.hum',\n",
       "  'hermsys.txt',\n",
       "  'heroic.txt',\n",
       "  'hi.tec',\n",
       "  'hierarch.txt',\n",
       "  'highland.epi',\n",
       "  'hilbilly.wri',\n",
       "  'history2.oop',\n",
       "  'hitchcoc.app',\n",
       "  'hitchcok.txt',\n",
       "  'hitler.59',\n",
       "  'hitler.txt',\n",
       "  'hitlerap.txt',\n",
       "  'homebrew.txt',\n",
       "  'homermmm.txt',\n",
       "  'hoonsrc.txt',\n",
       "  'hoosier.txt',\n",
       "  'hop.faq',\n",
       "  'horflick.txt',\n",
       "  'horoscop.jok',\n",
       "  'horoscop.txt',\n",
       "  'horoscope.txt',\n",
       "  'hotel.txt',\n",
       "  'hotnnot.hum',\n",
       "  'hotpeper.txt',\n",
       "  'how.bugs.breakd',\n",
       "  'how2bgod.txt',\n",
       "  'how2dotv.txt',\n",
       "  'howlong.hum',\n",
       "  'how_to_i.pro',\n",
       "  'hstlrtxt.txt',\n",
       "  'htswfren.txt',\n",
       "  'hum2',\n",
       "  'humatra.txt',\n",
       "  'humatran.jok',\n",
       "  'humor9.txt',\n",
       "  'humpty.dumpty',\n",
       "  'iced.tea',\n",
       "  'icm.hum',\n",
       "  'idaho.txt',\n",
       "  'idr2.txt',\n",
       "  'imbecile.txt',\n",
       "  'imprrisk.hum',\n",
       "  'impurmat.hum',\n",
       "  'incarhel.hum',\n",
       "  'indgrdn.txt',\n",
       "  'initials.rid',\n",
       "  'inlaws1.txt',\n",
       "  'inquirer.txt',\n",
       "  'ins1',\n",
       "  'insanity.hum',\n",
       "  'insect1.txt',\n",
       "  'insult',\n",
       "  'insult.lst',\n",
       "  'insults1.txt',\n",
       "  'insuranc.sty',\n",
       "  'insure.hum',\n",
       "  'interv.hum',\n",
       "  'investi.hum',\n",
       "  'iqtest',\n",
       "  'iremember',\n",
       "  'is_story.txt',\n",
       "  'italoink.txt',\n",
       "  'ivan.hum',\n",
       "  'jac&tuu.hum',\n",
       "  'jalapast.dip',\n",
       "  'jambalay.pol',\n",
       "  'japantv.txt',\n",
       "  'japice.bev',\n",
       "  'japrap.hum',\n",
       "  'jargon.phd',\n",
       "  'jason.fun',\n",
       "  'jawgumbo.fis',\n",
       "  'jawsalad.fis',\n",
       "  'jayjay.txt',\n",
       "  'jc-elvis.inf',\n",
       "  'jeffie.heh',\n",
       "  'jerky.rcp',\n",
       "  'jimhood.txt',\n",
       "  'johann',\n",
       "  'jokeju07.txt',\n",
       "  'jokes',\n",
       "  'jokes.txt',\n",
       "  'jokes1.txt',\n",
       "  'jon.txt',\n",
       "  'jrrt.riddle',\n",
       "  'jungjuic.bev',\n",
       "  'just2',\n",
       "  'justify',\n",
       "  'kaboom.hum',\n",
       "  'kanalx.txt',\n",
       "  'kashrut.txt',\n",
       "  'kid2',\n",
       "  'kid_diet.txt',\n",
       "  'killer.hum',\n",
       "  'killself.hum',\n",
       "  'kilroy',\n",
       "  'kilsmur.hum',\n",
       "  'kloo.txt',\n",
       "  'koans.txt',\n",
       "  'labels.txt',\n",
       "  'lampoon.jok',\n",
       "  'languag.jok',\n",
       "  'lansing.txt',\n",
       "  'law.sch',\n",
       "  'lawhunt.txt',\n",
       "  'laws.txt',\n",
       "  'lawskool.txt',\n",
       "  'lawsuniv.hum',\n",
       "  'lawyer.jok',\n",
       "  'lawyers.txt',\n",
       "  'lazarus.txt',\n",
       "  'la_times.hun',\n",
       "  'lbinter.hum',\n",
       "  'leech.txt',\n",
       "  'legal.hum',\n",
       "  'let.go',\n",
       "  'letgosh.txt',\n",
       "  'letter.txt',\n",
       "  'letterbx.txt',\n",
       "  'letter_f.sch',\n",
       "  'libraway.txt',\n",
       "  'liceprof.sty',\n",
       "  'lif&love.hum',\n",
       "  'lifeimag.hum',\n",
       "  'lifeinfo.hum',\n",
       "  'lifeonledge.txt',\n",
       "  'limerick.jok',\n",
       "  'lines.jok',\n",
       "  'lion.jok',\n",
       "  'lion.txt',\n",
       "  'lions.cat',\n",
       "  'lipkovits.txt',\n",
       "  'livnware.hum',\n",
       "  'llamas.txt',\n",
       "  'lll.hum',\n",
       "  'llong.hum',\n",
       "  'lobquad.hum',\n",
       "  'looser.hum',\n",
       "  'losers84.hum',\n",
       "  'losers86.hum',\n",
       "  'lost.txt',\n",
       "  'lotsa.jok',\n",
       "  'lozers',\n",
       "  'lozerzon.hum',\n",
       "  'lozeuser.hum',\n",
       "  'lp-assoc.txt',\n",
       "  'lucky.cha',\n",
       "  'ludeinfo.hum',\n",
       "  'ludeinfo.txt',\n",
       "  'luggage.hum',\n",
       "  'luvstory.txt',\n",
       "  'luzerzo2.hum',\n",
       "  'm0dzmen.hum',\n",
       "  'macsfarm.old',\n",
       "  'madhattr.jok',\n",
       "  'madscrib.hum',\n",
       "  'maecenas.hum',\n",
       "  'mailfrag.hum',\n",
       "  'makebeer.hum',\n",
       "  'making_y.wel',\n",
       "  'malechem.txt',\n",
       "  'manager.txt',\n",
       "  'manilla.hum',\n",
       "  'manners.txt',\n",
       "  'manspace.hum',\n",
       "  'margos.txt',\n",
       "  'marines.hum',\n",
       "  'marriage.hum',\n",
       "  'mash.hum',\n",
       "  'math.1',\n",
       "  'math.2',\n",
       "  'math.far',\n",
       "  'maxheadr',\n",
       "  'mcd.txt',\n",
       "  'mead.rcp',\n",
       "  'meat2.txt',\n",
       "  'meinkamp.hum',\n",
       "  'mel.txt',\n",
       "  'melodram.hum',\n",
       "  'memo.hum',\n",
       "  'memory.hum',\n",
       "  'men&wome.txt',\n",
       "  'mensroom.jok',\n",
       "  'merry.txt',\n",
       "  'miamadvi.hum',\n",
       "  'miami.hum',\n",
       "  'miamimth.txt',\n",
       "  'middle.age',\n",
       "  'mindvox',\n",
       "  'minn.txt',\n",
       "  'miranda.hum',\n",
       "  'misc.1',\n",
       "  'misery.hum',\n",
       "  'missdish',\n",
       "  'missheav.hum',\n",
       "  'mitch.txt',\n",
       "  'mlverb.hum',\n",
       "  'modemwld.txt',\n",
       "  'modest.hum',\n",
       "  'modstup',\n",
       "  'mog-history',\n",
       "  'montoys.txt',\n",
       "  'montpyth.hum',\n",
       "  'moonshin',\n",
       "  'moore.txt',\n",
       "  'moose.txt',\n",
       "  'moslem.txt',\n",
       "  'mothers.txt',\n",
       "  'motrbike.jok',\n",
       "  'mov_rail.txt',\n",
       "  'mowers.txt',\n",
       "  'mr.rogers',\n",
       "  'mrscienc.hum',\n",
       "  'mrsfield',\n",
       "  'msfields.txt',\n",
       "  'msorrow',\n",
       "  'mtm.hum',\n",
       "  'mtv.asc',\n",
       "  'mundane.v2',\n",
       "  'murph.jok',\n",
       "  'murphy.txt',\n",
       "  'murphys.txt',\n",
       "  'murphy_l.txt',\n",
       "  'mutate.hum',\n",
       "  'mydaywss.hum',\n",
       "  'myheart.hum',\n",
       "  'naivewiz.hum',\n",
       "  'namaste.txt',\n",
       "  'nameisreo.txt',\n",
       "  'namm',\n",
       "  'nasaglenn.txt',\n",
       "  'necropls.txt',\n",
       "  'netmask.txt',\n",
       "  'netnews.10',\n",
       "  'newcoke.txt',\n",
       "  'newconst.hum',\n",
       "  'newmex.hum',\n",
       "  'news.hum',\n",
       "  'nigel.1',\n",
       "  'nigel.10',\n",
       "  'nigel.2',\n",
       "  'nigel.3',\n",
       "  'nigel.4',\n",
       "  'nigel.5',\n",
       "  'nigel.6',\n",
       "  'nigel.7',\n",
       "  'nigel10.txt',\n",
       "  'nihgel_8.9',\n",
       "  'nintendo.jok',\n",
       "  'normal.boy',\n",
       "  'normalboy.txt',\n",
       "  'normquot.txt',\n",
       "  'nosuch_nasfic',\n",
       "  'novel.hum',\n",
       "  'nuke.hum',\n",
       "  'nukeplay.hum',\n",
       "  'nukewar.jok',\n",
       "  'nukewar.txt',\n",
       "  'nukwaste',\n",
       "  'number',\n",
       "  'number.killer',\n",
       "  'number_k.ill',\n",
       "  'nurds.hum',\n",
       "  'nysucks.hum',\n",
       "  'nzdrinks.txt',\n",
       "  'o-ttalk.hum',\n",
       "  'oakwood.txt',\n",
       "  'oam-001.txt',\n",
       "  'oam.nfo',\n",
       "  'oasis',\n",
       "  'oatbran.rec',\n",
       "  'oculis.rcp',\n",
       "  'odd_to.obs',\n",
       "  'odearakk.hum',\n",
       "  'office.txt',\n",
       "  'ohandre.hum',\n",
       "  'oilgluts.hum',\n",
       "  'old.txt',\n",
       "  'oldeng.hum',\n",
       "  'oldtime.sng',\n",
       "  'oldtime.txt',\n",
       "  'oliver.txt',\n",
       "  'oliver02.txt',\n",
       "  'onan.txt',\n",
       "  'one.par',\n",
       "  'onetoone.hum',\n",
       "  'onetotwo.hum',\n",
       "  'ookpik.hum',\n",
       "  'opinion.hum',\n",
       "  'oracle.jok',\n",
       "  'oranchic.pol',\n",
       "  'orgfrost.bev',\n",
       "  'ourfathr.txt',\n",
       "  'outawork.erl',\n",
       "  'outlimit.txt',\n",
       "  'oxymoron.jok',\n",
       "  'oxymoron.txt',\n",
       "  'ozarks.hum',\n",
       "  'p-law.hum',\n",
       "  'packard.txt',\n",
       "  'paddingurpapers.txt',\n",
       "  'parabl.hum',\n",
       "  'parades.hum',\n",
       "  'parsnip.txt',\n",
       "  'passage.hum',\n",
       "  'passenge.sim',\n",
       "  'pasta001.sal',\n",
       "  'pat.txt',\n",
       "  'pbcookie.des',\n",
       "  'peanuts.txt',\n",
       "  'peatchp.hum',\n",
       "  'pecker.txt',\n",
       "  'penisprt.txt',\n",
       "  'penndtch',\n",
       "  'pepper.txt',\n",
       "  'pepsideg.txt',\n",
       "  'petshop',\n",
       "  'phony.hum',\n",
       "  'phorse.hum',\n",
       "  'phunatdi.ana',\n",
       "  'phxbbs-m.txt',\n",
       "  'pickup.lin',\n",
       "  'pickup.txt',\n",
       "  'pipespec.txt',\n",
       "  'pizzawho.hum',\n",
       "  'planeget.hum',\n",
       "  'planetzero.txt',\n",
       "  'poets.hum',\n",
       "  'pol-corr.txt',\n",
       "  'polemom.txt',\n",
       "  'poli.tics',\n",
       "  'policpig.hum',\n",
       "  'poli_t.ics',\n",
       "  'poll2res.hum',\n",
       "  'polly.txt',\n",
       "  'polly_.new',\n",
       "  'poopie.txt',\n",
       "  'popconc.hum',\n",
       "  'popmach',\n",
       "  'popmusi.hum',\n",
       "  'post.nuc',\n",
       "  'pot.txt',\n",
       "  'potty.txt',\n",
       "  'pournell.spo',\n",
       "  'ppbeer.txt',\n",
       "  'prac1.jok',\n",
       "  'prac2.jok',\n",
       "  'prac3.jok',\n",
       "  'prac4.jok',\n",
       "  'pracjoke.txt',\n",
       "  'practica.txt',\n",
       "  'prawblim.hum',\n",
       "  'prayer.hum',\n",
       "  'primes.jok',\n",
       "  'princess.brd',\n",
       "  'pro-fact.hum',\n",
       "  'problem.txt',\n",
       "  'progrs.gph',\n",
       "  'proof.met',\n",
       "  'prooftec.txt',\n",
       "  'proposal.jok',\n",
       "  'proudlyserve.txt',\n",
       "  'prover.wisom',\n",
       "  'prover_w.iso',\n",
       "  'psalm.reagan',\n",
       "  'psalm23.txt',\n",
       "  'psalm_nixon',\n",
       "  'psalm_re.aga',\n",
       "  'psilaine.hum',\n",
       "  'psycho.txt',\n",
       "  'psych_pr.quo',\n",
       "  'pukeprom.jok',\n",
       "  'pun.txt',\n",
       "  'pure.mat',\n",
       "  'puzzle.spo',\n",
       "  'puzzles.jok',\n",
       "  'python_s.ong',\n",
       "  'q.pun',\n",
       "  'qttofu.vgn',\n",
       "  'quack26.txt',\n",
       "  'quantity.001',\n",
       "  'quantum.jok',\n",
       "  'quantum.phy',\n",
       "  'quest.hum',\n",
       "  'quick.jok',\n",
       "  'quotes.bug',\n",
       "  'quotes.jok',\n",
       "  'quotes.txt',\n",
       "  'quux_p.oem',\n",
       "  'rabbit.txt',\n",
       "  'racist.net',\n",
       "  'radexposed.txt',\n",
       "  'radiolaf.hum',\n",
       "  'rapmastr.hum',\n",
       "  'ratings.hum',\n",
       "  'ratspit.hum',\n",
       "  'raven.hum',\n",
       "  'readme.bat',\n",
       "  'reagan.hum',\n",
       "  'realest.txt',\n",
       "  'reasons.txt',\n",
       "  'rec.por',\n",
       "  'recepies.fun',\n",
       "  'recip1.txt',\n",
       "  'recipe.001',\n",
       "  'recipe.002',\n",
       "  'recipe.003',\n",
       "  'recipe.004',\n",
       "  'recipe.005',\n",
       "  'recipe.006',\n",
       "  'recipe.007',\n",
       "  'recipe.008',\n",
       "  'recipe.009',\n",
       "  'recipe.010',\n",
       "  'recipe.011',\n",
       "  'recipe.012',\n",
       "  'reconcil.hum',\n",
       "  'record_.gap',\n",
       "  'red-neck.jks',\n",
       "  'reddwarf.sng',\n",
       "  'reddye.hum',\n",
       "  'rednecks.txt',\n",
       "  'reeves.txt',\n",
       "  'relative.ada',\n",
       "  'religion.txt',\n",
       "  'renored.txt',\n",
       "  'renorthr.txt',\n",
       "  'rent-a_cat',\n",
       "  'rentals.hum',\n",
       "  'repair.hum',\n",
       "  'report.hum',\n",
       "  'research.hum',\n",
       "  'residncy.jok',\n",
       "  'resolutn.txt',\n",
       "  'resrch_p.hra',\n",
       "  'resrch_phrase',\n",
       "  'revolt.dj',\n",
       "  'richbred.txt',\n",
       "  'rinaldo.jok',\n",
       "  'rinaldos.law',\n",
       "  'rinaldos.txt',\n",
       "  'ripoffpc.hum',\n",
       "  'rns_bcl.txt',\n",
       "  'rns_bwl.txt',\n",
       "  'rns_ency.txt',\n",
       "  'roach.asc',\n",
       "  'roadpizz.txt',\n",
       "  'robot.tes',\n",
       "  'rocking.hum',\n",
       "  'rockmus.hum',\n",
       "  'sanshop.txt',\n",
       "  'saveface.hum',\n",
       "  'sawyer.txt',\n",
       "  'scam.txt',\n",
       "  'scratchy.txt',\n",
       "  'seafood.txt',\n",
       "  'seeds42.txt',\n",
       "  'sf-zine.pub',\n",
       "  'sfmovie.txt',\n",
       "  'shameonu.hum',\n",
       "  'shooters.txt',\n",
       "  'shorties.jok',\n",
       "  'shrink.news',\n",
       "  'shuimai.txt',\n",
       "  'shuttleb.hum',\n",
       "  'signatur.jok',\n",
       "  'sigs.txt',\n",
       "  'silverclaws.txt',\n",
       "  'simp.txt',\n",
       "  'sinksub.txt',\n",
       "  'skincat',\n",
       "  'skippy.hum',\n",
       "  'skippy.txt',\n",
       "  'slogans.txt',\n",
       "  'smackjok.hum',\n",
       "  'smartass.txt',\n",
       "  'smiley.txt',\n",
       "  'smokers.txt',\n",
       "  'smurf-03.txt',\n",
       "  'smurfkil.hum',\n",
       "  'smurfs.cc',\n",
       "  'smurf_co.txt',\n",
       "  'snapple.rum',\n",
       "  'snipe.txt',\n",
       "  'soccer.txt',\n",
       "  'socecon.hum',\n",
       "  'social.hum',\n",
       "  'socks.drx',\n",
       "  'solders.hum',\n",
       "  'soleleer.hum',\n",
       "  'solviets.hum',\n",
       "  'some_hu.mor',\n",
       "  'soporifi.abs',\n",
       "  'sorority.gir',\n",
       "  'spacever.hum',\n",
       "  'speling.msk',\n",
       "  'spelin_r.ifo',\n",
       "  'spider.hum',\n",
       "  'spoonlis.txt',\n",
       "  'spydust.hum',\n",
       "  'squids.gph',\n",
       "  'staff.txt',\n",
       "  'stagline.txt',\n",
       "  'standard.hum',\n",
       "  'startrek.txt',\n",
       "  'stereo.txt',\n",
       "  'steroid.txt',\n",
       "  'stone.hum',\n",
       "  'strattma.txt',\n",
       "  'stressman.txt',\n",
       "  'strine.txt',\n",
       "  'strsdiet.txt',\n",
       "  'studentb.txt',\n",
       "  'stuf10.txt',\n",
       "  'stuf11.txt',\n",
       "  'st_silic.txt',\n",
       "  'subb_lis.txt',\n",
       "  'subrdead.hum',\n",
       "  'suicide2.txt',\n",
       "  'sungenu.hum',\n",
       "  'supermar.rul',\n",
       "  'swearfrn.hum',\n",
       "  'sw_err.txt',\n",
       "  'symbol.hum',\n",
       "  'sysadmin.txt',\n",
       "  'sysman.txt',\n",
       "  't-10.hum',\n",
       "  't-shirt.hum',\n",
       "  'takenote.jok',\n",
       "  'talebeat.hum',\n",
       "  'talkbizr.txt',\n",
       "  'taping.hum',\n",
       "  'tarot.txt',\n",
       "  'teens.txt',\n",
       "  ...],\n",
       " [0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  84,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  93,\n",
       "  94,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  100,\n",
       "  101,\n",
       "  102,\n",
       "  103,\n",
       "  104,\n",
       "  105,\n",
       "  106,\n",
       "  107,\n",
       "  108,\n",
       "  109,\n",
       "  110,\n",
       "  111,\n",
       "  112,\n",
       "  113,\n",
       "  114,\n",
       "  115,\n",
       "  116,\n",
       "  117,\n",
       "  118,\n",
       "  119,\n",
       "  120,\n",
       "  121,\n",
       "  122,\n",
       "  123,\n",
       "  124,\n",
       "  125,\n",
       "  126,\n",
       "  127,\n",
       "  128,\n",
       "  129,\n",
       "  130,\n",
       "  131,\n",
       "  132,\n",
       "  133,\n",
       "  134,\n",
       "  135,\n",
       "  136,\n",
       "  137,\n",
       "  138,\n",
       "  139,\n",
       "  140,\n",
       "  141,\n",
       "  142,\n",
       "  143,\n",
       "  144,\n",
       "  145,\n",
       "  146,\n",
       "  147,\n",
       "  148,\n",
       "  149,\n",
       "  150,\n",
       "  151,\n",
       "  152,\n",
       "  153,\n",
       "  154,\n",
       "  155,\n",
       "  156,\n",
       "  157,\n",
       "  158,\n",
       "  159,\n",
       "  160,\n",
       "  161,\n",
       "  162,\n",
       "  163,\n",
       "  164,\n",
       "  165,\n",
       "  166,\n",
       "  167,\n",
       "  168,\n",
       "  169,\n",
       "  170,\n",
       "  171,\n",
       "  172,\n",
       "  173,\n",
       "  174,\n",
       "  175,\n",
       "  176,\n",
       "  177,\n",
       "  178,\n",
       "  179,\n",
       "  180,\n",
       "  181,\n",
       "  182,\n",
       "  183,\n",
       "  184,\n",
       "  185,\n",
       "  186,\n",
       "  187,\n",
       "  188,\n",
       "  189,\n",
       "  190,\n",
       "  191,\n",
       "  192,\n",
       "  193,\n",
       "  194,\n",
       "  195,\n",
       "  196,\n",
       "  197,\n",
       "  198,\n",
       "  199,\n",
       "  200,\n",
       "  201,\n",
       "  202,\n",
       "  203,\n",
       "  204,\n",
       "  205,\n",
       "  206,\n",
       "  207,\n",
       "  208,\n",
       "  209,\n",
       "  210,\n",
       "  211,\n",
       "  212,\n",
       "  213,\n",
       "  214,\n",
       "  215,\n",
       "  216,\n",
       "  217,\n",
       "  218,\n",
       "  219,\n",
       "  220,\n",
       "  221,\n",
       "  222,\n",
       "  223,\n",
       "  224,\n",
       "  225,\n",
       "  226,\n",
       "  227,\n",
       "  228,\n",
       "  229,\n",
       "  230,\n",
       "  231,\n",
       "  232,\n",
       "  233,\n",
       "  234,\n",
       "  235,\n",
       "  236,\n",
       "  237,\n",
       "  238,\n",
       "  239,\n",
       "  240,\n",
       "  241,\n",
       "  242,\n",
       "  243,\n",
       "  244,\n",
       "  245,\n",
       "  246,\n",
       "  247,\n",
       "  248,\n",
       "  249,\n",
       "  250,\n",
       "  251,\n",
       "  252,\n",
       "  253,\n",
       "  254,\n",
       "  255,\n",
       "  256,\n",
       "  257,\n",
       "  258,\n",
       "  259,\n",
       "  260,\n",
       "  261,\n",
       "  262,\n",
       "  263,\n",
       "  264,\n",
       "  265,\n",
       "  266,\n",
       "  267,\n",
       "  268,\n",
       "  269,\n",
       "  270,\n",
       "  271,\n",
       "  272,\n",
       "  273,\n",
       "  274,\n",
       "  275,\n",
       "  276,\n",
       "  277,\n",
       "  278,\n",
       "  279,\n",
       "  280,\n",
       "  281,\n",
       "  282,\n",
       "  283,\n",
       "  284,\n",
       "  285,\n",
       "  286,\n",
       "  287,\n",
       "  288,\n",
       "  289,\n",
       "  290,\n",
       "  291,\n",
       "  292,\n",
       "  293,\n",
       "  294,\n",
       "  295,\n",
       "  296,\n",
       "  297,\n",
       "  298,\n",
       "  299,\n",
       "  300,\n",
       "  301,\n",
       "  302,\n",
       "  303,\n",
       "  304,\n",
       "  305,\n",
       "  306,\n",
       "  307,\n",
       "  308,\n",
       "  309,\n",
       "  310,\n",
       "  311,\n",
       "  312,\n",
       "  313,\n",
       "  314,\n",
       "  315,\n",
       "  316,\n",
       "  317,\n",
       "  318,\n",
       "  319,\n",
       "  320,\n",
       "  321,\n",
       "  322,\n",
       "  323,\n",
       "  324,\n",
       "  325,\n",
       "  326,\n",
       "  327,\n",
       "  328,\n",
       "  329,\n",
       "  330,\n",
       "  331,\n",
       "  332,\n",
       "  333,\n",
       "  334,\n",
       "  335,\n",
       "  336,\n",
       "  337,\n",
       "  338,\n",
       "  339,\n",
       "  340,\n",
       "  341,\n",
       "  342,\n",
       "  343,\n",
       "  344,\n",
       "  345,\n",
       "  346,\n",
       "  347,\n",
       "  348,\n",
       "  349,\n",
       "  350,\n",
       "  351,\n",
       "  352,\n",
       "  353,\n",
       "  354,\n",
       "  355,\n",
       "  356,\n",
       "  357,\n",
       "  358,\n",
       "  359,\n",
       "  360,\n",
       "  361,\n",
       "  362,\n",
       "  363,\n",
       "  364,\n",
       "  365,\n",
       "  366,\n",
       "  367,\n",
       "  368,\n",
       "  369,\n",
       "  370,\n",
       "  371,\n",
       "  372,\n",
       "  373,\n",
       "  374,\n",
       "  375,\n",
       "  376,\n",
       "  377,\n",
       "  378,\n",
       "  379,\n",
       "  380,\n",
       "  381,\n",
       "  382,\n",
       "  383,\n",
       "  384,\n",
       "  385,\n",
       "  386,\n",
       "  387,\n",
       "  388,\n",
       "  389,\n",
       "  390,\n",
       "  391,\n",
       "  392,\n",
       "  393,\n",
       "  394,\n",
       "  395,\n",
       "  396,\n",
       "  397,\n",
       "  398,\n",
       "  399,\n",
       "  400,\n",
       "  401,\n",
       "  402,\n",
       "  403,\n",
       "  404,\n",
       "  405,\n",
       "  406,\n",
       "  407,\n",
       "  408,\n",
       "  409,\n",
       "  410,\n",
       "  411,\n",
       "  412,\n",
       "  413,\n",
       "  414,\n",
       "  415,\n",
       "  416,\n",
       "  417,\n",
       "  418,\n",
       "  419,\n",
       "  420,\n",
       "  421,\n",
       "  422,\n",
       "  423,\n",
       "  424,\n",
       "  425,\n",
       "  426,\n",
       "  427,\n",
       "  428,\n",
       "  429,\n",
       "  430,\n",
       "  431,\n",
       "  432,\n",
       "  433,\n",
       "  434,\n",
       "  435,\n",
       "  436,\n",
       "  437,\n",
       "  438,\n",
       "  439,\n",
       "  440,\n",
       "  441,\n",
       "  442,\n",
       "  443,\n",
       "  444,\n",
       "  445,\n",
       "  446,\n",
       "  447,\n",
       "  448,\n",
       "  449,\n",
       "  450,\n",
       "  451,\n",
       "  452,\n",
       "  453,\n",
       "  454,\n",
       "  455,\n",
       "  456,\n",
       "  457,\n",
       "  458,\n",
       "  459,\n",
       "  460,\n",
       "  461,\n",
       "  462,\n",
       "  463,\n",
       "  464,\n",
       "  465,\n",
       "  466,\n",
       "  467,\n",
       "  468,\n",
       "  469,\n",
       "  470,\n",
       "  471,\n",
       "  472,\n",
       "  473,\n",
       "  474,\n",
       "  475,\n",
       "  476,\n",
       "  477,\n",
       "  478,\n",
       "  479,\n",
       "  480,\n",
       "  481,\n",
       "  482,\n",
       "  483,\n",
       "  484,\n",
       "  485,\n",
       "  486,\n",
       "  487,\n",
       "  488,\n",
       "  489,\n",
       "  490,\n",
       "  491,\n",
       "  492,\n",
       "  493,\n",
       "  494,\n",
       "  495,\n",
       "  496,\n",
       "  497,\n",
       "  498,\n",
       "  499,\n",
       "  500,\n",
       "  501,\n",
       "  502,\n",
       "  503,\n",
       "  504,\n",
       "  505,\n",
       "  506,\n",
       "  507,\n",
       "  508,\n",
       "  509,\n",
       "  510,\n",
       "  511,\n",
       "  512,\n",
       "  513,\n",
       "  514,\n",
       "  515,\n",
       "  516,\n",
       "  517,\n",
       "  518,\n",
       "  519,\n",
       "  520,\n",
       "  521,\n",
       "  522,\n",
       "  523,\n",
       "  524,\n",
       "  525,\n",
       "  526,\n",
       "  527,\n",
       "  528,\n",
       "  529,\n",
       "  530,\n",
       "  531,\n",
       "  532,\n",
       "  533,\n",
       "  534,\n",
       "  535,\n",
       "  536,\n",
       "  537,\n",
       "  538,\n",
       "  539,\n",
       "  540,\n",
       "  541,\n",
       "  542,\n",
       "  543,\n",
       "  544,\n",
       "  545,\n",
       "  546,\n",
       "  547,\n",
       "  548,\n",
       "  549,\n",
       "  550,\n",
       "  551,\n",
       "  552,\n",
       "  553,\n",
       "  554,\n",
       "  555,\n",
       "  556,\n",
       "  557,\n",
       "  558,\n",
       "  559,\n",
       "  560,\n",
       "  561,\n",
       "  562,\n",
       "  563,\n",
       "  564,\n",
       "  565,\n",
       "  566,\n",
       "  567,\n",
       "  568,\n",
       "  569,\n",
       "  570,\n",
       "  571,\n",
       "  572,\n",
       "  573,\n",
       "  574,\n",
       "  575,\n",
       "  576,\n",
       "  577,\n",
       "  578,\n",
       "  579,\n",
       "  580,\n",
       "  581,\n",
       "  582,\n",
       "  583,\n",
       "  584,\n",
       "  585,\n",
       "  586,\n",
       "  587,\n",
       "  588,\n",
       "  589,\n",
       "  590,\n",
       "  591,\n",
       "  592,\n",
       "  593,\n",
       "  594,\n",
       "  595,\n",
       "  596,\n",
       "  597,\n",
       "  598,\n",
       "  599,\n",
       "  600,\n",
       "  601,\n",
       "  602,\n",
       "  603,\n",
       "  604,\n",
       "  605,\n",
       "  606,\n",
       "  607,\n",
       "  608,\n",
       "  609,\n",
       "  610,\n",
       "  611,\n",
       "  612,\n",
       "  613,\n",
       "  614,\n",
       "  615,\n",
       "  616,\n",
       "  617,\n",
       "  618,\n",
       "  619,\n",
       "  620,\n",
       "  621,\n",
       "  622,\n",
       "  623,\n",
       "  624,\n",
       "  625,\n",
       "  626,\n",
       "  627,\n",
       "  628,\n",
       "  629,\n",
       "  630,\n",
       "  631,\n",
       "  632,\n",
       "  633,\n",
       "  634,\n",
       "  635,\n",
       "  636,\n",
       "  637,\n",
       "  638,\n",
       "  639,\n",
       "  640,\n",
       "  641,\n",
       "  642,\n",
       "  643,\n",
       "  644,\n",
       "  645,\n",
       "  646,\n",
       "  647,\n",
       "  648,\n",
       "  649,\n",
       "  650,\n",
       "  651,\n",
       "  652,\n",
       "  653,\n",
       "  654,\n",
       "  655,\n",
       "  656,\n",
       "  657,\n",
       "  658,\n",
       "  659,\n",
       "  660,\n",
       "  661,\n",
       "  662,\n",
       "  663,\n",
       "  664,\n",
       "  665,\n",
       "  666,\n",
       "  667,\n",
       "  668,\n",
       "  669,\n",
       "  670,\n",
       "  671,\n",
       "  672,\n",
       "  673,\n",
       "  674,\n",
       "  675,\n",
       "  676,\n",
       "  677,\n",
       "  678,\n",
       "  679,\n",
       "  680,\n",
       "  681,\n",
       "  682,\n",
       "  683,\n",
       "  684,\n",
       "  685,\n",
       "  686,\n",
       "  687,\n",
       "  688,\n",
       "  689,\n",
       "  690,\n",
       "  691,\n",
       "  692,\n",
       "  693,\n",
       "  694,\n",
       "  695,\n",
       "  696,\n",
       "  697,\n",
       "  698,\n",
       "  699,\n",
       "  700,\n",
       "  701,\n",
       "  702,\n",
       "  703,\n",
       "  704,\n",
       "  705,\n",
       "  706,\n",
       "  707,\n",
       "  708,\n",
       "  709,\n",
       "  710,\n",
       "  711,\n",
       "  712,\n",
       "  713,\n",
       "  714,\n",
       "  715,\n",
       "  716,\n",
       "  717,\n",
       "  718,\n",
       "  719,\n",
       "  720,\n",
       "  721,\n",
       "  722,\n",
       "  723,\n",
       "  724,\n",
       "  725,\n",
       "  726,\n",
       "  727,\n",
       "  728,\n",
       "  729,\n",
       "  730,\n",
       "  731,\n",
       "  732,\n",
       "  733,\n",
       "  734,\n",
       "  735,\n",
       "  736,\n",
       "  737,\n",
       "  738,\n",
       "  739,\n",
       "  740,\n",
       "  741,\n",
       "  742,\n",
       "  743,\n",
       "  744,\n",
       "  745,\n",
       "  746,\n",
       "  747,\n",
       "  748,\n",
       "  749,\n",
       "  750,\n",
       "  751,\n",
       "  752,\n",
       "  753,\n",
       "  754,\n",
       "  755,\n",
       "  756,\n",
       "  757,\n",
       "  758,\n",
       "  759,\n",
       "  760,\n",
       "  761,\n",
       "  762,\n",
       "  763,\n",
       "  764,\n",
       "  765,\n",
       "  766,\n",
       "  767,\n",
       "  768,\n",
       "  769,\n",
       "  770,\n",
       "  771,\n",
       "  772,\n",
       "  773,\n",
       "  774,\n",
       "  775,\n",
       "  776,\n",
       "  777,\n",
       "  778,\n",
       "  779,\n",
       "  780,\n",
       "  781,\n",
       "  782,\n",
       "  783,\n",
       "  784,\n",
       "  785,\n",
       "  786,\n",
       "  787,\n",
       "  788,\n",
       "  789,\n",
       "  790,\n",
       "  791,\n",
       "  792,\n",
       "  793,\n",
       "  794,\n",
       "  795,\n",
       "  796,\n",
       "  797,\n",
       "  798,\n",
       "  799,\n",
       "  800,\n",
       "  801,\n",
       "  802,\n",
       "  803,\n",
       "  804,\n",
       "  805,\n",
       "  806,\n",
       "  807,\n",
       "  808,\n",
       "  809,\n",
       "  810,\n",
       "  811,\n",
       "  812,\n",
       "  813,\n",
       "  814,\n",
       "  815,\n",
       "  816,\n",
       "  817,\n",
       "  818,\n",
       "  819,\n",
       "  820,\n",
       "  821,\n",
       "  822,\n",
       "  823,\n",
       "  824,\n",
       "  825,\n",
       "  826,\n",
       "  827,\n",
       "  828,\n",
       "  829,\n",
       "  830,\n",
       "  831,\n",
       "  832,\n",
       "  833,\n",
       "  834,\n",
       "  835,\n",
       "  836,\n",
       "  837,\n",
       "  838,\n",
       "  839,\n",
       "  840,\n",
       "  841,\n",
       "  842,\n",
       "  843,\n",
       "  844,\n",
       "  845,\n",
       "  846,\n",
       "  847,\n",
       "  848,\n",
       "  849,\n",
       "  850,\n",
       "  851,\n",
       "  852,\n",
       "  853,\n",
       "  854,\n",
       "  855,\n",
       "  856,\n",
       "  857,\n",
       "  858,\n",
       "  859,\n",
       "  860,\n",
       "  861,\n",
       "  862,\n",
       "  863,\n",
       "  864,\n",
       "  865,\n",
       "  866,\n",
       "  867,\n",
       "  868,\n",
       "  869,\n",
       "  870,\n",
       "  871,\n",
       "  872,\n",
       "  873,\n",
       "  874,\n",
       "  875,\n",
       "  876,\n",
       "  877,\n",
       "  878,\n",
       "  879,\n",
       "  880,\n",
       "  881,\n",
       "  882,\n",
       "  883,\n",
       "  884,\n",
       "  885,\n",
       "  886,\n",
       "  887,\n",
       "  888,\n",
       "  889,\n",
       "  890,\n",
       "  891,\n",
       "  892,\n",
       "  893,\n",
       "  894,\n",
       "  895,\n",
       "  896,\n",
       "  897,\n",
       "  898,\n",
       "  899,\n",
       "  900,\n",
       "  901,\n",
       "  902,\n",
       "  903,\n",
       "  904,\n",
       "  905,\n",
       "  906,\n",
       "  907,\n",
       "  908,\n",
       "  909,\n",
       "  910,\n",
       "  911,\n",
       "  912,\n",
       "  913,\n",
       "  914,\n",
       "  915,\n",
       "  916,\n",
       "  917,\n",
       "  918,\n",
       "  919,\n",
       "  920,\n",
       "  921,\n",
       "  922,\n",
       "  923,\n",
       "  924,\n",
       "  925,\n",
       "  926,\n",
       "  927,\n",
       "  928,\n",
       "  929,\n",
       "  930,\n",
       "  931,\n",
       "  932,\n",
       "  933,\n",
       "  934,\n",
       "  935,\n",
       "  936,\n",
       "  937,\n",
       "  938,\n",
       "  939,\n",
       "  940,\n",
       "  941,\n",
       "  942,\n",
       "  943,\n",
       "  944,\n",
       "  945,\n",
       "  946,\n",
       "  947,\n",
       "  948,\n",
       "  949,\n",
       "  950,\n",
       "  951,\n",
       "  952,\n",
       "  953,\n",
       "  954,\n",
       "  955,\n",
       "  956,\n",
       "  957,\n",
       "  958,\n",
       "  959,\n",
       "  960,\n",
       "  961,\n",
       "  962,\n",
       "  963,\n",
       "  964,\n",
       "  965,\n",
       "  966,\n",
       "  967,\n",
       "  968,\n",
       "  969,\n",
       "  970,\n",
       "  971,\n",
       "  972,\n",
       "  973,\n",
       "  974,\n",
       "  975,\n",
       "  976,\n",
       "  977,\n",
       "  978,\n",
       "  979,\n",
       "  980,\n",
       "  981,\n",
       "  982,\n",
       "  983,\n",
       "  984,\n",
       "  985,\n",
       "  986,\n",
       "  987,\n",
       "  988,\n",
       "  989,\n",
       "  990,\n",
       "  991,\n",
       "  992,\n",
       "  993,\n",
       "  994,\n",
       "  995,\n",
       "  996,\n",
       "  997,\n",
       "  998,\n",
       "  999,\n",
       "  1000,\n",
       "  ...])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [0, 1, 2, 3, 4, 5]\n",
    "b = [0, 2, 3, 4, 6]\n",
    "\n",
    "query_OR_NOT(a, b, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ops(ops_string):\n",
    "    ops = ops_string.split(\",\")\n",
    "    ops = [op.strip() for op in ops]\n",
    "    for op in ops:\n",
    "        if(not(op == \"AND\" or op == \"OR\" or op == \"OR NOT\" or op == \"AND NOT\")):\n",
    "            return []\n",
    "    return ops\n",
    "\n",
    "def perform_query(posting1, posting2, op):\n",
    "    if(op == \"AND\"):\n",
    "        num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID = query_AND(posting1, posting2)\n",
    "        return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID\n",
    "    elif(op == \"OR\"):\n",
    "        num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID = query_OR(posting1, posting2)\n",
    "        return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID\n",
    "    elif(op == \"AND NOT\"):\n",
    "        num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID = query_AND_NOT(posting1, posting2)\n",
    "        return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID\n",
    "    elif(op == \"OR NOT\"):\n",
    "        num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID = query_OR_NOT(posting1, posting2)\n",
    "        return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID\n",
    "    else:\n",
    "        print(\"INVALID OPERATOR\")\n",
    "        return []\n",
    "\n",
    "def process_query(inv_index, query_tokens, ops):\n",
    "    # query_tokens = preprocess_query(query_text)\n",
    "    print(f\"Query tokens : {query_tokens}\\n\")\n",
    "    print(f\"Operators : \")\n",
    "    n_qtoks = len(query_tokens)\n",
    "    n_ops = n_qtoks - 1\n",
    "    # query_operato = copy.deepcopy(ops)\n",
    "    if(len(ops) > n_ops):\n",
    "        print(f\"Insufficient tokens in the query\\n\")\n",
    "        return -1, -1, []\n",
    "    elif(len(ops) < n_ops):\n",
    "        print(f\"Insufficient operators\")\n",
    "        return -1, -1, []\n",
    "    else:\n",
    "        total_comparisons = 0\n",
    "        if(n_ops == 1):\n",
    "            p1 = retreive_postings(inv_index, query_tokens[0])\n",
    "            p2 = retreive_postings(inv_index,query_tokens[1])\n",
    "            num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID = perform_query(p1, p2, ops[0])\n",
    "            return num_docs_retreived, num_comparisons, doc_names_retreived\n",
    "        else:\n",
    "            p1 = retreive_postings(inv_index, query_tokens[0])\n",
    "            p2 = retreive_postings(inv_index, query_tokens[1])\n",
    "            ndocs, total_comparisons, doc_names, ans = perform_query(p1, p2, ops[0])\n",
    "            qtok_ptr = 2\n",
    "            for i in range(1, len(ops)):\n",
    "                posting_tok = retreive_postings(inv_index, query_tokens[qtok_ptr])\n",
    "                ndocs, cmps, doc_names, ans = perform_query(ans, posting_tok, ops[i])\n",
    "                total_comparisons += cmps\n",
    "                qtok_ptr += 1\n",
    "            num_docs_retreived = len(ans)\n",
    "            doc_names_retreived = getDocsFromID(file_names, ans)\n",
    "            return num_docs_retreived, total_comparisons, doc_names_retreived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index_postings, inverted_index_frequency = create_inverted_index(file_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input query sequence : happy lion a bad boy\n",
      "\n",
      "Input operator sequence : [AND, OR, AND NOT]\n",
      "\n",
      "Input operator (array) : ['AND', 'OR', 'AND NOT']\n",
      "\n",
      "Query tokens : ['happy', 'lion', 'bad', 'boy']\n",
      "\n",
      "Query tokens : ['happy', 'lion', 'bad', 'boy']\n",
      "\n",
      "Operators : \n",
      "1133\n",
      "[1, 9, 15, 20, 23, 27, 28, 30, 52, 56, 83, 88, 92, 115, 116, 128, 129, 130, 135, 175, 182, 195, 213, 216, 223, 235, 251, 252, 256, 264, 268, 270, 278, 286, 288, 301, 316, 318, 324, 326, 346, 348, 349, 350, 351, 352, 354, 378, 379, 392, 395, 401, 406, 414, 416, 417, 418, 420, 421, 424, 425, 426, 437, 445, 450, 475, 480, 501, 505, 514, 529, 533, 537, 547, 548, 557, 581, 584, 586, 600, 607, 608, 610, 618, 622, 630, 635, 641, 645, 650, 663, 671, 672, 682, 691, 716, 717, 718, 724, 752, 753, 756, 782, 785, 786, 796, 804, 812, 813, 814, 815, 817, 822, 824, 835, 838, 839, 853, 855, 889, 909, 910, 911, 916, 921, 928, 933, 956, 960, 977, 980, 981, 985, 1006, 1016, 1017, 1019, 1020, 1028, 1033, 1034, 1035, 1039, 1042, 1044, 1053, 1055, 1057, 1063, 1066, 1085, 1086, 1087, 1094, 1113, 1117, 1120]\n",
      "Query : happy AND lion OR bad AND NOT boy \n",
      "\n",
      "\n",
      "\n",
      "No. of documents retreived: 207\n",
      "Number of comparisons: 1473\n",
      "Names of retreived documents: ['acronym.lis', 'adrian_e.faq', 'aeonint.txt', 'aids.txt', 'airlines', 'alflog.txt', 'anime.cli', 'annoy.fascist', 'aussie.lng', 'bad', 'bad-d', 'badday.hum', 'basehead.txt', 'beauty.tm', 'beer-g', 'beer.gam', 'beergame.hum', 'bhb.ill', 'billcat.hum', 'bimg.prn', 'blackadd', 'blackhol.hum', 'bless.bc', 'bmdn01.txt', 'bored.txt', 'brewing', 'brush1.txt', 'buffwing.pol', 'butcher.txt', 'butwrong.hum', 'bw.txt', 'byfb.txt', 'calculus.txt', 'cancer.rat', 'candy.txt', 'childhoo.jok', 'christop.int', 'chung.iv', 'clancy.txt', 'climbing.let', 'coffee.faq', 'coffeebeerwomen.txt', 'cogdis.txt', 'cold.fus', 'coldfake.hum', 'collected_quotes.txt', 'college.sla', 'comic_st.gui', 'commutin.jok', 'computer.txt', 'comrevi1.hum', 'cooplaws', 'crazy.txt', 'curiousgeorgie.txt', 'curse.txt', 'dalive', 'dead2.txt', 'dead4.txt', 'deadlysins.txt', 'deep.txt', 'devils.jok', 'dingding.hum', 'dining.out', 'doc-says.txt', 'docdict.txt', 'docspeak.txt', 'doggun.sto', 'drinks.gui', 'dthought.txt', 'dubltalk.jok', 'econridl.fun', 'elevator.fun', 'english', 'enlightenment.txt', 'epikarat.txt', 'epiquest.txt', 'eskimo.nel', 'facedeth.txt', 'fajitas.rcp', 'fascist.txt', 'fed.txt', 'fiber.txt', 'figure_1.txt', 'fireplacein.txt', 'flux_fix.txt', 'fuckyou2.txt', 'f_tang.txt', 'get.drunk.cheap', 'ghostsch.hum', 'good.txt', 'grail.txt', 'hack7.txt', 'hackmorality.txt', 'hammock.hum', 'highland.epi', 'hitler.59', 'hitler.txt', 'horflick.txt', 'how2bgod.txt', 'how_to_i.pro', 'indgrdn.txt', 'insult.lst', 'insults1.txt', 'is_story.txt', 'jac&tuu.hum', 'jerky.rcp', 'jokes1.txt', 'lansing.txt', 'laws.txt', 'lawsuniv.hum', 'lawyer.jok', 'lbinter.hum', 'lifeimag.hum', 'lifeonledge.txt', 'limerick.jok', 'livnware.hum', 'llong.hum', 'looser.hum', 'losers84.hum', 'lozers', 'lozeuser.hum', 'luggage.hum', 'luvstory.txt', 'meinkamp.hum', 'men&wome.txt', 'mensroom.jok', 'mindvox', 'minn.txt', 'missdish', 'modest.hum', 'modstup', 'mog-history', 'montpyth.hum', 'motrbike.jok', 'mtm.hum', 'murph.jok', 'murphy.txt', 'murphys.txt', 'murphy_l.txt', 'nameisreo.txt', 'nigel.1', 'nigel.10', 'nigel.2', 'nigel.3', 'nigel.4', 'nigel.7', 'nigel10.txt', 'nuke.hum', 'nukeplay.hum', 'number', 'oliver02.txt', 'onan.txt', 'oxymoron.jok', 'p-law.hum', 'paddingurpapers.txt', 'pat.txt', 'peatchp.hum', 'phxbbs-m.txt', 'pickup.lin', 'planetzero.txt', 'poets.hum', 'poopie.txt', 'potty.txt', 'psych_pr.quo', 'puzzles.jok', 'q.pun', 'quack26.txt', 'quest.hum', 'quick.jok', 'quotes.txt', 'racist.net', 'rec.por', 'reddye.hum', 'rentals.hum', 'rinaldo.jok', 'rinaldos.law', 'rinaldos.txt', 'ripoffpc.hum', 'sf-zine.pub', 'sfmovie.txt', 'skincat', 'smackjok.hum', 'snapple.rum', 'subrdead.hum', 'sw_err.txt', 'talebeat.hum', 'tarot.txt', 'teevee.hum', 'textgrap.hum', 'tickmoon.hum', 'trekfume.txt', 'tshirts.jok', 'twinkie.txt', 'twinpeak.txt', 'ukunderg.txt', 'variety2.asc', 'variety3.asc', 'voltron.hum', 'whoops.hum', 'woodbugs.txt', 'woodsmok.txt', 'worldend.hum', 'wrdnws1.txt', 'wrdnws2.txt', 'wrdnws3.txt', 'wrdnws9.txt', 'yuppies.hum']\n"
     ]
    }
   ],
   "source": [
    "input_query_sequence = input(\"Enter the space-separated query terms : \")\n",
    "input_operator_sequence = input(\"Enter the comma-separated operator sequence enclosed in '[]' : \")\n",
    "print(f\"Input query sequence : {input_query_sequence}\\n\")\n",
    "print(f\"Input operator sequence : {input_operator_sequence}\\n\")\n",
    "input_operators = process_ops(input_operator_sequence[1:len(input_operator_sequence) - 1])\n",
    "query_tokens = preprocess_query(input_query_sequence)\n",
    "print(f\"Input operator (array) : {input_operators}\\n\")\n",
    "print(f\"Query tokens : {query_tokens}\\n\")\n",
    "query_result_ndocs, query_result_total_cmps, query_result_doc_names = process_query(inverted_index_postings, query_tokens, input_operators)\n",
    "\n",
    "if(query_result_ndocs != -1):\n",
    "    query_generated = query_tokens[0] + \" \" + input_operators[0] + \" \" + query_tokens[1] + \" \"\n",
    "    ptr_qtoks = 2\n",
    "    for i in range(1, len(input_operators)):\n",
    "        query_generated += input_operators[i] + \" \"\n",
    "        query_generated += query_tokens[ptr_qtoks] + \" \"\n",
    "        ptr_qtoks += 1\n",
    "    print(f\"Query : {query_generated}\\n\\n\")\n",
    "    print(f\"\\nNo. of documents retreived: {query_result_ndocs}\\nNumber of comparisons: {query_result_total_cmps}\\nNames of retreived documents: {query_result_doc_names}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2370e07001db70a9c24f7e21173c51fbc4321340913a02830aed4885459fa0a0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
