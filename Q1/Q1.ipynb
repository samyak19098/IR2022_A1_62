{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sarthakj01/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/sarthakj01/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../Humor,Hist,Media,Food'\n",
    "file_names = os.listdir(data_dir)\n",
    "file_paths = [(data_dir + '/' + fname) for fname in file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def read_files(fpaths):\n",
    "    file_tokens = []\n",
    "    for fpath in fpaths:\n",
    "        f = open(fpath, 'r', encoding='utf-8', errors='replace')\n",
    "        ftxt_unprocessed = f.read()\n",
    "        # print(ftxt_unprocessed)\n",
    "        ftoks = preprocess_file(ftxt_unprocessed)\n",
    "        file_tokens.append(ftoks)\n",
    "    return file_tokens\n",
    "\n",
    "def isValidTok(tok):\n",
    "    if((tok not in string.punctuation) and (tok.isnumeric() == False) and (sum([0 if ch in string.punctuation else 1 for ch in tok]) >= 1)):\n",
    "        return True\n",
    "    return False \n",
    "\n",
    "def preprocess_file(file_text):\n",
    "\n",
    "    all_tokens = word_tokenize(file_text.lower())\n",
    "    all_unique_tokens = set(all_tokens)\n",
    "    stemmedTokens = set()\n",
    "    for token in all_unique_tokens:\n",
    "        stemmedToken = ps.stem(token)\n",
    "        stemmedTokens.add(stemmedToken)\n",
    "    tokens = stemmedTokens - stop_words\n",
    "    \n",
    "    valid_toks = []\n",
    "    for tok in tokens:\n",
    "        if(isValidTok(tok) == True):\n",
    "            valid_toks.append(tok)\n",
    "    return valid_toks\n",
    "    # print(final_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([0 if ch in string.punctuation else 1 for ch in \"a/2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_toks = read_files(file_paths)\n",
    "# print(file_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['especi', 'bad', 'tailbon', 'much', 'e-mail', 'direct', 'specif', 'goat', 'conveni', 'merit', 'caus', 'veri', 'killian', 'knee', 'advantag', 'difficult', 'oregano', 'puffi', 'etc', 'leroy', 'chuck', 'main', 'initi', 'date', 'thinner', 'bring', 'bouillon', 'best', 'recip', 'flay', 'pump', 'compress', 'expos', 'storag', 'determin', 'worth', 'deep-fat', 'prevent', 'varieti', 'younger', 'encas', 'sharp', 'tendon', 'ligament', 'slow', 'mothafucka', 'bleach', 'flank', 'wrap', 'clean', 'behind', 'step', 'rest', 'commerci', 'send', 'simpli', 'month', 'fine', 'space', 'drain', 'brought', 'bacon', 'beam', 'liver', 'care', 'greatli', 'collar', 'ha', 'thorough', 'pass', 'long-blad', 'roughli', 'wide', 'loop', 'meet', 'amount', 'recommend', 'first', 'worcestershir', 'massag', 'larger', 'fruit', 'vinegar', 'rather', 'liquid', 'configur', 'white', 'pure', 'vari', 'skin', 'tini', 'diagram', 'spread', 'toxin', 'throughout', 'hand', 'think', 'rememb', 'anu', 'hlv-infect', 'marinade/baste/dip/bloodi', 'dog', 'tough', 'taken', 'salt', 'top', 'insensit', 'age', 'paprika', 'foot', 'surfac', 'six', 'ed', 'entir', 'low-risk', 'boil', 'caucasian', 'pound', 'develop', 'type', 'princip', 'suffici', 'snuffit-l', 'qualifi', 'vessel', 'barrel', 'plenti', 'number', 'diet', 'habit', 'leverag', 'perfect', 'eat', 'greater', 'ash', 'need', 'blood', 'smaller', 'ordeal', 'thick', 'readili', 'heart', 'oven', 'bullet', 'dust', 'lose', 'rope', 'two', 'smoke', 'environ', 'group', 'suitabl', 'begin', 'score', 'void', 'instead', 'add', 'condit', 'anoth', 'etern', 'thicker', 'find', 'enorm', 'steer', 'contamin', 'new', 'end', 'brittl', 'steak', 'slightli', 'style', 'dispos', 'brain', 'distast', 'fri', '10:20pm', 'enough', 'limb', 'it_', 'viscera', 'torso', 'long', 'maintain', 'side', 'must', 'jaw', 'grip', 'drainag', 'thorac', 'meathook', 'room', 'electro.evangel', 'humeru', 'see', 'attent', 'rid', 'tabl', 'bob', 'pepper', 'befor', 'tomato', 'extrem', 'want', 'gopher.etext.org', 'one', 'almost', 'cook', 'surround', 'three', 'obstacl', 'sure', 'call', 'although', 'due', 'oil', 'blade', 'sauc', 'figur', 'rule', 'shoulder', 'hang', 'incis', 'flashboy', 'present', 'f.e.i.t.c.t.a.j', 'solut', 'suffic', 'organ', '5/14/95', 'yet', 'cage', 'thinli', 'strata', 'tool', 'task', 'subscrib', 'strip-styl', 'anyth', 'half', 'outhous', 'mix', 'start', 'corner', 'boni', 'market', 'gein', 'blow', 'follow', 'pig', 'saw', 'layer', 'volum', 'altern', 'debon', 'stage', 'weigh', 'complex', 'lemon', 'bobbacoo', 'affect', 'forehead', 'acquir', 'quarter', 'made', 'muscular', 'get', 'larg', 'lower', 'point', 'built', 'pulver', 'basic', 'parallel', 'pri', 'upright', 'around', 'print', 'happen', 'ever', 'whole', 'make', 'tast', 'right', 'meal', 'diseas', 'common', 'block', 'thirteen', 'request', 'retail', 'face', 'trunk', 'offal', 'coffe', 'support', 'sourc', 'p.n.n.a', 'occur', 'specimen', 'separ', 'solo', 'scrotum', 'lift', 'farm', 'easier', 'coe', 'femal', 'away', 'cow', 'qualiti', 'carver', 'averag', 'maggot', 'bone', 'flesh', 'reverend', 'along', 'carotid', 'perenni', 'deltoid', 'either', 'found', 'would', 'practic', 'person', 'blessed.in.slack', 'lip', 'caviti', 'connect', 'properli', 'citru', 'useabl', 'portion', 'calf', 'particular', 'inform', 'thoroughli', 'posit', 'version', 'may', 'upon', 'requir', 'safeti', 'set', 'intestin', 'yeti.gen', 'onc', 'thirti', 'opposit', 'secret', 'contain', 'avail', 'bodi', 'rind', 'wast', 'achil', 'stringi', 'suggest', 'aitch', 'tightli', 'groin', 'outer', 'aid', 'recogn', 'form', 'materi', 'head', 'curv', 'trophi', 'www', 'spine', 'reflect', 'membran', 'readi', 'chop', 'becom', 'cautiou', 'toward', 'molass', 'juici', 'certain', 'straight', 'inch', 'appear', 'subject', 'bacteria', 'cube', 'littl', 'back', 'herd', 'ankl', 'fact', 'bleed', 'twenti', 'largest', 'necessari', 'tsp', 'indic', 'joint', 'savori', 'fece', 'muscl', 'carri', 'tub', 'breastbon', 'cord', 'cleanli', 'fiber', 'belli', 'gland', 'roomi', 'inadequ', 'major', 'interior', 'smokehous', 'brown', 'abdomin', 'way', 'arteri', 'choic', 'steril', 'sweat', 'keep', 'idea', 'old', 'welcom', 'flat', 'circl', 'forearm', 'decreas', 'church', 'cris.com', 'attach', 'favorit', \"'ve\", 'thin', 'rib', 'past', 'consid', 'full', 'actual', 'ground', 'matur', 'upper', 'dash', 'tissu', 'directli', 'forward', 'decid', 'complet', 'chef', 'cut', 'insid', 'mass', 'produc', 'howev', 'nearli', 'becaus', 'optim', 'matter', 'tranquil', 'caution', 'throat', 'half-pelvi', 'piec', 'leav', 'go', 'consum', 'sugar', 'despit', 'locat', 'emet', 'pubic', 'servic', 'kept', 'roast', 'time', 'pattern', 'kidney', 'slice', 'cup', 'progress', 'beneath', 'dri', 'thu', 'excit', 'children', 'plan', 'chemic', 'hide', 'detach', 'flush', 'orange/pineapple/mango', 'front', 'overhead', 'stone', 'tblsp', 'buttock', 'hard', 'done', 'fatti', 'experiment', 'reason', 'manipul', 'fresh', 'jawlin', 'garlic', 'mail', 'youth', 'cleans', 'ani', 'stomach', 'assist', 'step-by-step', 'concentr', 'intern', 'clove', 'typic', 'neither', 'obvious', 'fat', 'trachea', 'guidelin', 'captiv', 'andrewm', 'sever', 'rais', 'culinari', 'somervil', 'roll', 'outsid', 'unless', 'bulk', 'gopher', 'barbecu', 'beef', 'stun', 'access', 'act', 'concret', 'build', 'import', 'handl', 'leg', 'unsubscrib', 'rump', 'knive', 'appar', 'view', 'compos', 'oz', 'bake', 'debat', 'immedi', 'exampl', 'fashion', 'chosen', 'pegram', 'render', 'eye', 'elbow', 'ipdlink', 'brisket', 'beer', 'releas', 'cleaver', 'result', 'gut', 'drunk', 'alway', 'solar', 'decapit', 'food', 'whiskey', 'gener', 'taint', 'easiest', 'kill', 'histori', 'control', 'round', 'shitter', 'function', 'live', 'divid', 'pickl', 'feet', 'easili', 'desir', 'place', 'preced', 'use', 'left', 'skull', 'shorter', 'effect', 'triangular', '3/4', 'anim', 'r.revv.', 'without', 'diaphragm', 'final', 'near', 'juic', 'broad', 'suspici', 'wish', 'hacksaw', 'outlin', 'minc', 'interfer', 'strip', 'fillet', 'old-fashion', 'ripe', 'owner', 'hoist', 'cayenn', 'list', 'spat', 'receptacl', 'red', 'choos', 'trim', 'effort', 'split', 'store', 'ant', 'purg', 'wrist', 'stream', 'provid', 'extern', 'like', 'firepit', 'mis-221', 'dinky-dao', 'lung', 'quickli', 'firm', '100-200', 'prefer', 'larynx', 'bottom', 'genit', 'water', 'pelvi', 'male', 'beauti', 'possibl', 'euthanasia', 'short', 'noth', 'bung', 'arson', 'knife', 'avoid', 'rebel.rais', 'continu', 'physic', 'twist', 'depth', 'clench', 'infinit', 'imbal', 'doe', 'case', 'year', 'simpl', 'behead', 'open', 'freezer', 'ftp', 'arm', 'length', 'process', 'percentag', 'one-fourth', 'onion', 'bodili', 'consumpt', 'someon', '/pub/zines/snuffit', 'field', 'dobb', 'onli', 'dead', 'crossbar', 'devil', 'neck', 'achiev', 'carv', 'ingredi', 'meat', 'armpit', 'marbl', 'guid', 'item', 'halv', 'depend', 'purpos', 'even', 'excis', 'fairli', 'hip', 'receiv', 'known', 'peopl', 'middl', 'wine', 'breast', 'ftp.etext.org', 'short-blad', 'disloc', 'central', 'unexpect', 'next', 'sake', 'burial', 'butcher', 'rush', 'zines/snuffit', 'rang', 'backbon', 'peel', 'instal', 'tender', 'liter', 'allow', 'bowel', 'ear-to-ear', 'feed', 'good', 'spinal', 'c.o.e.', 'secondli', '//www.paranoia.com/coe/', 'design', 'basil', 'saleabl', 'fluid', 'ritual', 'tennesse', 'remov', 'bother', 'inner', 'tongu', 'flavor', 'line', 'easi', 'apart', 'tie', 'struggl', 'anatomi', 'pull', 'serv', 'observ', 'accordingli', 'thigh', 'dilut', 'crisp', 'elder', 'refer', \"'s\", 'fast', 'retriev', 'danger', 'deep', 'compar', 'remain', 'divis', 'ahead', 'hairless', 'eviscer', 'also', 'whether', 'prepar', 'scaveng', 'help', 'section', 'work', 'plexu', 'wall', 'notic', 'health', 'bred', 'period', 'method', 'netcom.com', 'edg', 'box', 'earli', 'adrenalin', 'level', 'soup', 'close', 'infect', 'thi', 'wire', 'area', 'slaughter', '1/4', 'break', 'listserv', 'black', 'carcass', 'hair', 'mention', 'skeleton', 'untouch', 'improv', 'poison', '1/8', 'ined', 'hatchet', 'insert', 'small', 'jalepeno', 'stapl', 'crush', 'abov', 'obtain', 'vertebra', 'thought', '1/2', 'wait', 'appet', 'unconsci', 'imper', 'hour', 'ideal', 'system', 'fit', 'well', 'http', 'increas', 'peni', 'human', 'lightli', 'twine', 'singl', '_snuff', 'wash']\n"
     ]
    }
   ],
   "source": [
    "print(file_toks[451])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index(file_toks):\n",
    "    inv_index = {}\n",
    "    for i in range(len(file_toks)):\n",
    "        for tok in file_toks[i]:\n",
    "            if(tok not in inv_index.keys()):\n",
    "                inv_index[tok] = [i]\n",
    "            else:\n",
    "                inv_index[tok].append(i)\n",
    "    inv_index = dict(sorted(inv_index.items()))\n",
    "    terms_list = inv_index.keys()\n",
    "    for word in terms_list:\n",
    "        inv_index[word].sort()\n",
    "    return inv_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocsFromID(file_names, doc_IDs):\n",
    "    doc_names = []\n",
    "    for doc_ID in doc_IDs:\n",
    "        doc_names.append(file_names[doc_ID])\n",
    "    return doc_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_equal_arrays(arr1, arr2):\n",
    "\n",
    "    if(len(arr1) != len(arr2)):\n",
    "        return False\n",
    "    \n",
    "    arr1 = sorted(arr1)\n",
    "    arr2 = sorted(arr2)\n",
    "    for i in range(len(arr1)):\n",
    "        if(arr1[i] != arr2[i]):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_AND(inv_index, term1, term2, verbose=False):\n",
    "\n",
    "    terms_list = inv_index.keys()\n",
    "\n",
    "    if((term1 not in terms_list) or (term2 not in terms_list)):\n",
    "        return 0, 0, []\n",
    "\n",
    "    posting1 = inv_index[term1]\n",
    "    posting2 = inv_index[term2]\n",
    "    ptr1 = 0\n",
    "    ptr2 = 0\n",
    "    answer_docID = []\n",
    "\n",
    "    num_comparisons = 0\n",
    "\n",
    "    while(ptr1 < len(posting1) and ptr2 < len(posting2)):\n",
    "        num_comparisons += 1\n",
    "        # print(f\"1 : {posting1[ptr1]} , 2: {posting2[ptr2]}\")\n",
    "\n",
    "        if(posting1[ptr1] == posting2[ptr2]):\n",
    "            answer_docID.append(posting1[ptr1])\n",
    "            ptr1 += 1\n",
    "            ptr2 += 1\n",
    "        elif(posting1[ptr1] < posting2[ptr2]):\n",
    "            ptr1 += 1\n",
    "        else:\n",
    "            ptr2 += 1\n",
    "\n",
    "    num_docs_retreived = len(answer_docID)\n",
    "    doc_names_retreived = getDocsFromID(file_names, answer_docID)\n",
    "\n",
    "    if(verbose==True):\n",
    "        print(f\"Query: {term1} AND {term2}\\nNo. of documents retreived: {num_docs_retreived}\\nMinimum number of comparisons: {num_comparisons}\\nNames of retreived documents: {doc_names_retreived}\")\n",
    "    \n",
    "    # temp_verification = list(set(posting1) & set(posting2))\n",
    "    # print(f\"Verification: {check_equal_arrays(temp_verification, answer_docID)}\")\n",
    "\n",
    "    return num_docs_retreived, num_comparisons, doc_names_retreived\n",
    "\n",
    "def query_OR(inv_index, term1, term2, verbose=False):\n",
    "    \n",
    "    terms_list = inv_index.keys()\n",
    "\n",
    "    if((term1 not in terms_list) and (term2 not in terms_list)):\n",
    "        return 0, 0, []\n",
    "    elif((term1 not in terms_list) and (term2 in terms_list)):\n",
    "        ans_docs = inv_index[term2]\n",
    "        return len(ans_docs), 0, ans_docs\n",
    "    elif((term1 in terms_list) and (term2 not in terms_list)):\n",
    "        ans_docs = inv_index[term1]\n",
    "        return len(ans_docs), 0, ans_docs\n",
    "    else:\n",
    "        posting1 = inv_index[term1]\n",
    "        posting2 = inv_index[term2]\n",
    "        ptr1 = 0\n",
    "        ptr2 = 0\n",
    "        answer_docID = []\n",
    "\n",
    "        num_comparisons = 0\n",
    "\n",
    "        while(ptr1 < len(posting1) and ptr2 < len(posting2)):\n",
    "            num_comparisons += 1\n",
    "            # print(f\"1 : {posting1[ptr1]} , 2: {posting2[ptr2]}\")\n",
    "\n",
    "            if(posting1[ptr1] == posting2[ptr2]):\n",
    "                answer_docID.append(posting1[ptr1])\n",
    "                ptr1 += 1\n",
    "                ptr2 += 1\n",
    "            elif(posting1[ptr1] < posting2[ptr2]):\n",
    "                answer_docID.append(posting1[ptr1])\n",
    "                ptr1 += 1\n",
    "            else:\n",
    "                answer_docID.append(posting2[ptr2])\n",
    "                ptr2 += 1\n",
    "        while(ptr1 < len(posting1)):\n",
    "            answer_docID.append(posting1[ptr1])\n",
    "            ptr1 += 1\n",
    "        while(ptr2 < len(posting2)):\n",
    "            answer_docID.append(posting2[ptr2])\n",
    "            ptr2 += 1\n",
    "\n",
    "\n",
    "        num_docs_retreived = len(answer_docID)\n",
    "        doc_names_retreived = getDocsFromID(file_names, answer_docID)\n",
    "\n",
    "        if(verbose==True):\n",
    "            print(f\"Query: {term1} OR {term2}\\nNo. of documents retreived: {num_docs_retreived}\\nMinimum number of comparisons: {num_comparisons}\\nNames of retreived documents: {doc_names_retreived}\")\n",
    "        \n",
    "        # temp_verification = list(set(posting1) | set(posting2))\n",
    "        # print(f\"Verification: {check_equal_arrays(temp_verification, answer_docID)}\")\n",
    "        return num_docs_retreived, num_comparisons, doc_names_retreived\n",
    "\n",
    "def perform_NOT(inv_index, term):\n",
    "\n",
    "    all_docIDs = [docID for docID in range(len(file_names))]\n",
    "    if(term not in inv_index.keys()):\n",
    "        return all_docIDs\n",
    "\n",
    "    posting = inv_index[term]\n",
    "    for docID in posting:\n",
    "        all_docIDs.remove(docID)\n",
    "\n",
    "    return all_docIDs\n",
    "\n",
    "def query_AND_NOT(inv_index, term1, term2, verbose=False):\n",
    "\n",
    "    terms_list = inv_index.keys()\n",
    "\n",
    "    if((term1 not in terms_list)):\n",
    "        return 0, 0, []\n",
    "    \n",
    "    posting1 = inv_index[term1]\n",
    "    posting2 = perform_NOT(inv_index, term2)\n",
    "    ptr1 = 0\n",
    "    ptr2 = 0\n",
    "    answer_docID = []\n",
    "\n",
    "    num_comparisons = 0\n",
    "\n",
    "    while(ptr1 < len(posting1) and ptr2 < len(posting2)):\n",
    "        num_comparisons += 1\n",
    "        # print(f\"1 : {posting1[ptr1]} , 2: {posting2[ptr2]}\")\n",
    "        if(posting1[ptr1] == posting2[ptr2]):\n",
    "            answer_docID.append(posting1[ptr1])\n",
    "            ptr1 += 1\n",
    "            ptr2 += 1\n",
    "        elif(posting1[ptr1] < posting2[ptr2]):\n",
    "            ptr1 += 1\n",
    "        else:\n",
    "            ptr2 += 1\n",
    "\n",
    "    num_docs_retreived = len(answer_docID)\n",
    "    doc_names_retreived = getDocsFromID(file_names, answer_docID)\n",
    "\n",
    "    if(verbose==True):\n",
    "        print(f\"Query: {term1} AND NOT {term2}\\nNo. of documents retreived: {num_docs_retreived}\\nMinimum number of comparisons: {num_comparisons}\\nNames of retreived documents: {doc_names_retreived}\")\n",
    "    \n",
    "    # temp_verification = list(set(posting1) | set(posting2))\n",
    "    # print(f\"Verification: {check_equal_arrays(temp_verification, answer_docID)}\")\n",
    "    return num_docs_retreived, num_comparisons, doc_names_retreived\n",
    "\n",
    "def query_OR_NOT(inv_index, term1, term2, verbose=False):\n",
    "    \n",
    "    terms_list = inv_index.keys()\n",
    "\n",
    "    if((term1 not in terms_list) and (term2 not in terms_list)):\n",
    "        ans_docs = perform_NOT(inv_index, term2)\n",
    "        return len(ans_docs), 0, ans_docs\n",
    "    elif((term1 not in terms_list) and (term2 in terms_list)):\n",
    "        ans_docs = perform_NOT(inv_index, term2)\n",
    "        return len(ans_docs), 0, ans_docs\n",
    "    # elif((term1 in terms_list) and (term2 not in terms_list)):\n",
    "    #     ans_docs = inv_index[term1]\n",
    "    #     return len(ans_docs), 0, ans_docs\n",
    "    else:\n",
    "        posting1 = inv_index[term1]\n",
    "        posting2 = perform_NOT(inv_index, term2)\n",
    "        ptr1 = 0\n",
    "        ptr2 = 0\n",
    "        answer_docID = []\n",
    "\n",
    "        num_comparisons = 0\n",
    "\n",
    "        while(ptr1 < len(posting1) and ptr2 < len(posting2)):\n",
    "            num_comparisons += 1\n",
    "            # print(f\"1 : {posting1[ptr1]} , 2: {posting2[ptr2]}\")\n",
    "\n",
    "            if(posting1[ptr1] == posting2[ptr2]):\n",
    "                answer_docID.append(posting1[ptr1])\n",
    "                ptr1 += 1\n",
    "                ptr2 += 1\n",
    "            elif(posting1[ptr1] < posting2[ptr2]):\n",
    "                answer_docID.append(posting1[ptr1])\n",
    "                ptr1 += 1\n",
    "            else:\n",
    "                answer_docID.append(posting2[ptr2])\n",
    "                ptr2 += 1\n",
    "        while(ptr1 < len(posting1)):\n",
    "            answer_docID.append(posting1[ptr1])\n",
    "            ptr1 += 1\n",
    "        while(ptr2 < len(posting2)):\n",
    "            answer_docID.append(posting2[ptr2])\n",
    "            ptr2 += 1\n",
    "\n",
    "\n",
    "        num_docs_retreived = len(answer_docID)\n",
    "        doc_names_retreived = getDocsFromID(file_names, answer_docID)\n",
    "\n",
    "        if(verbose==True):\n",
    "            print(f\"Query: {term1} OR NOT {term2}\\nNo. of documents retreived: {num_docs_retreived}\\nMinimum number of comparisons: {num_comparisons}\\nNames of retreived documents: {doc_names_retreived}\")\n",
    "        \n",
    "        # temp_verification = list(set(posting1) | set(posting2))\n",
    "        # print(f\"Verification: {check_equal_arrays(temp_verification, answer_docID)}\")\n",
    "        return num_docs_retreived, num_comparisons, doc_names_retreived\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = create_inverted_index(file_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: water AND NOT effect\n",
      "No. of documents retreived: 229\n",
      "Minimum number of comparisons: 994\n",
      "Names of retreived documents: ['tickmoon.hum', 'brewing', 'gingbeer.txt', 'quick.jok', 'lawsuniv.hum', 'venison.txt', 'jambalay.pol', 'boe.hum', 'murphy.txt', 'cookie.1', 'fusion.gal', 'letter.txt', 'makebeer.hum', 'who.txt', 'montoys.txt', 'bond-2.txt', 'cooking.fun', 'bad-d', 'films_gl.txt', 'realest.txt', 'freshman.hum', 'nysucks.hum', 'butwrong.hum', 'mensroom.jok', 'whitbred.txt', 'woods.txt', 'commutin.jok', 'aeonint.txt', 'jerky.rcp', 'greenchi.txt', 'fartinfo.txt', 'jalapast.dip', 'recipe.011', 'nosuch_nasfic', 'bread.rcp', 'nigel.2', 'firstaid.txt', 'jokeju07.txt', 'jokes.txt', 'kashrut.txt', 'cokeform.txt', 'econridl.fun', 'turkey.fun', 'topten.hum', 'antimead.bev', 'insult', 'wisconsi.txt', 'texican.dic', 'margos.txt', 'baklava.des', 'caramels.des', 'pot.txt', 'lotsa.jok', 'from.hum', 'coke1', 'firecamp.txt', 'hangover.txt', 'suicide2.txt', 'venganza.txt', 'booze2.fun', 'thecube.hum', 'hamburge.nam', 'dandwine.bev', 'bad', 'jawgumbo.fis', 'fascist.txt', \"terrmcd'.hum\", 'epi_bnb.txt', 'seeds42.txt', 'hotnnot.hum', 'hackingcracking.txt', 'rns_ency.txt', 'recepies.fun', 'bnbguide.txt', 'readme.bat', 'smurf-03.txt', 'jokes', 'boarchil.txt', 'un.happy', 'firstaid.inf', 'moonshin', 'capital.txt', 'cultmov.faq', 'recipe.007', 'oam-001.txt', 'eggroll1.mea', 'potty.txt', 'insect1.txt', 'egg-bred.txt', 'coke.txt', 'foodtips', 'oxymoron.jok', 'fusion.sup', 'icm.hum', 'aussie.lng', 'blkbnsrc.vgn', 'bread.rec', 'batrbred.txt', 'arcadian.txt', 'inlaws1.txt', 'recip1.txt', 'chung.iv', 'old.txt', 'trukdeth.txt', 'recipe.005', 'robot.tes', 'pasta001.sal', 'ghostsch.hum', 'subrdead.hum', 'bhang.fun', 'insult.lst', 'cmu.share', 'skincat', 'goldwatr.txt', 'beauty.tm', 'parsnip.txt', 'thermite.ana', 'shameonu.hum', 'oculis.rcp', 'japice.bev', 'pickup.txt', 'valujet.txt', 'oliver.txt', 'harmful.hum', 'mash.hum', 'frogeye1.sal', 'hotel.txt', 'planeget.hum', 'psych_pr.quo', 'booze1.fun', 'oldtime.txt', 'strattma.txt', 'phorse.hum', 'smurfkil.hum', 'coffee.txt', 'drinks.txt', 'jokes1.txt', 'co-car.jok', 'wagon.hum', 'wetdream.hum', 'pipespec.txt', 'lozerzon.hum', 'texican.lex', 'shooters.txt', 'dead-r', 'startrek.txt', 'insults1.txt', 'recipe.012', 'gack!.txt', 'phony.hum', 'engmuffn.txt', 'mothers.txt', 'gd_gal.txt', 'butstcod.fis', 'brownie.rec', 'calif.hum', 'ghostfun.hum', 'peatchp.hum', 'focaccia.brd', 'meat2.txt', 'quantity.001', 'bredcake.des', 'poopie.txt', 'x-drinks.txt', 'top10.txt', 'fuck!.txt', 'coke.fun', 'madscrib.hum', 'diet.txt', 'lll.hum', 'indgrdn.txt', 'dining.out', 'exam.50', 'testchri.txt', 'jc-elvis.inf', 'tribble.hum', 'bread.txt', 'puzzles.jok', 'dead5.txt', 'recipe.002', 'paddingurpapers.txt', 'zucantom.sal', 'chili.txt', 'recipe.004', 'renored.txt', 'shuimai.txt', 'ateam.epi', 'recipe.006', 'test2.jok', 'enquire.hum', 'ppbeer.txt', 'egglentl.vgn', 'hierarch.txt', 'a_fish_c.apo', 'richbred.txt', 'japantv.txt', 't_zone.jok', 'recipe.008', 'ozarks.hum', 'jungjuic.bev', 'bakebred.txt', 'recipe.001', 'cold.fus', 'beer.txt', 'wonton.txt', 'homebrew.txt', 'smackjok.hum', 'annoy.fascist', 'a-team', 'onetoone.hum', 'bnbeg2.4.txt', 'drunk.txt', 'feggaqui.txt', 'koans.txt', 'incarhel.hum', 'btscke04.des', 'mitch.txt', 'cake.rec', 'zuccmush.sal', 'btscke01.des', 'wood', 'tpquotes.txt', 'penndtch', 'wrdnws6.txt', 'appetiz.rcp', 'beer-gui', 'lipkovits.txt', 'zen.txt', 'blkbean.txt']\n"
     ]
    }
   ],
   "source": [
    "num_docs_AND, min_cmps_AND, doc_names_AND = query_AND_NOT(inverted_index, 'water', 'effect', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: water OR effect\n",
      "No. of documents retreived: 448\n",
      "Minimum number of comparisons: 447\n",
      "Names of retreived documents: ['tickmoon.hum', 'radexposed.txt', 'brewing', 'catin.hat', 'mel.txt', 'gingbeer.txt', 'hop.faq', 'dead2.txt', 'quick.jok', 'tuna.lab', 'coollngo2.txt', 'prac2.jok', 'lawsuniv.hum', 'venison.txt', 'jambalay.pol', 'cogdis.txt', 'boe.hum', 'murphy.txt', 'cookie.1', 'candy.txt', 'hi.tec', 'luggage.hum', 'fusion.gal', 'letter.txt', 'makebeer.hum', 'brush1.txt', 'moose.txt', 'who.txt', 'montoys.txt', 'bond-2.txt', 'chinese.txt', 'onetotwo.hum', 'cooking.fun', 'bad-d', 'films_gl.txt', 'blake7.lis', 'realest.txt', 'freshman.hum', 'cartoon.laws', 'nysucks.hum', 'butwrong.hum', 'mensroom.jok', 'progrs.gph', 'staff.txt', 'pure.mat', 'whitbred.txt', 'nigel.10', 'woods.txt', 'commutin.jok', 'aeonint.txt', 'jerky.rcp', 'nameisreo.txt', 'greenchi.txt', 'antibiot.txt', 'fartinfo.txt', 'jalapast.dip', 'ukunderg.txt', 'rockmus.hum', 'recipe.011', 'nosuch_nasfic', 'stuf11.txt', 'lazarus.txt', 'filmgoof.txt', 'bread.rcp', 'consp.txt', 'facedeth.txt', 'nigel.2', 'computer.txt', 'firstaid.txt', 'jokeju07.txt', 'lbinter.hum', 'jokes.txt', 'telecom.q', 'kashrut.txt', 'gd_ql.txt', 'classicm.hum', 'cokeform.txt', 'nukewar.txt', 'econridl.fun', 'turkey.fun', 'topten.hum', 'antimead.bev', 'insult', 'transp.txt', 'pepper.txt', 'fearcola.hum', 'lines.jok', 'worldend.hum', 'butcher.txt', 'wisconsi.txt', 'merry.txt', 'texican.dic', 'yogurt.asc', 'margos.txt', 'strine.txt', 'pizzawho.hum', 'baklava.des', 'caramels.des', 'pot.txt', 'bored.txt', 'lotsa.jok', 'eskimo.nel', 'malechem.txt', 'phxbbs-m.txt', 'dym', 'from.hum', 'truthlsd.hum', 'insanity.hum', 'outawork.erl', 'coke1', 'wacky.ani', 'firecamp.txt', 'fajitas.rcp', 'hangover.txt', 'relative.ada', 'suicide2.txt', 'basehead.txt', 'all_grai', 'iced.tea', 'venganza.txt', 'stuf10.txt', 'allfam.epi', 'goforth.hum', 'commword.hum', 'booze2.fun', 'let.go', 'gd_ol.txt', 'thecube.hum', 'hamburge.nam', 'letgosh.txt', 'dandwine.bev', 'solviets.hum', 'bad', 'ayurved.txt', 'jawgumbo.fis', 'fascist.txt', 'tnd.1', 'college.txt', \"terrmcd'.hum\", 'epi_bnb.txt', 'seeds42.txt', 'atombomb.hum', 'calvin.txt', 'hotnnot.hum', 'hackingcracking.txt', 'smartass.txt', 'rns_ency.txt', 'adt_miam.txt', 'humor9.txt', 'recepies.fun', 'rocking.hum', 'bnbguide.txt', 'readme.bat', 'smurf-03.txt', 'lifeonledge.txt', 'oasis', 'acetab1.txt', 'jokes', 'boarchil.txt', 'un.happy', 'firstaid.inf', 'deathhem.txt', 'anime.cli', 'nuke.hum', '1st_aid.txt', 'moonshin', 'mowers.txt', 'vegan.rcp', 'capital.txt', 'cultmov.faq', 'recipe.007', 'oam-001.txt', 'answers', 'practica.txt', 'eggroll1.mea', 'potty.txt', 'insect1.txt', 'byfb.txt', 'bhb.ill', 'egg-bred.txt', 'jargon.phd', 'coke.txt', 'acne1.txt', 'foodtips', 'oxymoron.jok', 'fusion.sup', 'icm.hum', 'flux_fix.txt', 'aussie.lng', 'blkbnsrc.vgn', 'bread.rec', 'batrbred.txt', 'bw.txt', 'shorties.jok', 'losers86.hum', 'aniherb.txt', 'arcadian.txt', 'inlaws1.txt', 'recip1.txt', 'conan.txt', 'men&wome.txt', 'chung.iv', 'old.txt', 'various.txt', 'c0dez.txt', 'modemwld.txt', 'mog-history', 'scratchy.txt', 'whatbbs', 'contract.moo', 'mead.rcp', 'gd_tznew.txt', 'episimp2.txt', 'arthriti.txt', 'trukdeth.txt', 'recipe.005', 'robot.tes', 'back1.txt', 'cartoon_.txt', 'thievco.txt', 'pasta001.sal', 'epiquest.txt', 'ghostsch.hum', 'subrdead.hum', 'bhang.fun', 'prac1.jok', 'insult.lst', 'cmu.share', 'skincat', 'goldwatr.txt', 'coldfake.hum', 'horflick.txt', 'mutate.hum', 'devils.jok', 'beauty.tm', 'bbh_intv.txt', 'parsnip.txt', 'thermite.ana', 'shameonu.hum', 'oculis.rcp', 'japice.bev', 'coffee.faq', 'mindvox', 'gotukola.hrb', 'beginn.ers', 'pickup.txt', 'valujet.txt', 'oliver.txt', 'harmful.hum', 'pickup.lin', 'mash.hum', 'aphrodis.txt', 'frogeye1.sal', 'ohandre.hum', 'hotel.txt', 'planeget.hum', 'psych_pr.quo', 'booze1.fun', 'grail.txt', 'oldtime.txt', 'strattma.txt', 'oliver02.txt', 'phorse.hum', 'figure_1.txt', 'smurfkil.hum', 'coffee.txt', 'blackhol.hum', 'drinks.txt', 'jokes1.txt', 'co-car.jok', 'wagon.hum', 'wetdream.hum', 'memo.hum', 'manners.txt', 'cybrtrsh.txt', 'pipespec.txt', 'doggun.sto', 'lozerzon.hum', 'texican.lex', 'cartoon.law', 'shooters.txt', 'simp.txt', 'dead-r', 'booze.fun', 'startrek.txt', 'insults1.txt', 'fiber.txt', 'recipe.012', 'gack!.txt', 'prac3.jok', 'climbing.let', 'phony.hum', 'deadlysins.txt', 'outlimit.txt', 'dead4.txt', 'luvstory.txt', 'b12.txt', 'socecon.hum', 'engmuffn.txt', 'mothers.txt', 'gd_gal.txt', 'libraway.txt', 'quack26.txt', 'butstcod.fis', 'brownie.rec', 'lozeuser.hum', 'calif.hum', 'popmusi.hum', 'hackmorality.txt', 'idaho.txt', 'food', 'ghostfun.hum', 'prac4.jok', 'peatchp.hum', 'focaccia.brd', 'comrevi1.hum', 'meat2.txt', 'cbmatic.hum', 'woodsmok.txt', 'top10st2.txt', 'quantity.001', 'bredcake.des', 'poopie.txt', 'bbq.txt', 'pracjoke.txt', 'x-drinks.txt', 'college.sla', 'top10.txt', 'snapple.rum', 'fuck!.txt', 'coke.fun', 'madscrib.hum', 'sfmovie.txt', 'calamus.hrb', 'diet.txt', 'lll.hum', 'nigel.4', 'indgrdn.txt', 'dining.out', 'montpyth.hum', 'exam.50', 'testchri.txt', 'drinks.gui', 'nintendo.jok', 'jc-elvis.inf', 'proudlyserve.txt', 'final-ex.txt', 'tribble.hum', 'bread.txt', 'puzzles.jok', 'sysman.txt', 'crzycred.lst', 'packard.txt', 'dead5.txt', 'recipe.002', 'english.txt', 'atherosc.txt', 'paddingurpapers.txt', 'test.jok', 'bmdn01.txt', 'zucantom.sal', 'dead3.txt', 'misc.1', 'mlverb.hum', 'finalexm.hum', 'chili.txt', 'recipe.004', 'renored.txt', 'newcoke.txt', 'freudonseuss.txt', 'shuimai.txt', 'ateam.epi', 'hack7.txt', 'recipe.006', 'test2.jok', 'nigel10.txt', 'gown.txt', 'enquire.hum', 'howlong.hum', 'timetr.hum', 'epi_rns.txt', 'ppbeer.txt', 'egglentl.vgn', 'blaster.hum', 'collected_quotes.txt', 'hierarch.txt', 'a_fish_c.apo', 'richbred.txt', 'urban.txt', 'japantv.txt', 'cartoon_laws.txt', 't_zone.jok', 'top10st1.txt', 'amazing.epi', 'recipe.008', 'sw_err.txt', 'ozarks.hum', 'wrdnws8.txt', 'acronyms.txt', 'jungjuic.bev', 'bakebred.txt', 'jason.fun', 'recipe.001', 'roadpizz.txt', 'epikarat.txt', 'cold.fus', 'beer.txt', 'wonton.txt', 'get.drunk.cheap', 'homebrew.txt', 'legal.hum', 'smackjok.hum', 'annoy.fascist', 'a-team', 'onetoone.hum', 'bagelope.txt', 'bnbeg2.4.txt', 'boneles2.txt', 'drunk.txt', 'feggaqui.txt', 'koans.txt', 'xibovac.txt', 'quux_p.oem', 'incarhel.hum', 'btscke04.des', 'aids.txt', 'kanalx.txt', 'mitch.txt', 'critic.txt', 'cake.rec', 'turbo.hum', 'zuccmush.sal', 'btscke01.des', 'epi_tton.txt', 'damiana.hrb', 'wood', 'tpquotes.txt', 'penndtch', 'rent-a_cat', 'poll2res.hum', 'wrdnws6.txt', 'llong.hum', 'appetiz.rcp', 'beer-gui', 'lipkovits.txt', 'zen.txt', 'is_story.txt', 'blkbean.txt']\n"
     ]
    }
   ],
   "source": [
    "num_docs_OR, min_cmps_OR, doc_names_OR = query_OR(inverted_index, 'water', 'effect', verbose=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2370e07001db70a9c24f7e21173c51fbc4321340913a02830aed4885459fa0a0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
