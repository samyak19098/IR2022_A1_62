{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import string\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../Humor,Hist,Media,Food'\n",
    "file_names = os.listdir(data_dir) #reading the data directory to list all the files\n",
    "file_paths = [(data_dir + '/' + fname) for fname in file_names] #forming file paths\n",
    "docID_to_doc_mapping = {} #forming docID to doc name mapping\n",
    "for i in range(len(file_names)):\n",
    "    docID_to_doc_mapping[i] = file_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_files(fpaths):\n",
    "    '''\n",
    "        Reads the files and pre process every file's text to form word tokens for every file.\n",
    "        Returns a 2-D list containing word tokens for every file\n",
    "    '''\n",
    "    file_tokens = []\n",
    "    for fpath in fpaths:\n",
    "        f = open(fpath, 'r', encoding='utf-8', errors='replace') #open the file\n",
    "        ftxt_unprocessed = f.read() #read the text of the file\n",
    "        ftoks = preprocess_file(ftxt_unprocessed) #preprocessing the text to form word tokens\n",
    "        file_tokens.append(ftoks)\n",
    "    return file_tokens\n",
    "\n",
    "def check_alnum(tok):\n",
    "    '''\n",
    "        Remove non-alphanumeric characters from a string\n",
    "    '''\n",
    "    tok = ''.join(x for x in tok if x.isalnum() == True)\n",
    "    return tok\n",
    "def remove_punct(tok):\n",
    "    '''\n",
    "        Remove the punctuation in token\n",
    "    '''\n",
    "    punctuations = string.punctuation\n",
    "    tok = ''.join(x for x in tok if x not in punctuations)\n",
    "    return tok\n",
    "\n",
    "def remove_space(tok):\n",
    "    '''\n",
    "        Remove the spaces in token\n",
    "    '''\n",
    "    tok = ''.join(x for x in tok if x != ' ')\n",
    "    return tok\n",
    "\n",
    "def preprocess_file(file_text):\n",
    "    '''\n",
    "        Preprocess the file text and converting to word tokens\n",
    "        Input: string File text\n",
    "        Returns file_tokens, word tokens for the pre processed text\n",
    "    '''\n",
    "    #converting text to lowercase\n",
    "    text = file_text.lower()\n",
    "\n",
    "    #Fixing the contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    #Performing word tokenization\n",
    "    all_toks = word_tokenize(text)\n",
    "    all_unique_toks = set(all_toks) #taking only the unique tokens\n",
    "\n",
    "    #Omitting all the non-alphanumeric characters in tokens\n",
    "    all_unique_toks = [check_alnum(x) for x in all_unique_toks]\n",
    "\n",
    "    #removing stopwords\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    file_toks = [tok for tok in all_unique_toks if tok not in stop_words]\n",
    "\n",
    "    #removing punctations if any remain\n",
    "    toks_no_punct = []\n",
    "    for tok in file_toks:\n",
    "        ctok = remove_punct(tok)\n",
    "        if(ctok != \"\"):\n",
    "            toks_no_punct.append(ctok)\n",
    "\n",
    "    #removing spaces in any remain\n",
    "    cleaned_toks = []\n",
    "    for tok in toks_no_punct:\n",
    "        ctok = remove_space(tok)\n",
    "        if(ctok != \"\"):\n",
    "            cleaned_toks.append(ctok)\n",
    "    cleaned_toks = list(set(cleaned_toks))\n",
    "    final_tokens = [tok for tok in cleaned_toks]\n",
    "\n",
    "    return final_tokens\n",
    "\n",
    "def preprocess_query(query_text):\n",
    "    '''\n",
    "        Performs the pre processing of the query text\n",
    "        Input: A string representing query text\n",
    "        Output: Query word tokens\n",
    "    '''\n",
    "\n",
    "    #Performing lowercasing\n",
    "    text = query_text.lower()\n",
    "    \n",
    "    #Fixing the contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    #word tokenization\n",
    "    all_toks = word_tokenize(text)\n",
    "\n",
    "    #taking only unique tokens\n",
    "    all_unique_toks = []\n",
    "    for tok in all_toks:\n",
    "        if(tok not in all_unique_toks):\n",
    "            all_unique_toks.append(tok)\n",
    "\n",
    "    #Omitting all the non-alphanumeric characters in tokens\n",
    "    all_unique_toks = [check_alnum(x) for x in all_unique_toks]\n",
    "\n",
    "    #removing stopwords\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    file_toks = [tok for tok in all_unique_toks if tok not in stop_words]\n",
    "\n",
    "    #removing punctations if any remain\n",
    "    toks_no_punct = []\n",
    "    for tok in file_toks:\n",
    "        ctok = remove_punct(tok)\n",
    "        if(ctok != \"\"):\n",
    "            toks_no_punct.append(ctok)\n",
    "\n",
    "    #removing spaces in any remain\n",
    "    cleaned_toks = []\n",
    "    for tok in toks_no_punct:\n",
    "        ctok = remove_space(tok)\n",
    "        if(ctok != \"\"):\n",
    "            cleaned_toks.append(ctok)\n",
    "    \n",
    "    #finally taking all the unique query tokens after all the preprocessing steps\n",
    "    final_tokens = []\n",
    "    for tok in cleaned_toks:\n",
    "        if(tok not in final_tokens):\n",
    "            final_tokens.append(tok)\n",
    "\n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the files to extract word tokens from each and every file\n",
    "file_toks = read_files(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index(file_toks):\n",
    "    ''' \n",
    "        This function build the inverted index. It takes in the word tokens of each file as input and returns two dictionaries: \n",
    "        inv_index_postings - corresponding to the posting list of all the terms containg the document IDs correspondg to each term and \n",
    "        inv_index_frequency - corresponding to the document frequency of each term (no. of documents containing the term).\n",
    "        for each term.\n",
    "    '''\n",
    "    inv_index_postings = {} #this is dictionary to store the postings for the terms\n",
    "    inv_index_frequency = {} #this is dictionary to store the frequency values for the terms\n",
    "\n",
    "    #Iterate over all the files\n",
    "    for i in range(len(file_toks)):\n",
    "        #For each file, iterate over all the file tokens\n",
    "        for tok in file_toks[i]:\n",
    "            if(tok not in inv_index_postings.keys()): #if the token is not yet present as a term in the index\n",
    "                inv_index_postings[tok] = [i] #create a new entry for the term in the index and intilize it with document_ID 'i'\n",
    "                inv_index_frequency[tok] = 1 #create a new entry for the term in the frequency array and initialize frequency value as 1.\n",
    "            else: #else if the token is already present as a term in the index\n",
    "                if(i not in inv_index_postings[tok]): #if the document ID 'i' is not yet added to the posting list for the term\n",
    "                    inv_index_postings[tok].append(i) #then add that\n",
    "                    inv_index_frequency[tok] += 1 #also increment the frequency value by 1\n",
    "\n",
    "    inv_index_postings = dict(sorted(inv_index_postings.items())) #sort the index in alphabetical order wrt terms\n",
    "    terms_list = inv_index_postings.keys()\n",
    "    for word in terms_list:\n",
    "        inv_index_postings[word].sort() #sort the indivdual posting lists correponging to each term in the index\n",
    "    return inv_index_postings, inv_index_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocsFromID(docID_to_docs, doc_IDs):\n",
    "    '''\n",
    "        Given a list of document IDs, it outputs the document names corresponding to thos IDs.\n",
    "        Input: docID_to_docs (mapping between docID -> doc_name), docIDs - list of input document IDs\n",
    "        Returns: doc_names - list of doc_names corresponding to document IDs in doc_IDs\n",
    "    '''\n",
    "    doc_names = []\n",
    "    for doc_ID in doc_IDs:\n",
    "        doc_names.append(docID_to_docs[doc_ID])\n",
    "    return doc_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive_postings(inv_index_postings, term):\n",
    "    '''\n",
    "        Given a term, retreive its posting list.\n",
    "        Input: inv_index_postings - inverted index postings, term\n",
    "        Returns: [] if term not in index, posting list for the term otherwise\n",
    "    '''\n",
    "    terms_list = inv_index_postings.keys()\n",
    "    if(term not in terms_list):\n",
    "        return []\n",
    "    else:\n",
    "        posting = inv_index_postings[term]\n",
    "        return posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive_frequency(inv_index_frequency, term):\n",
    "    '''\n",
    "        Given a term, retreive its frequency value.\n",
    "        Input: inv_index_frequency - inverted index frequency array, term\n",
    "        Returns: 0 if term not in index, frequency value for the term otherwise\n",
    "    '''\n",
    "    terms_list = inv_index_frequency.keys()\n",
    "    if(term not in terms_list):\n",
    "        return 0\n",
    "    else:\n",
    "        freq = inv_index_frequency[term]\n",
    "        return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive_term_info(inv_index_postings, inv_index_frequency, term):\n",
    "    '''\n",
    "        Given a term, retreive both its posting list & frequency value.\n",
    "        Input: inv_index_postings - inverted index (posting lists for all the terms), inv_index_frequency - inverted index frequency array, term\n",
    "        Returns: [], 0 if term not in index; posting, frequency value for the term otherwise\n",
    "    '''\n",
    "    terms_list = inv_index_postings.keys()\n",
    "    if(term not in terms_list):\n",
    "        return [], 0\n",
    "    else:\n",
    "        posting = inv_index_postings[term]\n",
    "        freq = inv_index_frequency[term]\n",
    "        return posting, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_AND(posting1, posting2, verbose=False):\n",
    "    '''\n",
    "        Implementation of the AND query\n",
    "    '''\n",
    "    #posting1, posting2 - posting list of the arguments of the AND operation\n",
    "\n",
    "    #If either posting1 or posting2 is empty their AND will return an empty list\n",
    "    if(posting1 == [] or posting2 == []):\n",
    "        return 0, 0, [], []\n",
    "\n",
    "    #We will iterate over both the posting lists and perform their intersection by using a merging based algorithm\n",
    "    ptr1 = 0 #pointer that iterates over posting1\n",
    "    ptr2 = 0 #pointer that iterates over posting2\n",
    "    answer_docID = [] #contains the answer\n",
    "\n",
    "    num_comparisons = 0 #intialize number of comparison as 0\n",
    "\n",
    "    while(ptr1 < len(posting1) and ptr2 < len(posting2)): #checking that both pointers remain in range\n",
    "        num_comparisons += 1 #increment the number of comparisons by one\n",
    "        if(posting1[ptr1] == posting2[ptr2]):\n",
    "            #when the the value pointed by ptr1 and ptr2 is same, means that this is a common document ID \n",
    "            #present in both posting1 and positng2 and henche will be included in our answer.\n",
    "            answer_docID.append(posting1[ptr1])\n",
    "            ptr1 += 1\n",
    "            ptr2 += 1\n",
    "        #otherwise move the pointer pointing to a smaller value forward by one\n",
    "        elif(posting1[ptr1] < posting2[ptr2]):\n",
    "            ptr1 += 1\n",
    "        else:\n",
    "            ptr2 += 1\n",
    "\n",
    "    num_docs_retreived = len(answer_docID) #number of docs retreived\n",
    "    doc_names_retreived = getDocsFromID(docID_to_doc_mapping, answer_docID) #names of docs retreived\n",
    "\n",
    "    if(verbose==True):\n",
    "        print(f\"AND Query: \\nNo. of documents retreived: {num_docs_retreived}\\nMinimum number of comparisons: {num_comparisons}\\nNames of retreived documents: {doc_names_retreived}\")\n",
    "\n",
    "    return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID\n",
    "\n",
    "def query_OR(posting1, posting2, verbose=False):\n",
    "    '''\n",
    "        Implementation of the OR query\n",
    "    '''\n",
    "\n",
    "    #posting1, posting2 - posting list of the arguments of the OR operation\n",
    "\n",
    "    #if both posting1 and posting2 are empty lists, their OR i.e a  kind of union will be empty\n",
    "    if((posting1 == []) and (posting2 == [])):\n",
    "        return 0, 0, [], []\n",
    "    #else if posting1 is empty but posting2 is not, then their OR i.e a  kind of union will only contain all values of posting2\n",
    "    elif((posting1 == []) and (posting2 != [])):\n",
    "        ans_docs = posting2\n",
    "        return len(ans_docs), 0, getDocsFromID(docID_to_doc_mapping, ans_docs), ans_docs\n",
    "    #else if posting2 is empty but posting1 is not, then their OR i.e a  kind of union will only contain all values of posting1\n",
    "    elif((posting1 != []) and (posting2 == [])):\n",
    "        ans_docs = posting1\n",
    "        return len(ans_docs), 0, getDocsFromID(docID_to_doc_mapping, ans_docs), ans_docs\n",
    "    #else when none of them are empty\n",
    "    else:\n",
    "\n",
    "        #We will iterate over both the posting lists and perform their union by using a merging based algorithm\n",
    "        ptr1 = 0 #pointer that iterates over posting1\n",
    "        ptr2 = 0 #pointer that iterates over posting2\n",
    "        answer_docID = [] #contains the answer\n",
    "\n",
    "        num_comparisons = 0 #intialize number of comparison as 0\n",
    "        \n",
    "        while(ptr1 < len(posting1) and ptr2 < len(posting2)): #checking that both pointers remain in range\n",
    "            num_comparisons += 1 #increment the number of comparisons by 1\n",
    "            if(posting1[ptr1] == posting2[ptr2]):\n",
    "            #when the the value pointed by ptr1 and ptr2 is same, means that this is a common document ID \n",
    "            #present in both posting1 and positng2 and hence it will be included only once in our answer.\n",
    "                answer_docID.append(posting1[ptr1])\n",
    "                ptr1 += 1\n",
    "                ptr2 += 1\n",
    "            #otherwise include the document ID having lower magnitude and move the pointer pointing to a smaller value forward by one\n",
    "            elif(posting1[ptr1] < posting2[ptr2]):\n",
    "                answer_docID.append(posting1[ptr1])\n",
    "                ptr1 += 1\n",
    "            else:\n",
    "                answer_docID.append(posting2[ptr2])\n",
    "                ptr2 += 1\n",
    "        #If the posting1 list has not been completly iterated, finish iterating it anc include all the values remaining in that in our answer\n",
    "        while(ptr1 < len(posting1)):\n",
    "            answer_docID.append(posting1[ptr1])\n",
    "            ptr1 += 1\n",
    "        #If the posting2 list has not been completly iterated, finish iterating it anc include all the values remaining in that in our answer\n",
    "        while(ptr2 < len(posting2)):\n",
    "            answer_docID.append(posting2[ptr2])\n",
    "            ptr2 += 1\n",
    "\n",
    "        num_docs_retreived = len(answer_docID) #number of docs retreived\n",
    "        doc_names_retreived = getDocsFromID(docID_to_doc_mapping, answer_docID) #names of docs received\n",
    "\n",
    "        if(verbose==True):\n",
    "            print(f\"Query OR\\nNo. of documents retreived: {num_docs_retreived}\\nMinimum number of comparisons: {num_comparisons}\\nNames of retreived documents: {doc_names_retreived}\")\n",
    "        \n",
    "        return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID\n",
    "\n",
    "def perform_NOT(posting):\n",
    "    '''\n",
    "        Perform the NOT operation given a posting list\n",
    "    '''\n",
    "    #Here we simply return all the valid values(/docIDs) that are not present the input posting list\n",
    "    all_docIDs = [docID for docID in range(len(file_names))]\n",
    "    if(posting == []): #if the input posting list is empty  return all document IDs\n",
    "        return all_docIDs\n",
    "    for docID in posting: #else remove the document ID from the universal set\n",
    "        all_docIDs.remove(docID)\n",
    "    return all_docIDs\n",
    "\n",
    "def query_AND_NOT(posting1, posting2, verbose=False):\n",
    "    '''\n",
    "        Perform AND NOT query\n",
    "    '''\n",
    "\n",
    "    if(posting1 == []): #if posting1 is empty, then it's AND with anything will be empty\n",
    "        return 0, 0, [], []\n",
    "    \n",
    "    ptr1 = 0 #pointer that iterates over posting1\n",
    "    ptr2 = 0 #pointer that iterates over posting2\n",
    "    answer_docID = [] #stores the answers\n",
    "\n",
    "    num_comparisons = 0 #initialize number of comparisons as zero\n",
    "\n",
    "    while(ptr1 < len(posting1) and ptr2 < len(posting2)): #checking that both pointers remain in range\n",
    "        num_comparisons += 1 #increment the number of comparisons by 1\n",
    "        if(posting1[ptr1] == posting2[ptr2]):\n",
    "            #when the the value pointed by ptr1 and ptr2 is same, means that this is a common document ID \n",
    "            #present in both posting1 and positng2 and hence it will not be included in our answer.\n",
    "            ptr1 += 1\n",
    "            ptr2 += 1\n",
    "        elif(posting1[ptr1] < posting2[ptr2]): #if ptr1 lags behind ptr2 then we have to add the element of posting1 to our answer and increment ptr1\n",
    "            answer_docID.append(posting1[ptr1])\n",
    "            ptr1 += 1\n",
    "        else: #else do not add anything to answer and increment ptr2\n",
    "            ptr2 += 1\n",
    "    while(ptr1 < len(posting1)): #if anything is left in posting1 then add it till ptr1 reaches the end\n",
    "        answer_docID.append(posting1[ptr1])\n",
    "        ptr1 += 1\n",
    "    \n",
    "    num_docs_retreived = len(answer_docID) #number of docs retreived\n",
    "    doc_names_retreived = getDocsFromID(docID_to_doc_mapping, answer_docID) #names of docs retreived\n",
    "\n",
    "    if(verbose==True):\n",
    "        print(f\"Query AND NOT\\nNo. of documents retreived: {num_docs_retreived}\\nMinimum number of comparisons: {num_comparisons}\\nNames of retreived documents: {doc_names_retreived}\")\n",
    "    \n",
    "    return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID\n",
    "\n",
    "def query_OR_NOT(posting1, posting2, verbose=False):\n",
    "    '''\n",
    "        Perform OR NOT query\n",
    "    '''\n",
    "    #Here we follow the approach that to that to do: x OR NOT y, we first calculate NOT y and then calculate its OR with x.\n",
    "\n",
    "    posting2_NOT = perform_NOT(posting2) #perform NOT of posting2\n",
    "\n",
    "    #if both posting1 and posting2_NOT are empty lists, their OR i.e a  kind of union will be empty\n",
    "    if((posting1 == []) and (posting2_NOT == [])):\n",
    "        return 0, 0, [], []\n",
    "    #else if posting1 is empty but posting2_NOT is not, then their OR i.e a  kind of union will only contain all values of posting2_NOT\n",
    "    elif((posting1 == []) and (posting2_NOT != [])):\n",
    "        ans_docs = posting2_NOT\n",
    "        return len(ans_docs), 0, getDocsFromID(docID_to_doc_mapping, ans_docs), ans_docs\n",
    "    #else if posting2_NOT is empty but posting1 is not, then their OR i.e a  kind of union will only contain all values of posting1\n",
    "    elif((posting1 != []) and (posting2_NOT == [])):\n",
    "        ans_docs = posting1\n",
    "        return len(ans_docs), 0, getDocsFromID(docID_to_doc_mapping, ans_docs), ans_docs\n",
    "    else:\n",
    "        \n",
    "        #now we will perform AND of posting1 and posting2_NOT. We will iterate over both the posting lists and perform their union by using a merging based algorithm\n",
    "        ptr1 = 0 #pointer that iterates over posting1\n",
    "        ptr2 = 0 #pointer that iterates over posting2_NOT\n",
    "        answer_docID = [] #contains the answer(docIDs)\n",
    "        num_comparisons = 0 #initialize number of comparisons as 0\n",
    "\n",
    "        while(ptr1 < len(posting1) and ptr2 < len(posting2_NOT)): #checking that both pointers remain in range\n",
    "            num_comparisons += 1 #increment the number of comparisons by one\n",
    "            if(posting1[ptr1] == posting2_NOT[ptr2]):\n",
    "                #when the the value pointed by ptr1 and ptr2 is same, means that this is a common document ID \n",
    "                #present in both posting1 and positng2 and hence it will be included only once in our answer.\n",
    "                answer_docID.append(posting1[ptr1])\n",
    "                ptr1 += 1\n",
    "                ptr2 += 1\n",
    "            #otherwise include the document ID having lower magnitude and move the pointer pointing to a smaller value forward by one\n",
    "            elif(posting1[ptr1] < posting2_NOT[ptr2]):\n",
    "                answer_docID.append(posting1[ptr1])\n",
    "                ptr1 += 1\n",
    "            else:\n",
    "                answer_docID.append(posting2_NOT[ptr2])\n",
    "                ptr2 += 1\n",
    "        #If the posting1 list has not been completly iterated, finish iterating it and include all the values remaining in that in our answer\n",
    "        while(ptr1 < len(posting1)):\n",
    "            answer_docID.append(posting1[ptr1])\n",
    "            ptr1 += 1\n",
    "        #If the posting2_NOT list has not been completly iterated, finish iterating it anc include all the values remaining in that in our answer\n",
    "        while(ptr2 < len(posting2_NOT)):\n",
    "            answer_docID.append(posting2_NOT[ptr2])\n",
    "            ptr2 += 1\n",
    "\n",
    "\n",
    "        num_docs_retreived = len(answer_docID) #number of docs retreived\n",
    "        doc_names_retreived = getDocsFromID(docID_to_doc_mapping, answer_docID) #names of docs retreived\n",
    "\n",
    "        if(verbose==True):\n",
    "            print(f\"Query OR NOT\\nNo. of documents retreived: {num_docs_retreived}\\nMinimum number of comparisons: {num_comparisons}\\nNames of retreived documents: {doc_names_retreived}\")\n",
    "        \n",
    "        return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ops(ops_string):\n",
    "    '''\n",
    "        This function processes the input operation sequence and converts that into an array containing the operators that have to be applied\n",
    "    '''\n",
    "    ops = ops_string.split(\",\")\n",
    "    ops = [op.strip() for op in ops]\n",
    "    for op in ops:\n",
    "        if(not(op == \"AND\" or op == \"OR\" or op == \"OR NOT\" or op == \"AND NOT\")):\n",
    "            return []\n",
    "    return ops\n",
    "\n",
    "def perform_query(posting1, posting2, op):\n",
    "    '''\n",
    "        Given the postings and an operator(string op), call this function calls the appropriate operation(AND/OR/AND NOT/OR NOT)\n",
    "    '''\n",
    "    if(op == \"AND\"):\n",
    "        num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID = query_AND(posting1, posting2)\n",
    "        return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID\n",
    "    elif(op == \"OR\"):\n",
    "        num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID = query_OR(posting1, posting2)\n",
    "        return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID\n",
    "    elif(op == \"AND NOT\"):\n",
    "        num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID = query_AND_NOT(posting1, posting2)\n",
    "        return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID\n",
    "    elif(op == \"OR NOT\"):\n",
    "        num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID = query_OR_NOT(posting1, posting2)\n",
    "        return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID\n",
    "    else:\n",
    "        print(\"INVALID OPERATOR\")\n",
    "        return []\n",
    "\n",
    "def process_query(inv_index, query_tokens, ops):\n",
    "    '''\n",
    "        This function processes the whole query sequentially.\n",
    "    '''\n",
    "    n_qtoks = len(query_tokens)\n",
    "    n_ops = n_qtoks - 1 #setting the needed number of operators as number of query tokens minus one\n",
    "    if(len(ops) > n_ops): #if there are more operators than needed\n",
    "        print(f\"Insufficient tokens in the query / Excess operators\")\n",
    "        print(\"\\n----------------------------------------------\\n\")\n",
    "        return -1, -1, []\n",
    "    elif(len(ops) < n_ops): #if there are less operators than needed\n",
    "        print(f\"Insufficient operators / Excess query tokens\")\n",
    "        print(\"\\n----------------------------------------------\\n\")\n",
    "        return -1, -1, []\n",
    "    else:\n",
    "        total_comparisons = 0 #initiliazing the total number of comparisons for the whole query as zero\n",
    "        if(n_ops == 1): #if there is only one operator\n",
    "            p1 = retreive_postings(inv_index, query_tokens[0])\n",
    "            p2 = retreive_postings(inv_index,query_tokens[1])\n",
    "            num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID = perform_query(p1, p2, ops[0])\n",
    "            return num_docs_retreived, num_comparisons, doc_names_retreived\n",
    "        else:\n",
    "            #sequentially keep on processing the operations given\n",
    "            p1 = retreive_postings(inv_index, query_tokens[0])\n",
    "            p2 = retreive_postings(inv_index, query_tokens[1])\n",
    "            ndocs, total_comparisons, doc_names, ans = perform_query(p1, p2, ops[0])\n",
    "            qtok_ptr = 2\n",
    "            for i in range(1, len(ops)): #sequentially take up each operator and perform the operation between previous operations answer and the next token\n",
    "                posting_tok = retreive_postings(inv_index, query_tokens[qtok_ptr])\n",
    "                ndocs, cmps, doc_names, ans = perform_query(ans, posting_tok, ops[i])\n",
    "                total_comparisons += cmps #increment the total number of comparisons\n",
    "                qtok_ptr += 1\n",
    "            num_docs_retreived = len(ans) #number of documents finally retreived\n",
    "            doc_names_retreived = getDocsFromID(docID_to_doc_mapping, ans) #names of the documents finally retreived\n",
    "            return num_docs_retreived, total_comparisons, doc_names_retreived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index_postings, inverted_index_frequency = create_inverted_index(file_toks) #creating the inverted index and inverted_index_frequency dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input query sequence : deem lion\n",
      "Input operator sequence : [OR]\n",
      "Input operator (array) : ['OR']\n",
      "Preprocessed query tokens : ['deem', 'lion']\n",
      "\n",
      "Query : deem OR lion \n",
      "\n",
      "No. of documents retreived: 23\n",
      "Number of comparisons: 19\n",
      "Names of retreived documents: ['bitnet.txt', 'boneles2.txt', 'booze1.fun', 'christop.int', 'collected_quotes.txt', 'computer.txt', 'deep.txt', 'dromes.txt', 'dthought.txt', 'filmgoof.txt', 'japantv.txt', 'lion.jok', 'lion.txt', 'lions.cat', 'llong.hum', 'murphys.txt', 'murphy_l.txt', 'onetotwo.hum', 'pracjoke.txt', 'puzzles.jok', 'stuf10.txt', 'three.txt', 'tpquotes.txt']\n",
      "\n",
      "----------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_queries = int(input(\"Enter the number of queries you want to run : \"))\n",
    "for i in range(num_queries):\n",
    "    input_query_sequence = input(\"Enter the space-separated query terms : \")\n",
    "    input_operator_sequence = input(\"Enter the comma-separated operator sequence enclosed in '[]' : \")\n",
    "    print(f\"Input query sequence : {input_query_sequence}\")\n",
    "    print(f\"Input operator sequence : {input_operator_sequence}\")\n",
    "    input_operators = process_ops(input_operator_sequence[1:len(input_operator_sequence) - 1])\n",
    "    query_tokens = preprocess_query(input_query_sequence)\n",
    "    print(f\"Input operator (array) : {input_operators}\")\n",
    "    print(f\"Preprocessed query tokens : {query_tokens}\\n\")\n",
    "    query_result_ndocs, query_result_total_cmps, query_result_doc_names = process_query(inverted_index_postings, query_tokens, input_operators)\n",
    "    \n",
    "    if(query_result_ndocs != -1):\n",
    "        query_generated = query_tokens[0] + \" \" + input_operators[0] + \" \" + query_tokens[1] + \" \"\n",
    "        ptr_qtoks = 2\n",
    "        for i in range(1, len(input_operators)):\n",
    "            query_generated += input_operators[i] + \" \"\n",
    "            query_generated += query_tokens[ptr_qtoks] + \" \"\n",
    "            ptr_qtoks += 1\n",
    "        print(f\"Query : {query_generated}\\n\")\n",
    "        print(f\"No. of documents retreived: {query_result_ndocs}\\nNumber of comparisons: {query_result_total_cmps}\\nNames of retreived documents: {query_result_doc_names}\")\n",
    "        print(\"\\n----------------------------------------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2370e07001db70a9c24f7e21173c51fbc4321340913a02830aed4885459fa0a0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
