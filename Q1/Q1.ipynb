{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# from substitutions import appos\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import string\n",
    "import numpy as np\n",
    "import contractions\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../Humor,Hist,Media,Food'\n",
    "file_names = os.listdir(data_dir) #reading the data directory to list all the files\n",
    "file_paths = [(data_dir + '/' + fname) for fname in file_names] #forming file paths\n",
    "docID_to_doc_mapping = {} #forming docID to doc name mapping\n",
    "for i in range(len(file_names)):\n",
    "    docID_to_doc_mapping[i] = file_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_files(fpaths):\n",
    "    '''\n",
    "        Reads the files and pre process every file's text to form word tokens for every file.\n",
    "        Returns a 2-D list containing word tokens for every file\n",
    "    '''\n",
    "    file_tokens = []\n",
    "    for fpath in fpaths:\n",
    "        f = open(fpath, 'r', encoding='utf-8', errors='replace') #open the file\n",
    "        ftxt_unprocessed = f.read() #read the text of the file\n",
    "        ftoks = preprocess_file(ftxt_unprocessed) #preprocessing the text to form word tokens\n",
    "        file_tokens.append(ftoks)\n",
    "    return file_tokens\n",
    "\n",
    "def check_alnum(tok):\n",
    "    '''\n",
    "        Remove non-alphanumeric characters from a string\n",
    "    '''\n",
    "    tok = ''.join(x for x in tok if x.isalnum() == True)\n",
    "    return tok\n",
    "def remove_punct(tok):\n",
    "    '''\n",
    "        Remove the punctuation in token\n",
    "    '''\n",
    "    punctuations = string.punctuation\n",
    "    tok = ''.join(x for x in tok if x not in punctuations)\n",
    "    return tok\n",
    "\n",
    "def remove_space(tok):\n",
    "    '''\n",
    "        Remove the spaces in token\n",
    "    '''\n",
    "    tok = ''.join(x for x in tok if x != ' ')\n",
    "    return tok\n",
    "\n",
    "def preprocess_file(file_text):\n",
    "    '''\n",
    "        Preprocess the file text and converting to word tokens\n",
    "        Input: string File text\n",
    "        Returns file_tokens, word tokens for the pre processed text\n",
    "    '''\n",
    "    #converting text to lowercase\n",
    "    text = file_text.lower()\n",
    "\n",
    "    #Fixing the contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    #Performing word tokenization\n",
    "    all_toks = word_tokenize(text)\n",
    "    all_unique_toks = set(all_toks) #taking only the unique tokens\n",
    "\n",
    "    #Omitting all the non-alphanumeric characters in tokens\n",
    "    all_unique_toks = [check_alnum(x) for x in all_unique_toks]\n",
    "\n",
    "    #removing stopwords\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    file_toks = [tok for tok in all_unique_toks if tok not in stop_words]\n",
    "\n",
    "    #removing punctations if any remain\n",
    "    toks_no_punct = []\n",
    "    for tok in file_toks:\n",
    "        ctok = remove_punct(tok)\n",
    "        if(ctok != \"\"):\n",
    "            toks_no_punct.append(ctok)\n",
    "\n",
    "    #removing spaces in any remain & check if it is a number\n",
    "    cleaned_toks = []\n",
    "    for tok in toks_no_punct:\n",
    "        ctok = remove_space(tok)\n",
    "        if(ctok != \"\"):\n",
    "            cleaned_toks.append(ctok)\n",
    "    cleaned_toks = list(set(cleaned_toks))\n",
    "    \n",
    "    #stemming tokens\n",
    "    # stemmer = PorterStemmer()\n",
    "    # stemmed_toks = set()\n",
    "\n",
    "    # for tok in cleaned_toks:\n",
    "    #     stok = stemmer.stem(tok)\n",
    "    #     if(stok != \"\" or (stok not in stop_words)): #consider non-blank tokens and the ones which have length greater than 1\n",
    "    #         stemmed_toks.add(stok)\n",
    "    # print(f\"After stemming and removing single letters :\\n{stemmed_toks}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    final_tokens = [tok for tok in cleaned_toks]\n",
    "\n",
    "    return final_tokens\n",
    "\n",
    "def preprocess_query(query_text):\n",
    "    '''\n",
    "        Performs the pre processing of the query text\n",
    "        Input: A string representing query text\n",
    "        Output: Query word tokens\n",
    "    '''\n",
    "\n",
    "    #Performing lowercasing\n",
    "    text = query_text.lower()\n",
    "    \n",
    "    # print(f\"Original Query Tokens : \\n{word_tokenize(text)}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    \n",
    "    #Fixing the contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    #omitting all non-alphanumeric characters\n",
    "    # text = ''.join([x if (x.isalnum() == True or x == \" \") else \" \" for x in text ])\n",
    "\n",
    "    #word tokenization\n",
    "    all_toks = word_tokenize(text)\n",
    "\n",
    "    #taking only unique tokens\n",
    "    all_unique_toks = []\n",
    "    for tok in all_toks:\n",
    "        if(tok not in all_unique_toks):\n",
    "            all_unique_toks.append(tok)\n",
    "\n",
    "    #Omitting all the non-alphanumeric characters in tokens\n",
    "    all_unique_toks = [check_alnum(x) for x in all_unique_toks]\n",
    "\n",
    "    # print(f\"All unique tokens :\\n{all_unique_toks}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "\n",
    "    #removing stopwords\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    file_toks = [tok for tok in all_unique_toks if tok not in stop_words]\n",
    "    # print(f\"After removing stopwords :\\n{file_toks}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "\n",
    "    #removing punctations if any remain\n",
    "    toks_no_punct = []\n",
    "    for tok in file_toks:\n",
    "        ctok = remove_punct(tok)\n",
    "        # print(f\"-> before removal : {tok} || After removal : {ctok}\")\n",
    "        if(ctok != \"\"):\n",
    "            toks_no_punct.append(ctok)\n",
    "    # print(f\"After removing puncts :\\n{toks_no_punct}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "\n",
    "    #removing spaces in any remain & check if it is a number\n",
    "    cleaned_toks = []\n",
    "    for tok in toks_no_punct:\n",
    "        ctok = remove_space(tok)\n",
    "        # print(f\"-> before removal : {tok} || After removal : {ctok}\")\n",
    "        if(ctok != \"\"):\n",
    "            cleaned_toks.append(ctok)\n",
    "    \n",
    "    #finally taking all the unique query tokens after all the preprocessing steps\n",
    "    final_tokens = []\n",
    "    for tok in cleaned_toks:\n",
    "        if(tok not in final_tokens):\n",
    "            final_tokens.append(tok)\n",
    "\n",
    "    #stemming tokens\n",
    "    # stemmer = PorterStemmer()\n",
    "    # stemmed_toks = set()\n",
    "\n",
    "    # for tok in cleaned_toks:\n",
    "    #     stok = stemmer.stem(tok)\n",
    "    #     if(stok != \"\" and len(stok) > 1 and (stok not in stop_words)): #consider non-blank tokens and the ones which have length greater than 1\n",
    "    #         stemmed_toks.add(stok)\n",
    "    # print(f\"After stemming and removing single letters :\\n{stemmed_toks}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    # final_tokens = [tok for tok in cleaned_toks]\n",
    "\n",
    "    return final_tokens\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the files to extract word tokens from each and every file\n",
    "file_toks = read_files(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index(file_toks):\n",
    "    ''' \n",
    "        This function build the inverted index. It takes in the word tokens of each file as input and returns two dictionaries: \n",
    "        inv_index_postings - corresponding to the posting list of all the terms containg the document IDs correspondg to each term and \n",
    "        inv_index_frequency - corresponding to the document frequency of each term (no. of documents containing the term).\n",
    "        for each term.\n",
    "    '''\n",
    "    inv_index_postings = {} #this is dictionary to store the postings for the terms\n",
    "    inv_index_frequency = {} #this is dictionary to store the frequency values for the terms\n",
    "\n",
    "    #Iterate over all the files\n",
    "    for i in range(len(file_toks)):\n",
    "        #For each file, iterate over all the file tokens\n",
    "        for tok in file_toks[i]:\n",
    "            if(tok not in inv_index_postings.keys()): #if the token is not yet present as a term in the index\n",
    "                inv_index_postings[tok] = [i] #create a new entry for the term in the index and intilize it with document_ID 'i'\n",
    "                inv_index_frequency[tok] = 1 #create a new entry for the term in the frequency array and initialize frequency value as 1.\n",
    "            else: #else if the token is already present as a term in the index\n",
    "                if(i not in inv_index_postings[tok]): #if the document ID 'i' is not yet added to the posting list for the term\n",
    "                    inv_index_postings[tok].append(i) #then add that\n",
    "                    inv_index_frequency[tok] += 1 #also increment the frequency value by 1\n",
    "\n",
    "    inv_index_postings = dict(sorted(inv_index_postings.items())) #sort the index in alphabetical order wrt terms\n",
    "    terms_list = inv_index_postings.keys()\n",
    "    for word in terms_list:\n",
    "        inv_index_postings[word].sort() #sort the indivdual posting lists correponging to each term in the index\n",
    "    return inv_index_postings, inv_index_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocsFromID(docID_to_docs, doc_IDs):\n",
    "    '''\n",
    "        Given a list of document IDs, it outputs the document names corresponding to thos IDs.\n",
    "        Input: docID_to_docs (mapping between docID -> doc_name), docIDs - list of input document IDs\n",
    "        Returns: doc_names - list of doc_names corresponding to document IDs in doc_IDs\n",
    "    '''\n",
    "    doc_names = []\n",
    "    for doc_ID in doc_IDs:\n",
    "        doc_names.append(docID_to_docs[doc_ID])\n",
    "    return doc_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive_postings(inv_index_postings, term):\n",
    "    '''\n",
    "        Given a term, retreive its posting list.\n",
    "        Input: inv_index_postings - inverted index postings, term\n",
    "        Returns: [] if term not in index, posting list for the term otherwise\n",
    "    '''\n",
    "    terms_list = inv_index_postings.keys()\n",
    "    if(term not in terms_list):\n",
    "        return []\n",
    "    else:\n",
    "        posting = inv_index_postings[term]\n",
    "        return posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive_frequency(inv_index_frequency, term):\n",
    "    '''\n",
    "        Given a term, retreive its frequency value.\n",
    "        Input: inv_index_frequency - inverted index frequency array, term\n",
    "        Returns: 0 if term not in index, frequency value for the term otherwise\n",
    "    '''\n",
    "    terms_list = inv_index_frequency.keys()\n",
    "    if(term not in terms_list):\n",
    "        return 0\n",
    "    else:\n",
    "        freq = inv_index_frequency[term]\n",
    "        return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive_term_info(inv_index_postings, inv_index_frequency, term):\n",
    "    '''\n",
    "        Given a term, retreive both its posting list & frequency value.\n",
    "        Input: inv_index_postings - inverted index (posting lists for all the terms), inv_index_frequency - inverted index frequency array, term\n",
    "        Returns: [], 0 if term not in index; posting, frequency value for the term otherwise\n",
    "    '''\n",
    "    terms_list = inv_index_postings.keys()\n",
    "    if(term not in terms_list):\n",
    "        return [], 0\n",
    "    else:\n",
    "        posting = inv_index_postings[term]\n",
    "        freq = inv_index_frequency[term]\n",
    "        return posting, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_AND(posting1, posting2, verbose=False):\n",
    "    '''\n",
    "        Implementation of the AND query\n",
    "    '''\n",
    "    #posting1, posting2 - posting list of the arguments of the AND operation\n",
    "\n",
    "    #If either posting1 or posting2 is empty their AND will return an empty list\n",
    "    if(posting1 == [] or posting2 == []):\n",
    "        return 0, 0, [], []\n",
    "\n",
    "    #We will iterate over both the posting lists and perform their intersection by using a merging based algorithm\n",
    "    ptr1 = 0 #pointer that iterates over posting1\n",
    "    ptr2 = 0 #pointer that iterates over posting2\n",
    "    answer_docID = [] #contains the answer\n",
    "\n",
    "    num_comparisons = 0 #intialize number of comparison as 0\n",
    "\n",
    "    while(ptr1 < len(posting1) and ptr2 < len(posting2)): #checking that both pointers remain in range\n",
    "        num_comparisons += 1 #increment the number of comparisons by one\n",
    "        if(posting1[ptr1] == posting2[ptr2]):\n",
    "            #when the the value pointed by ptr1 and ptr2 is same, means that this is a common document ID \n",
    "            #present in both posting1 and positng2 and henche will be included in our answer.\n",
    "            answer_docID.append(posting1[ptr1])\n",
    "            ptr1 += 1\n",
    "            ptr2 += 1\n",
    "        #otherwise move the pointer pointing to a smaller value forward by one\n",
    "        elif(posting1[ptr1] < posting2[ptr2]):\n",
    "            ptr1 += 1\n",
    "        else:\n",
    "            ptr2 += 1\n",
    "\n",
    "    num_docs_retreived = len(answer_docID) #number of docs retreived\n",
    "    doc_names_retreived = getDocsFromID(docID_to_doc_mapping, answer_docID) #names of docs retreived\n",
    "\n",
    "    if(verbose==True):\n",
    "        print(f\"AND Query: \\nNo. of documents retreived: {num_docs_retreived}\\nMinimum number of comparisons: {num_comparisons}\\nNames of retreived documents: {doc_names_retreived}\")\n",
    "\n",
    "    return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID\n",
    "\n",
    "def query_OR(posting1, posting2, verbose=False):\n",
    "    '''\n",
    "        Implementation of the OR query\n",
    "    '''\n",
    "\n",
    "    #posting1, posting2 - posting list of the arguments of the OR operation\n",
    "\n",
    "    #if both posting1 and posting2 are empty lists, their OR i.e a  kind of union will be empty\n",
    "    if((posting1 == []) and (posting2 == [])):\n",
    "        return 0, 0, [], []\n",
    "    #else if posting1 is empty but posting2 is not, then their OR i.e a  kind of union will only contain all values of posting2\n",
    "    elif((posting1 == []) and (posting2 != [])):\n",
    "        ans_docs = posting2\n",
    "        return len(ans_docs), 0, getDocsFromID(docID_to_doc_mapping, ans_docs), ans_docs\n",
    "    #else if posting2 is empty but posting1 is not, then their OR i.e a  kind of union will only contain all values of posting1\n",
    "    elif((posting1 != []) and (posting2 == [])):\n",
    "        ans_docs = posting1\n",
    "        return len(ans_docs), 0, getDocsFromID(docID_to_doc_mapping, ans_docs), ans_docs\n",
    "    #else when none of them are empty\n",
    "    else:\n",
    "\n",
    "        #We will iterate over both the posting lists and perform their union by using a merging based algorithm\n",
    "        ptr1 = 0 #pointer that iterates over posting1\n",
    "        ptr2 = 0 #pointer that iterates over posting2\n",
    "        answer_docID = [] #contains the answer\n",
    "\n",
    "        num_comparisons = 0 #intialize number of comparison as 0\n",
    "        \n",
    "        while(ptr1 < len(posting1) and ptr2 < len(posting2)): #checking that both pointers remain in range\n",
    "            num_comparisons += 1 #increment the number of comparisons by 1\n",
    "            if(posting1[ptr1] == posting2[ptr2]):\n",
    "            #when the the value pointed by ptr1 and ptr2 is same, means that this is a common document ID \n",
    "            #present in both posting1 and positng2 and hence it will be included only once in our answer.\n",
    "                answer_docID.append(posting1[ptr1])\n",
    "                ptr1 += 1\n",
    "                ptr2 += 1\n",
    "            #otherwise include the document ID having lower magnitude and move the pointer pointing to a smaller value forward by one\n",
    "            elif(posting1[ptr1] < posting2[ptr2]):\n",
    "                answer_docID.append(posting1[ptr1])\n",
    "                ptr1 += 1\n",
    "            else:\n",
    "                answer_docID.append(posting2[ptr2])\n",
    "                ptr2 += 1\n",
    "        #If the posting1 list has not been completly iterated, finish iterating it anc include all the values remaining in that in our answer\n",
    "        while(ptr1 < len(posting1)):\n",
    "            answer_docID.append(posting1[ptr1])\n",
    "            ptr1 += 1\n",
    "        #If the posting2 list has not been completly iterated, finish iterating it anc include all the values remaining in that in our answer\n",
    "        while(ptr2 < len(posting2)):\n",
    "            answer_docID.append(posting2[ptr2])\n",
    "            ptr2 += 1\n",
    "\n",
    "        num_docs_retreived = len(answer_docID) #number of docs retreived\n",
    "        doc_names_retreived = getDocsFromID(docID_to_doc_mapping, answer_docID) #names of docs received\n",
    "\n",
    "        if(verbose==True):\n",
    "            print(f\"Query OR\\nNo. of documents retreived: {num_docs_retreived}\\nMinimum number of comparisons: {num_comparisons}\\nNames of retreived documents: {doc_names_retreived}\")\n",
    "        \n",
    "        return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID\n",
    "\n",
    "def perform_NOT(posting):\n",
    "    '''\n",
    "        Perform the NOT operation given a posting list\n",
    "    '''\n",
    "    #Here we simply return all the valid values(/docIDs) that are not present the input posting list\n",
    "    all_docIDs = [docID for docID in range(len(file_names))]\n",
    "    if(posting == []): #if the input posting list is empty  return all document IDs\n",
    "        return all_docIDs\n",
    "    for docID in posting: #else remove the document ID from the universal set\n",
    "        all_docIDs.remove(docID)\n",
    "    return all_docIDs\n",
    "\n",
    "def query_AND_NOT(posting1, posting2, verbose=False):\n",
    "    '''\n",
    "        Perform AND NOT query\n",
    "    '''\n",
    "\n",
    "    if(posting1 == []):\n",
    "        return 0, 0, [], []\n",
    "    \n",
    "    ptr1 = 0 #pointer that iterates over posting1\n",
    "    ptr2 = 0 #pointer that iterates over posting2\n",
    "    answer_docID = [] #stores the answers\n",
    "\n",
    "    num_comparisons = 0\n",
    "\n",
    "    while(ptr1 < len(posting1) and ptr2 < len(posting2)):\n",
    "        num_comparisons += 1\n",
    "        if(posting1[ptr1] == posting2[ptr2]):\n",
    "            ptr1 += 1\n",
    "            ptr2 += 1\n",
    "        elif(posting1[ptr1] < posting2[ptr2]):\n",
    "            answer_docID.append(posting1[ptr1])\n",
    "            ptr1 += 1\n",
    "        else:\n",
    "            ptr2 += 1\n",
    "    while(ptr1 < len(posting1)):\n",
    "        answer_docID.append(posting1[ptr1])\n",
    "        ptr1 += 1\n",
    "    \n",
    "    num_docs_retreived = len(answer_docID) #number of docs retreived\n",
    "    doc_names_retreived = getDocsFromID(docID_to_doc_mapping, answer_docID) #names of docs retreived\n",
    "\n",
    "    if(verbose==True):\n",
    "        print(f\"Query AND NOT\\nNo. of documents retreived: {num_docs_retreived}\\nMinimum number of comparisons: {num_comparisons}\\nNames of retreived documents: {doc_names_retreived}\")\n",
    "    \n",
    "    return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID\n",
    "\n",
    "def query_OR_NOT(posting1, posting2, verbose=False):\n",
    "    '''\n",
    "        Perform OR NOT query\n",
    "    '''\n",
    "    #Here we follow the approach that to that to do: x OR NOT y, we first calculate NOT y and then calculate its OR with x.\n",
    "\n",
    "    posting2_NOT = perform_NOT(posting2) #perform NOT of posting2\n",
    "\n",
    "    #if both posting1 and posting2_NOT are empty lists, their OR i.e a  kind of union will be empty\n",
    "    if((posting1 == []) and (posting2_NOT == [])):\n",
    "        return 0, 0, [], []\n",
    "    #else if posting1 is empty but posting2_NOT is not, then their OR i.e a  kind of union will only contain all values of posting2_NOT\n",
    "    elif((posting1 == []) and (posting2_NOT != [])):\n",
    "        ans_docs = posting2_NOT\n",
    "        return len(ans_docs), 0, getDocsFromID(docID_to_doc_mapping, ans_docs), ans_docs\n",
    "    #else if posting2_NOT is empty but posting1 is not, then their OR i.e a  kind of union will only contain all values of posting1\n",
    "    elif((posting1 != []) and (posting2_NOT == [])):\n",
    "        ans_docs = posting1\n",
    "        return len(ans_docs), 0, getDocsFromID(docID_to_doc_mapping, ans_docs), ans_docs\n",
    "    else:\n",
    "        \n",
    "        #now we will perform AND of posting1 and posting2_NOT. We will iterate over both the posting lists and perform their union by using a merging based algorithm\n",
    "        ptr1 = 0 #pointer that iterates over posting1\n",
    "        ptr2 = 0 #pointer that iterates over posting2_NOT\n",
    "        answer_docID = [] #contains the answer(docIDs)\n",
    "        num_comparisons = 0 #initialize number of comparisons as 0\n",
    "\n",
    "        while(ptr1 < len(posting1) and ptr2 < len(posting2_NOT)): #checking that both pointers remain in range\n",
    "            num_comparisons += 1 #increment the number of comparisons by one\n",
    "            if(posting1[ptr1] == posting2_NOT[ptr2]):\n",
    "                #when the the value pointed by ptr1 and ptr2 is same, means that this is a common document ID \n",
    "                #present in both posting1 and positng2 and hence it will be included only once in our answer.\n",
    "                answer_docID.append(posting1[ptr1])\n",
    "                ptr1 += 1\n",
    "                ptr2 += 1\n",
    "            #otherwise include the document ID having lower magnitude and move the pointer pointing to a smaller value forward by one\n",
    "            elif(posting1[ptr1] < posting2_NOT[ptr2]):\n",
    "                answer_docID.append(posting1[ptr1])\n",
    "                ptr1 += 1\n",
    "            else:\n",
    "                answer_docID.append(posting2_NOT[ptr2])\n",
    "                ptr2 += 1\n",
    "        #If the posting1 list has not been completly iterated, finish iterating it and include all the values remaining in that in our answer\n",
    "        while(ptr1 < len(posting1)):\n",
    "            answer_docID.append(posting1[ptr1])\n",
    "            ptr1 += 1\n",
    "        #If the posting2_NOT list has not been completly iterated, finish iterating it anc include all the values remaining in that in our answer\n",
    "        while(ptr2 < len(posting2_NOT)):\n",
    "            answer_docID.append(posting2_NOT[ptr2])\n",
    "            ptr2 += 1\n",
    "\n",
    "\n",
    "        num_docs_retreived = len(answer_docID) #number of docs retreived\n",
    "        doc_names_retreived = getDocsFromID(docID_to_doc_mapping, answer_docID) #names of docs retreived\n",
    "\n",
    "        if(verbose==True):\n",
    "            print(f\"Query OR NOT\\nNo. of documents retreived: {num_docs_retreived}\\nMinimum number of comparisons: {num_comparisons}\\nNames of retreived documents: {doc_names_retreived}\")\n",
    "        \n",
    "        return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ops(ops_string):\n",
    "    '''\n",
    "        This function processes the input operation sequence and converts that into an array containing the operators that have to be applied\n",
    "    '''\n",
    "    ops = ops_string.split(\",\")\n",
    "    ops = [op.strip() for op in ops]\n",
    "    for op in ops:\n",
    "        if(not(op == \"AND\" or op == \"OR\" or op == \"OR NOT\" or op == \"AND NOT\")):\n",
    "            return []\n",
    "    return ops\n",
    "\n",
    "def perform_query(posting1, posting2, op):\n",
    "    '''\n",
    "        Given the postings and an operator(string op), call this function calls the appropriate operation(AND/OR/AND NOT/OR NOT)\n",
    "    '''\n",
    "    if(op == \"AND\"):\n",
    "        num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID = query_AND(posting1, posting2)\n",
    "        return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID\n",
    "    elif(op == \"OR\"):\n",
    "        num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID = query_OR(posting1, posting2)\n",
    "        return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID\n",
    "    elif(op == \"AND NOT\"):\n",
    "        num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID = query_AND_NOT(posting1, posting2)\n",
    "        return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID\n",
    "    elif(op == \"OR NOT\"):\n",
    "        num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID = query_OR_NOT(posting1, posting2)\n",
    "        return num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID\n",
    "    else:\n",
    "        print(\"INVALID OPERATOR\")\n",
    "        return []\n",
    "\n",
    "def process_query(inv_index, query_tokens, ops):\n",
    "    '''\n",
    "        This function processes the whole query sequentially.\n",
    "    '''\n",
    "    n_qtoks = len(query_tokens)\n",
    "    n_ops = n_qtoks - 1 #setting the needed number of operators as number of query tokens minus one\n",
    "    if(len(ops) > n_ops): #if there are more operators than needed\n",
    "        print(f\"Insufficient tokens in the query / Excess operators\\n\")\n",
    "        return -1, -1, []\n",
    "    elif(len(ops) < n_ops): #if there are less operators than needed\n",
    "        print(f\"Insufficient operators / Excess query tokens\")\n",
    "        return -1, -1, []\n",
    "    else:\n",
    "        total_comparisons = 0 #initiliazing the total number of comparisons for the whole query as zero\n",
    "        if(n_ops == 1): #if there is only one operator\n",
    "            p1 = retreive_postings(inv_index, query_tokens[0])\n",
    "            p2 = retreive_postings(inv_index,query_tokens[1])\n",
    "            num_docs_retreived, num_comparisons, doc_names_retreived, answer_docID = perform_query(p1, p2, ops[0])\n",
    "            return num_docs_retreived, num_comparisons, doc_names_retreived\n",
    "        else:\n",
    "            #sequentially keep on processing the operations given\n",
    "            p1 = retreive_postings(inv_index, query_tokens[0])\n",
    "            p2 = retreive_postings(inv_index, query_tokens[1])\n",
    "            ndocs, total_comparisons, doc_names, ans = perform_query(p1, p2, ops[0])\n",
    "            qtok_ptr = 2\n",
    "            for i in range(1, len(ops)): #sequentially take up each operator and perform the operation between previous operations answer and the next token\n",
    "                posting_tok = retreive_postings(inv_index, query_tokens[qtok_ptr])\n",
    "                ndocs, cmps, doc_names, ans = perform_query(ans, posting_tok, ops[i])\n",
    "                total_comparisons += cmps #increment the total number of comparisons\n",
    "                qtok_ptr += 1\n",
    "            num_docs_retreived = len(ans) #number of documents finally retreived\n",
    "            doc_names_retreived = getDocsFromID(docID_to_doc_mapping, ans) #names of the documents finally retreived\n",
    "            return num_docs_retreived, total_comparisons, doc_names_retreived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index_postings, inverted_index_frequency = create_inverted_index(file_toks) #creating the inverted index and inverted_index_frequency dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input query sequence : jungle samyak dance happy\n",
      "Input operator sequence : [AND NOT, OR, OR NOT]\n",
      "Input operator (array) : ['AND NOT', 'OR', 'OR NOT']\n",
      "Query tokens : ['jungle', 'samyak', 'dance', 'happy']\n",
      "\n",
      "Query : jungle AND NOT samyak OR dance OR NOT happy \n",
      "\n",
      "\n",
      "\n",
      "No. of documents retreived: 1011\n",
      "Number of comparisons: 1077\n",
      "Names of retreived documents: ['1st_aid.txt', 'a-team', 'abbott.txt', 'aboutada.txt', 'acetab1.txt', 'aclamt.txt', 'acne1.txt', 'acronym.lis', 'acronym.txt', 'acronyms.txt', 'adameve.hum', 'adcopy.hum', 'addrmeri.txt', 'admin.txt', 'ads.txt', 'adt_miam.txt', 'advrtize.txt', 'aeonint.txt', 'age.txt', 'aggie.txt', 'aids.txt', 'airlines', 'alabama.txt', 'alcatax.txt', 'alcohol.hum', 'alflog.txt', 'allusion', 'ambrose.bie', 'amchap2.txt', 'analogy.hum', 'aniherb.txt', 'annoy.fascist', 'anorexia.txt', 'answers', 'anthropo.stu', 'antibiot.txt', 'antimead.bev', 'aphrodis.txt', 'appbred.brd', 'applepie.des', 'apsaucke.des', 'apsnet.txt', 'arab.dic', 'arcadian.txt', 'argotdic.txt', 'arnold.txt', 'art-fart.hum', 'arthriti.txt', 'ateam.epi', 'atherosc.txt', 'att.txt', 'aussie.lng', 'avengers.lis', 'awespinh.sal', 'ayurved.txt', 'a_fish_c.apo', 'a_tv_t-p.com', 'b-2.jok', 'b12.txt', 'back1.txt', 'bad', 'bad-d', 'bad.jok', 'badday.hum', 'bagelope.txt', 'bakebred.txt', 'baklava.des', 'banana01.brd', 'banana02.brd', 'banana03.brd', 'banana04.brd', 'banana05.brd', 'bank.rob', 'barney.cn1', 'barney.txt', 'basehead.txt', 'batrbred.txt', 'bb', 'bbc_vide.cat', 'bbh_intv.txt', 'bbq.txt', 'beapimp.hum', 'beauty.tm', 'beave.hum', 'beer-g', 'beer-gui', 'beer.gam', 'beer.txt', 'beerdiag.txt', 'beergame.hum', 'beergame.txt', 'beerjesus.hum', 'beershrm.fis', 'beershrp.fis', 'beerwarn.txt', 'beesherb.txt', 'beginn.ers', 'berryeto.bev', 'bhang.fun', 'bhb.ill', 'bible.txt', 'bigpic1.hum', 'billcat.hum', 'bimg.prn', 'bingbong.hum', 'bitchcar.hum', 'bitnet.txt', 'blackadd', 'blackapp.hum', 'blake7.lis', 'bless.bc', 'blkbean.txt', 'blkbnsrc.vgn', 'blood.txt', 'blooprs1.asc', 'bnbeg2.4.txt', 'bnbguide.txt', 'bnb_quot.txt', 'boarchil.txt', 'boatmemo.jok', 'boe.hum', 'bond-2.txt', 'boneles2.txt', 'booknuti.txt', 'booze.fun', 'booze1.fun', 'booze2.fun', 'bored.txt', 'boston.geog', 'brdpudd.des', 'bread.rcp', 'bread.rec', 'bread.txt', 'breadpud.des', 'bredcake.des', 'brewing', 'browneco.hum', 'brownie.rec', 'brush1.txt', 'btaco.txt', 'btcisfre.hum', 'btscke01.des', 'btscke02.des', 'btscke03.des', 'btscke04.des', 'btscke05.des', 'buffwing.pol', 'bugbreak.hum', 'bugs.txt', 'buldrwho.txt', 'bunacald.fis', 'burrito.mea', 'butcher.txt', 'butstcod.fis', 'butwrong.hum', 'buzzword.hum', 'bw-phwan.hat', 'bw-summe.hat', 'byfb.txt', 'c0dez.txt', 'cabbage.txt', 'caesardr.sal', 'cake.rec', 'calamus.hrb', 'calculus.txt', 'calif.hum', 'calvin.txt', 'cancer.rat', 'candy.txt', 'candybar.fun', 'capital.txt', 'caramels.des', 'carowner.txt', 'cars.txt', 'cartoon.law', 'cartoon.laws', 'cartoon_.txt', 'cartoon_laws.txt', 'cartwb.son', 'cast.lis', 'catballs.hum', 'catin.hat', 'catranch.hum', 'catstory.txt', 'cbmatic.hum', 'cereal.txt', 'cform2.txt', 'cgs_lst.txt', 'chainltr.txt', 'change.hum', 'charity.hum', 'cheapfar.hum', 'cheapin.la', 'chickenheadbbs.txt', 'chickens.jok', 'chickens.txt', 'childhoo.jok', 'childrenbooks.txt', 'chili.txt', 'chinese.txt', 'chinesec.hum', 'choco-ch.ips', 'chung.iv', 'chunnel.txt', 'clancy.txt', 'classicm.hum', 'climbing.let', 'cmu.share', 'co-car.jok', 'cockney.alp', 'coffee.faq', 'coffee.txt', 'coffeebeerwomen.txt', 'cogdis.txt', 'coke.fun', 'coke.txt', 'coke1', 'cokeform.txt', 'coke_fan.naz', 'coladrik.fun', 'coladrik.txt', 'cold.fus', 'coldfake.hum', 'college.hum', 'college.txt', 'commutin.jok', 'commword.hum', 'computer.txt', 'comrevi1.hum', 'conan.txt', 'confucius_say.txt', 'consp.txt', 'contract.moo', 'cookberk', 'cookbkly.how', 'cookie.1', 'cooking.fun', 'cooking.jok', 'coollngo2.txt', 'cooplaws', 'cops.txt', 'corporat.txt', 'court.quips', 'coyote.txt', 'crazy.txt', 'critic.txt', 'crzycred.lst', 'cuchy.hum', 'cucumber.jok', 'cucumber.txt', 'cuisine.txt', 'cultmov.faq', 'curiousgeorgie.txt', 'curry.hrb', 'curry.txt', 'curse.txt', 'cybrtrsh.txt', 'd-ned.hum', 'dalive', 'damiana.hrb', 'dandwine.bev', 'dark.suc', 'dead-r', 'dead2.txt', 'dead3.txt', 'deadlysins.txt', 'deathhem.txt', 'defectiv.hum', 'desk.txt', 'deterior.hum', 'devils.jok', 'diesmurf.txt', 'diet.txt', 'dieter.txt', 'dingding.hum', 'dining.out', 'dirtword.txt', 'disaster.hum', 'disclmr.txt', 'disclym.txt', 'doc-says.txt', 'docdict.txt', 'docspeak.txt', 'doggun.sto', 'donut.txt', 'dover.poem', 'draxamus.txt', 'drinker.txt', 'drinkrul.jok', 'drinks.txt', 'drive.txt', 'dromes.txt', 'druggame.hum', 'drugshum.hum', 'drunk.txt', 'dubltalk.jok', 'eandb.drx', 'earp', 'eatme.txt', 'egg-bred.txt', 'egglentl.vgn', 'eggroll1.mea', 'electric.txt', 'element.jok', 'elephant.fun', 'elevator.fun', 'empeval.txt', 'engineer.hum', 'english', 'english.txt', 'engmuffn.txt', 'engrhyme.txt', 'enlightenment.txt', 'enquire.hum', 'epikarat.txt', 'epiquest.txt', 'episimp2.txt', 'epitaph', 'epi_.txt', 'epi_bnb.txt', 'epi_merm.txt', 'epi_tton.txt', 'eskimo.nel', 'exam.50', 'excuse.txt', 'excuse30.txt', 'excuses.txt', 'exidy.txt', 'facedeth.txt', 'failure.txt', 'fajitas.rcp', 'farsi.phrase', 'farsi.txt', 'fartinfo.txt', 'fartting.txt', 'fascist.txt', 'fbipizza.txt', 'fearcola.hum', 'fed.txt', 'fegg!int.txt', 'feggaqui.txt', 'feggmagi.txt', 'feista01.dip', 'female.jok', 'fiber.txt', 'filmgoof.txt', 'films_gl.txt', 'final-ex.txt', 'finalexm.hum', 'firecamp.txt', 'firstaid.inf', 'firstaid.txt', 'fish.rec', 'flattax.hum', 'flowchrt', 'flowchrt.txt', 'flux_fix.txt', 'focaccia.brd', 'food', 'foodtips', 'footfun.hum', 'forsooth.hum', 'free-cof.fee', 'freshman.hum', 'freudonseuss.txt', 'frogeye1.sal', 'from.hum', 'fuck!.txt', 'fudge.txt', 'fusion.gal', 'fusion.sup', 'fwksfun.hum', 'f_tang.txt', 'gack!.txt', 'gaiahuma', 'gameshow.txt', 'ganamembers.txt', 'garlpast.vgn', 'gas.txt', 'gd_alf.txt', 'gd_drwho.txt', 'gd_flybd.txt', 'gd_frasr.txt', 'gd_gal.txt', 'gd_hhead.txt', 'gd_liqtv.txt', 'gd_maxhd.txt', 'gd_ol.txt', 'gd_ql.txt', 'gd_sgrnd.txt', 'gd_tznew.txt', 'german.aut', 'ghostfun.hum', 'gingbeer.txt', 'girlspeak.txt', 'godmonth.txt', 'gohome.hum', 'goldwatr.txt', 'golnar.txt', 'good.txt', 'gotukola.hrb', 'gown.txt', 'grail.txt', 'grammar.jok', 'greenchi.txt', 'grospoem.txt', 'growth.txt', 'gumbo.txt', 'hack', 'hack7.txt', 'hackingcracking.txt', 'hackmorality.txt', 'hacktest.txt', 'hamburge.nam', 'hammock.hum', 'hangover.txt', 'harmful.hum', 'hate.hum', 'hbo_spec.rev', 'headlnrs', 'hecomes.jok', 'hedgehog.txt', 'height.txt', 'hell.jok', 'hell.txt', 'herb!.hum', 'hermsys.txt', 'heroic.txt', 'hi.tec', 'hierarch.txt', 'highland.epi', 'hilbilly.wri', 'history2.oop', 'hitchcoc.app', 'hitchcok.txt', 'hitler.59', 'hitler.txt', 'hitlerap.txt', 'homebrew.txt', 'hoonsrc.txt', 'hoosier.txt', 'hop.faq', 'horoscop.jok', 'horoscop.txt', 'horoscope.txt', 'hotel.txt', 'hotnnot.hum', 'hotpeper.txt', 'how.bugs.breakd', 'how2bgod.txt', 'how2dotv.txt', 'howlong.hum', 'how_to_i.pro', 'hstlrtxt.txt', 'htswfren.txt', 'hum2', 'humatra.txt', 'humatran.jok', 'humor9.txt', 'humpty.dumpty', 'iced.tea', 'icm.hum', 'idaho.txt', 'idr2.txt', 'imbecile.txt', 'impurmat.hum', 'incarhel.hum', 'indgrdn.txt', 'initials.rid', 'inlaws1.txt', 'inquirer.txt', 'ins1', 'insanity.hum', 'insect1.txt', 'insult', 'insure.hum', 'interv.hum', 'investi.hum', 'iqtest', 'iremember', 'is_story.txt', 'italoink.txt', 'ivan.hum', 'jac&tuu.hum', 'jalapast.dip', 'jambalay.pol', 'japantv.txt', 'japice.bev', 'jargon.phd', 'jason.fun', 'jawgumbo.fis', 'jawsalad.fis', 'jayjay.txt', 'jc-elvis.inf', 'jeffie.heh', 'jimhood.txt', 'johann', 'jokeju07.txt', 'jokes', 'jokes.txt', 'jokes1.txt', 'jon.txt', 'jrrt.riddle', 'jungjuic.bev', 'just2', 'justify', 'kaboom.hum', 'kanalx.txt', 'kashrut.txt', 'kid2', 'kid_diet.txt', 'killer.hum', 'killself.hum', 'kilroy', 'kilsmur.hum', 'kloo.txt', 'labels.txt', 'lampoon.jok', 'languag.jok', 'lansing.txt', 'law.sch', 'lawhunt.txt', 'laws.txt', 'lawskool.txt', 'lawsuniv.hum', 'lawyer.jok', 'lawyers.txt', 'lazarus.txt', 'la_times.hun', 'leech.txt', 'legal.hum', 'let.go', 'letgosh.txt', 'letter.txt', 'letterbx.txt', 'letter_f.sch', 'libraway.txt', 'liceprof.sty', 'lifeimag.hum', 'lifeinfo.hum', 'limerick.jok', 'lines.jok', 'lion.jok', 'lion.txt', 'lions.cat', 'livnware.hum', 'llamas.txt', 'lll.hum', 'lobquad.hum', 'losers84.hum', 'losers86.hum', 'lost.txt', 'lotsa.jok', 'lozers', 'lozerzon.hum', 'lozeuser.hum', 'lp-assoc.txt', 'lucky.cha', 'ludeinfo.hum', 'ludeinfo.txt', 'luggage.hum', 'luzerzo2.hum', 'm0dzmen.hum', 'madhattr.jok', 'maecenas.hum', 'mailfrag.hum', 'makebeer.hum', 'making_y.wel', 'malechem.txt', 'manager.txt', 'manilla.hum', 'manners.txt', 'manspace.hum', 'margos.txt', 'marines.hum', 'mash.hum', 'math.1', 'math.2', 'math.far', 'maxheadr', 'mcd.txt', 'meat2.txt', 'meinkamp.hum', 'mel.txt', 'memo.hum', 'memory.hum', 'men&wome.txt', 'mensroom.jok', 'merry.txt', 'miamadvi.hum', 'miami.hum', 'miamimth.txt', 'middle.age', 'minn.txt', 'miranda.hum', 'misc.1', 'misery.hum', 'missdish', 'missheav.hum', 'mitch.txt', 'mlverb.hum', 'modemwld.txt', 'modest.hum', 'modstup', 'mog-history', 'montoys.txt', 'montpyth.hum', 'moonshin', 'moore.txt', 'moslem.txt', 'mothers.txt', 'motrbike.jok', 'mov_rail.txt', 'mowers.txt', 'mr.rogers', 'mrsfield', 'msfields.txt', 'mtv.asc', 'mundane.v2', 'murph.jok', 'murphy.txt', 'mutate.hum', 'mydaywss.hum', 'myheart.hum', 'naivewiz.hum', 'namaste.txt', 'nameisreo.txt', 'namm', 'nasaglenn.txt', 'necropls.txt', 'netmask.txt', 'netnews.10', 'newcoke.txt', 'newconst.hum', 'newmex.hum', 'news.hum', 'nigel.1', 'nigel.10', 'nigel.2', 'nigel.3', 'nigel.4', 'nigel.5', 'nigel.6', 'nigel.7', 'nigel10.txt', 'nintendo.jok', 'normal.boy', 'normalboy.txt', 'nosuch_nasfic', 'nuke.hum', 'nukeplay.hum', 'nukewar.jok', 'nukwaste', 'number', 'number.killer', 'number_k.ill', 'nurds.hum', 'nysucks.hum', 'nzdrinks.txt', 'o-ttalk.hum', 'oakwood.txt', 'oam-001.txt', 'oam.nfo', 'oasis', 'oatbran.rec', 'oculis.rcp', 'odd_to.obs', 'odearakk.hum', 'office.txt', 'ohandre.hum', 'oilgluts.hum', 'old.txt', 'oldeng.hum', 'oldtime.sng', 'oldtime.txt', 'oliver.txt', 'oliver02.txt', 'onan.txt', 'one.par', 'onetoone.hum', 'ookpik.hum', 'opinion.hum', 'oracle.jok', 'oranchic.pol', 'orgfrost.bev', 'ourfathr.txt', 'outawork.erl', 'outlimit.txt', 'oxymoron.jok', 'oxymoron.txt', 'ozarks.hum', 'p-law.hum', 'packard.txt', 'paddingurpapers.txt', 'parabl.hum', 'parades.hum', 'parsnip.txt', 'passage.hum', 'passenge.sim', 'pasta001.sal', 'pat.txt', 'pbcookie.des', 'peanuts.txt', 'pecker.txt', 'penisprt.txt', 'penndtch', 'pepper.txt', 'pepsideg.txt', 'petshop', 'phony.hum', 'phorse.hum', 'phunatdi.ana', 'phxbbs-m.txt', 'pickup.lin', 'pickup.txt', 'pipespec.txt', 'pizzawho.hum', 'planeget.hum', 'pol-corr.txt', 'polemom.txt', 'poli.tics', 'policpig.hum', 'poli_t.ics', 'poll2res.hum', 'polly.txt', 'polly_.new', 'poopie.txt', 'popconc.hum', 'popmach', 'popmusi.hum', 'post.nuc', 'pot.txt', 'potty.txt', 'pournell.spo', 'pracjoke.txt', 'prawblim.hum', 'prayer.hum', 'primes.jok', 'princess.brd', 'pro-fact.hum', 'problem.txt', 'progrs.gph', 'proof.met', 'prooftec.txt', 'proposal.jok', 'proudlyserve.txt', 'prover.wisom', 'prover_w.iso', 'psalm.reagan', 'psalm23.txt', 'psalm_nixon', 'psalm_re.aga', 'psilaine.hum', 'pun.txt', 'pure.mat', 'puzzle.spo', 'puzzles.jok', 'python_s.ong', 'q.pun', 'qttofu.vgn', 'quantity.001', 'quantum.jok', 'quantum.phy', 'quest.hum', 'quick.jok', 'quotes.bug', 'quotes.jok', 'rabbit.txt', 'racist.net', 'radiolaf.hum', 'rapmastr.hum', 'ratings.hum', 'ratspit.hum', 'raven.hum', 'readme.bat', 'reagan.hum', 'realest.txt', 'reasons.txt', 'rec.por', 'recepies.fun', 'recip1.txt', 'recipe.001', 'recipe.002', 'recipe.003', 'recipe.004', 'recipe.005', 'recipe.006', 'recipe.007', 'recipe.008', 'recipe.009', 'recipe.010', 'recipe.011', 'recipe.012', 'reconcil.hum', 'record_.gap', 'red-neck.jks', 'reddwarf.sng', 'reddye.hum', 'rednecks.txt', 'reeves.txt', 'relative.ada', 'religion.txt', 'renored.txt', 'renorthr.txt', 'rent-a_cat', 'repair.hum', 'report.hum', 'research.hum', 'residncy.jok', 'resolutn.txt', 'resrch_p.hra', 'resrch_phrase', 'revolt.dj', 'richbred.txt', 'rinaldo.jok', 'rinaldos.law', 'rinaldos.txt', 'roach.asc', 'roadpizz.txt', 'robot.tes', 'rocking.hum', 'rockmus.hum', 'sanshop.txt', 'saveface.hum', 'sawyer.txt', 'scam.txt', 'seafood.txt', 'seeds42.txt', 'sf-zine.pub', 'sfmovie.txt', 'shameonu.hum', 'shooters.txt', 'shorties.jok', 'shrink.news', 'shuimai.txt', 'shuttleb.hum', 'silverclaws.txt', 'simp.txt', 'sinksub.txt', 'skincat', 'slogans.txt', 'smackjok.hum', 'smartass.txt', 'smiley.txt', 'smokers.txt', 'smurf-03.txt', 'smurfkil.hum', 'smurfs.cc', 'smurf_co.txt', 'snapple.rum', 'snipe.txt', 'soccer.txt', 'socecon.hum', 'social.hum', 'socks.drx', 'solders.hum', 'solviets.hum', 'some_hu.mor', 'soporifi.abs', 'sorority.gir', 'spacever.hum', 'spelin_r.ifo', 'spider.hum', 'spoonlis.txt', 'spydust.hum', 'squids.gph', 'staff.txt', 'stagline.txt', 'standard.hum', 'startrek.txt', 'stereo.txt', 'steroid.txt', 'strattma.txt', 'stressman.txt', 'strsdiet.txt', 'studentb.txt', 'stuf10.txt', 'stuf11.txt', 'st_silic.txt', 'subb_lis.txt', 'suicide2.txt', 'sungenu.hum', 'supermar.rul', 'swearfrn.hum', 'sw_err.txt', 'symbol.hum', 'sysadmin.txt', 'sysman.txt', 't-10.hum', 't-shirt.hum', 'takenote.jok', 'talkbizr.txt', 'taping.hum', 'tarot.txt', 'teens.txt', 'teevee.hum', 'telecom.q', 'televisi.hum', 'televisi.txt', 'temphell.jok', 'terbear.txt', 'termpoem.txt', 'terms.hum', \"terrmcd'.hum\", 'terrnieg.hum', 'test.hum', 'test.jok', 'test2.jok', 'testchri.txt', 'texbeef.txt', 'textgrap.hum', 'tfepisod.hum', 'tfpoems.hum', 'thermite.ana', 'thesis.beh', 'the_ant.txt', 'the_math.hel', 'three.txt', 'throwawa.hum', 'tickmoon.hum', 'timetr.hum', 'tnd.1', 'top10.elf', 'top10.txt', 'top10st1.txt', 'topten.hum', 'toxcwast.hum', 'tpquote2.txt', 'tpquotes.txt', 'transp.txt', 'trekfume.txt', 'trekwes.hum', 'tribble.hum', 'truthlsd.hum', 'truths.hum', 'tshirts.jok', 'tuflife.txt', 'tuna.lab', 'turbo.hum', 'turing.shr', 'turkey.fun', 'twilight.txt', 'twinkie.txt', 'twinkies.jok', 'twinpeak.txt', 'ukunderg.txt', 'units.mea', 'univ.odd', 'unochili.txt', 'vaguemag.90s', 'valujet.txt', 'variety1.asc', 'various.txt', 'vegkill.txt', 'venganza.txt', 'venison.txt', 'voltron.hum', 'vonthomp', 'wagit.txt', 'wagon.hum', 'washroom.txt', 'watchlip.hum', 'wedding.hum', 'weight.txt', 'weights.hum', 'welfare', 'welfare.txt', 'wetdream.hum', 'whatbbs', 'whatthe.hum', 'whitbred.txt', 'who.txt', 'whoon1st.hum', 'whoops.hum', 'why-me.hum', 'widows', 'wisconsi.txt', 'wisdom', 'wkrp.epi', 'women.jok', 'wonton.txt', 'wood', 'woodbine.txt', 'woodbugs.txt', 'woods.txt', 'woodsmok.txt', 'woolly_m.amm', 'word.hum', 'worldend.hum', 'wrdnws1.txt', 'wrdnws2.txt', 'wrdnws3.txt', 'wrdnws4.txt', 'wrdnws5.txt', 'wrdnws6.txt', 'wrdnws7.txt', 'wrdnws8.txt', 'wrdnws9.txt', 'x-drinks.txt', 'xibovac.txt', 'xtermin8.hum', 'y.txt', 'yjohncse.hum', 'yogisays.txt', 'yogurt.asc', 'yuban.txt', 'yuppies.hum', 'zgtoilet.txt', 'zodiac.hum', 'zucantom.sal', 'zuccmush.sal']\n"
     ]
    }
   ],
   "source": [
    "input_query_sequence = input(\"Enter the space-separated query terms : \")\n",
    "input_operator_sequence = input(\"Enter the comma-separated operator sequence enclosed in '[]' : \")\n",
    "print(f\"Input query sequence : {input_query_sequence}\")\n",
    "print(f\"Input operator sequence : {input_operator_sequence}\")\n",
    "input_operators = process_ops(input_operator_sequence[1:len(input_operator_sequence) - 1])\n",
    "query_tokens = preprocess_query(input_query_sequence)\n",
    "print(f\"Input operator (array) : {input_operators}\")\n",
    "print(f\"Query tokens : {query_tokens}\\n\")\n",
    "query_result_ndocs, query_result_total_cmps, query_result_doc_names = process_query(inverted_index_postings, query_tokens, input_operators)\n",
    "\n",
    "if(query_result_ndocs != -1):\n",
    "    query_generated = query_tokens[0] + \" \" + input_operators[0] + \" \" + query_tokens[1] + \" \"\n",
    "    ptr_qtoks = 2\n",
    "    for i in range(1, len(input_operators)):\n",
    "        query_generated += input_operators[i] + \" \"\n",
    "        query_generated += query_tokens[ptr_qtoks] + \" \"\n",
    "        ptr_qtoks += 1\n",
    "    print(f\"Query : {query_generated}\\n\\n\")\n",
    "    print(f\"\\nNo. of documents retreived: {query_result_ndocs}\\nNumber of comparisons: {query_result_total_cmps}\\nNames of retreived documents: {query_result_doc_names}\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2370e07001db70a9c24f7e21173c51fbc4321340913a02830aed4885459fa0a0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
