{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# from substitutions import appos\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import string\n",
    "import numpy as np\n",
    "import contractions\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../Humor,Hist,Media,Food'\n",
    "file_names = os.listdir(data_dir)\n",
    "file_paths = [(data_dir + '/' + fname) for fname in file_names]\n",
    "docID_to_doc = {}\n",
    "for i in range(len(file_names)):\n",
    "    docID_to_doc[i] = file_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are are not no do not he is going to.\n"
     ]
    }
   ],
   "source": [
    "x = contractions.fix(\"We're ain't no don't he's going to.\")\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = set(stopwords.words('english'))\n",
    "# stemmer = PorterStemmer()\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "def read_files(fpaths):\n",
    "    file_tokens = []\n",
    "    for fpath in fpaths:\n",
    "        f = open(fpath, 'r', encoding='utf-8', errors='replace')\n",
    "        ftxt_unprocessed = f.read()\n",
    "        # print(ftxt_unprocessed)\n",
    "        ftoks = preprocess_file(ftxt_unprocessed)\n",
    "        file_tokens.append(ftoks)\n",
    "    return file_tokens\n",
    "\n",
    "def isValidTok(tok):\n",
    "    if((tok not in string.punctuation) and (tok.isnumeric() == False) and (sum([0 if ch in string.punctuation else 1 for ch in tok]) >= 1)):\n",
    "        return True\n",
    "    return False \n",
    "def check_alnum(tok):\n",
    "    tok = ''.join(x for x in tok if x.isalnum() == True)\n",
    "    return tok\n",
    "def remove_punct(tok):\n",
    "    punctuations = string.punctuation\n",
    "    tok = ''.join(x for x in tok if x not in punctuations)\n",
    "    return tok\n",
    "\n",
    "def remove_space(tok):\n",
    "    tok = ''.join(x for x in tok if x != ' ')\n",
    "    return tok\n",
    "\n",
    "def preprocess_file(file_text):\n",
    "    '''\n",
    "        Preprocess the file text\n",
    "    '''\n",
    "    #converting text to lowercase\n",
    "    text = file_text.lower()\n",
    "    \n",
    "    # print(f\"Original tokens : \\n{word_tokenize(text)}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "\n",
    "    #Fixing the contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    #omitting all non-alphanumeric characters\n",
    "    # text = ''.join([x if (x.isalnum() == True or x == \" \") else \" \" for x in text ])\n",
    "\n",
    "    #word tokenization\n",
    "    all_toks = word_tokenize(text)\n",
    "    all_unique_toks = set(all_toks)\n",
    "\n",
    "    all_unique_toks = [check_alnum(x) for x in all_unique_toks]\n",
    "\n",
    "    # print(f\"All unique tokens :\\n{all_unique_toks}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    #removing stopwords\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    file_toks = [tok for tok in all_unique_toks if tok not in stop_words]\n",
    "    # print(f\"After removing stopwords :\\n{file_toks}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    #removing punctations if any remain\n",
    "    toks_no_punct = []\n",
    "    for tok in file_toks:\n",
    "        ctok = remove_punct(tok)\n",
    "        # print(f\"-> before removal : {tok} || After removal : {ctok}\")\n",
    "        if(ctok != \"\"):\n",
    "            toks_no_punct.append(ctok)\n",
    "    # print(f\"After removing puncts :\\n{toks_no_punct}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "\n",
    "    #removing spaces in any remain & check if it is a number\n",
    "    cleaned_toks = []\n",
    "    for tok in toks_no_punct:\n",
    "        ctok = remove_space(tok)\n",
    "        # print(f\"-> before removal : {tok} || After removal : {ctok}\")\n",
    "        if(ctok != \"\" and ctok.isnumeric() == False  and len(ctok) > 1):\n",
    "            cleaned_toks.append(ctok)\n",
    "    # print(f\"After removing spaces and removing numbers :\\n{cleaned_toks}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    #stemming tokens\n",
    "    # stemmer = PorterStemmer()\n",
    "    # stemmed_toks = set()\n",
    "\n",
    "    # for tok in cleaned_toks:\n",
    "    #     stok = stemmer.stem(tok)\n",
    "    #     if(stok != \"\" and len(stok) > 1 and (stok not in stop_words)): #consider non-blank tokens and the ones which have length greater than 1\n",
    "    #         stemmed_toks.add(stok)\n",
    "    # print(f\"After stemming and removing single letters :\\n{stemmed_toks}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    final_tokens = [tok for tok in cleaned_toks]\n",
    "\n",
    "    return final_tokens\n",
    "\n",
    "# def preprocess_file(file_text):\n",
    "#     '''\n",
    "#         Preprocess the file text\n",
    "#     '''\n",
    "    \n",
    "#     #converting to lower case\n",
    "#     text = file_text.lower()\n",
    "\n",
    "#     #fix the contractions\n",
    "#     text = contractions.fix(text)\n",
    "\n",
    "\n",
    "\n",
    "#     #word tokenization\n",
    "#     all_tokens = word_tokenize(text)\n",
    "#     # print(f\"All toks {all_tokens}\")\n",
    "#     all_unique_tokens = set(all_tokens)\n",
    "\n",
    "#     # print(all_unique_tokens)\n",
    "\n",
    "#     lemmatized_toks = set()\n",
    "#     for unq_tok in all_unique_tokens:\n",
    "#         stemmed_tok = lemmatizer.lemmatize(unq_tok)\n",
    "#         lemmatized_toks.add(stemmed_tok)\n",
    "#     # print(lemmatized_toks)\n",
    "    \n",
    "#     tokens = list(lemmatized_toks - stop_words)\n",
    "    \n",
    "\n",
    "#     # ps = PorterStemmer()\n",
    "#     valid_toks = []\n",
    "#     for tok in tokens:\n",
    "#         if(isValidTok(tok) == True):\n",
    "#             valid_toks.append(tok)\n",
    "#     return valid_toks\n",
    "#     # print(final_tokens)\n",
    "\n",
    "def preprocess_query(query_text):\n",
    "\n",
    "    text = query_text.lower()\n",
    "    \n",
    "    # print(f\"Original Query Tokens : \\n{word_tokenize(text)}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    \n",
    "    #Fixing the contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    #omitting all non-alphanumeric characters\n",
    "    # text = ''.join([x if (x.isalnum() == True or x == \" \") else \" \" for x in text ])\n",
    "\n",
    "    #word tokenization\n",
    "    all_toks = word_tokenize(text)\n",
    "    all_unique_toks = []\n",
    "    for tok in all_toks:\n",
    "        if(tok not in all_unique_toks):\n",
    "            all_unique_toks.append(tok)\n",
    "\n",
    "    all_unique_toks = [check_alnum(x) for x in all_unique_toks]\n",
    "\n",
    "    # print(f\"All unique tokens :\\n{all_unique_toks}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    #removing stopwords\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    file_toks = [tok for tok in all_unique_toks if tok not in stop_words]\n",
    "    # print(f\"After removing stopwords :\\n{file_toks}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    #removing punctations if any remain\n",
    "    toks_no_punct = []\n",
    "    for tok in file_toks:\n",
    "        ctok = remove_punct(tok)\n",
    "        # print(f\"-> before removal : {tok} || After removal : {ctok}\")\n",
    "        if(ctok != \"\"):\n",
    "            toks_no_punct.append(ctok)\n",
    "    # print(f\"After removing puncts :\\n{toks_no_punct}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "\n",
    "    #removing spaces in any remain & check if it is a number\n",
    "    cleaned_toks = []\n",
    "    for tok in toks_no_punct:\n",
    "        ctok = remove_space(tok)\n",
    "        # print(f\"-> before removal : {tok} || After removal : {ctok}\")\n",
    "        if(ctok != \"\" and ctok.isnumeric() == False and len(ctok) > 1):\n",
    "            cleaned_toks.append(ctok)\n",
    "    # print(f\"After removing spaces and removing numbers :\\n{cleaned_toks}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    #stemming tokens\n",
    "    # stemmer = PorterStemmer()\n",
    "    # stemmed_toks = set()\n",
    "\n",
    "    # for tok in cleaned_toks:\n",
    "    #     stok = stemmer.stem(tok)\n",
    "    #     if(stok != \"\" and len(stok) > 1 and (stok not in stop_words)): #consider non-blank tokens and the ones which have length greater than 1\n",
    "    #         stemmed_toks.add(stok)\n",
    "    # print(f\"After stemming and removing single letters :\\n{stemmed_toks}\\n\")\n",
    "    # print(\"\\n-------------------------\\n\")\n",
    "    final_tokens = [tok for tok in cleaned_toks]\n",
    "\n",
    "    return final_tokens\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_toks = read_files(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index(file_toks):\n",
    "    inv_index_postings = {}\n",
    "    inv_index_frequency = {}\n",
    "    for i in range(len(file_toks)):\n",
    "        for tok in file_toks[i]:\n",
    "            if(tok not in inv_index_postings.keys()):\n",
    "                inv_index_postings[tok] = [i]\n",
    "                inv_index_frequency[tok] = 1\n",
    "            else:\n",
    "                inv_index_postings[tok].append(i)\n",
    "                inv_index_frequency[tok] += 1\n",
    "    inv_index_postings = dict(sorted(inv_index_postings.items()))\n",
    "    terms_list = inv_index_postings.keys()\n",
    "    for word in terms_list:\n",
    "        inv_index_postings[word].sort()\n",
    "    return inv_index_postings, inv_index_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocsFromID(file_names, doc_IDs):\n",
    "    doc_names = []\n",
    "    for doc_ID in doc_IDs:\n",
    "        doc_names.append(file_names[doc_ID])\n",
    "    return doc_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_equal_arrays(arr1, arr2):\n",
    "\n",
    "    if(len(arr1) != len(arr2)):\n",
    "        return False\n",
    "    \n",
    "    arr1 = sorted(arr1)\n",
    "    arr2 = sorted(arr2)\n",
    "    for i in range(len(arr1)):\n",
    "        if(arr1[i] != arr2[i]):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive_postings(inv_index_postings, term):\n",
    "    terms_list = inv_index_postings.keys()\n",
    "    if(term not in terms_list):\n",
    "        return []\n",
    "    else:\n",
    "        posting = inv_index_postings[term]\n",
    "        return posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive_frequency(inv_index_frequency, term):\n",
    "    terms_list = inv_index_frequency.keys()\n",
    "    if(term not in terms_list):\n",
    "        return 0\n",
    "    else:\n",
    "        freq = inv_index_frequency[term]\n",
    "        return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive_term_info(inv_index_postings, inv_index_frequency, term):\n",
    "    terms_list = inv_index_postings.keys()\n",
    "    if(term not in terms_list):\n",
    "        return [], 0\n",
    "    else:\n",
    "        posting = inv_index_postings[term]\n",
    "        freq = inv_index_frequency[term]\n",
    "        return posting, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_AND(posting1, posting2, verbose=False):\n",
    "\n",
    "\n",
    "    if(posting1 == [] or posting2 == []):\n",
    "        return 0, 0, []\n",
    "\n",
    "    ptr1 = 0\n",
    "    ptr2 = 0\n",
    "    answer_docID = []\n",
    "\n",
    "    num_comparisons = 0\n",
    "\n",
    "    while(ptr1 < len(posting1) and ptr2 < len(posting2)):\n",
    "        num_comparisons += 1\n",
    "        # print(f\"1 : {posting1[ptr1]} , 2: {posting2[ptr2]}\")\n",
    "\n",
    "        if(posting1[ptr1] == posting2[ptr2]):\n",
    "            answer_docID.append(posting1[ptr1])\n",
    "            ptr1 += 1\n",
    "            ptr2 += 1\n",
    "        elif(posting1[ptr1] < posting2[ptr2]):\n",
    "            ptr1 += 1\n",
    "        else:\n",
    "            ptr2 += 1\n",
    "\n",
    "    num_docs_retreived = len(answer_docID)\n",
    "    doc_names_retreived = getDocsFromID(file_names, answer_docID)\n",
    "\n",
    "    if(verbose==True):\n",
    "        print(f\"AND Query: \\nNo. of documents retreived: {num_docs_retreived}\\nMinimum number of comparisons: {num_comparisons}\\nNames of retreived documents: {doc_names_retreived}\")\n",
    "    \n",
    "    # temp_verification = list(set(posting1) & set(posting2))\n",
    "    # print(f\"Verification: {check_equal_arrays(temp_verification, answer_docID)}\")\n",
    "\n",
    "    return num_docs_retreived, num_comparisons, doc_names_retreived\n",
    "\n",
    "def query_OR(posting1, posting2, verbose=False):\n",
    "    \n",
    "\n",
    "    if((posting1 == []) and (posting2 == [])):\n",
    "        return 0, 0, []\n",
    "    elif((posting1 == []) and (posting2 != [])):\n",
    "        ans_docs = posting2\n",
    "        return len(ans_docs), 0, ans_docs\n",
    "    elif((posting1 != []) and (posting2 == [])):\n",
    "        ans_docs = posting1\n",
    "        return len(ans_docs), 0, ans_docs\n",
    "    else:\n",
    "        ptr1 = 0\n",
    "        ptr2 = 0\n",
    "        answer_docID = []\n",
    "\n",
    "        num_comparisons = 0\n",
    "\n",
    "        while(ptr1 < len(posting1) and ptr2 < len(posting2)):\n",
    "            num_comparisons += 1\n",
    "            # print(f\"1 : {posting1[ptr1]} , 2: {posting2[ptr2]}\")\n",
    "\n",
    "            if(posting1[ptr1] == posting2[ptr2]):\n",
    "                answer_docID.append(posting1[ptr1])\n",
    "                ptr1 += 1\n",
    "                ptr2 += 1\n",
    "            elif(posting1[ptr1] < posting2[ptr2]):\n",
    "                answer_docID.append(posting1[ptr1])\n",
    "                ptr1 += 1\n",
    "            else:\n",
    "                answer_docID.append(posting2[ptr2])\n",
    "                ptr2 += 1\n",
    "        while(ptr1 < len(posting1)):\n",
    "            answer_docID.append(posting1[ptr1])\n",
    "            ptr1 += 1\n",
    "        while(ptr2 < len(posting2)):\n",
    "            answer_docID.append(posting2[ptr2])\n",
    "            ptr2 += 1\n",
    "\n",
    "\n",
    "        num_docs_retreived = len(answer_docID)\n",
    "        doc_names_retreived = getDocsFromID(file_names, answer_docID)\n",
    "\n",
    "        if(verbose==True):\n",
    "            print(f\"Query OR\\nNo. of documents retreived: {num_docs_retreived}\\nMinimum number of comparisons: {num_comparisons}\\nNames of retreived documents: {doc_names_retreived}\")\n",
    "        \n",
    "        # temp_verification = list(set(posting1) | set(posting2))\n",
    "        # print(f\"Verification: {check_equal_arrays(temp_verification, answer_docID)}\")\n",
    "        return num_docs_retreived, num_comparisons, doc_names_retreived\n",
    "\n",
    "def perform_NOT(posting):\n",
    "\n",
    "    all_docIDs = [docID for docID in range(len(file_names))]\n",
    "    if(posting == []):\n",
    "        return all_docIDs\n",
    "    for docID in posting:\n",
    "        all_docIDs.remove(docID)\n",
    "\n",
    "    return all_docIDs\n",
    "\n",
    "def query_AND_NOT(posting1, posting2, verbose=False):\n",
    "\n",
    "    if((posting1 == [])):\n",
    "        return 0, 0, []\n",
    "    \n",
    "    posting2_NOT = perform_NOT(posting2)\n",
    "    ptr1 = 0\n",
    "    ptr2 = 0\n",
    "    answer_docID = []\n",
    "\n",
    "    num_comparisons = 0\n",
    "\n",
    "    while(ptr1 < len(posting1) and ptr2 < len(posting2_NOT)):\n",
    "        num_comparisons += 1\n",
    "        # print(f\"1 : {posting1[ptr1]} , 2: {posting2[ptr2]}\")\n",
    "        if(posting1[ptr1] == posting2_NOT[ptr2]):\n",
    "            answer_docID.append(posting1[ptr1])\n",
    "            ptr1 += 1\n",
    "            ptr2 += 1\n",
    "        elif(posting1[ptr1] < posting2_NOT[ptr2]):\n",
    "            ptr1 += 1\n",
    "        else:\n",
    "            ptr2 += 1\n",
    "\n",
    "    num_docs_retreived = len(answer_docID)\n",
    "    doc_names_retreived = getDocsFromID(file_names, answer_docID)\n",
    "\n",
    "    if(verbose==True):\n",
    "        print(f\"Query AND NOT\\nNo. of documents retreived: {num_docs_retreived}\\nMinimum number of comparisons: {num_comparisons}\\nNames of retreived documents: {doc_names_retreived}\")\n",
    "    \n",
    "    # temp_verification = list(set(posting1) | set(posting2))\n",
    "    # print(f\"Verification: {check_equal_arrays(temp_verification, answer_docID)}\")\n",
    "    return num_docs_retreived, num_comparisons, doc_names_retreived\n",
    "\n",
    "def query_OR_NOT(posting1, posting2, verbose=False):\n",
    "    \n",
    "    if((posting1 == []) and (posting2 == [])):\n",
    "        ans_docs = perform_NOT(posting2)\n",
    "        return len(ans_docs), 0, ans_docs\n",
    "    elif((posting1 == []) and (posting2 != [])):\n",
    "        ans_docs = perform_NOT(posting2)\n",
    "        return len(ans_docs), 0, ans_docs\n",
    "    elif((posting1 != []) and (posting2 == [])):\n",
    "        ans_docs = posting1\n",
    "        return len(ans_docs), 0, ans_docs\n",
    "    else:\n",
    "        \n",
    "        posting2_NOT = perform_NOT(posting2)\n",
    "        ptr1 = 0\n",
    "        ptr2 = 0\n",
    "        answer_docID = []\n",
    "\n",
    "        num_comparisons = 0\n",
    "\n",
    "        while(ptr1 < len(posting1) and ptr2 < len(posting2_NOT)):\n",
    "            num_comparisons += 1\n",
    "            # print(f\"1 : {posting1[ptr1]} , 2: {posting2[ptr2]}\")\n",
    "\n",
    "            if(posting1[ptr1] == posting2_NOT[ptr2]):\n",
    "                answer_docID.append(posting1[ptr1])\n",
    "                ptr1 += 1\n",
    "                ptr2 += 1\n",
    "            elif(posting1[ptr1] < posting2_NOT[ptr2]):\n",
    "                answer_docID.append(posting1[ptr1])\n",
    "                ptr1 += 1\n",
    "            else:\n",
    "                answer_docID.append(posting2_NOT[ptr2])\n",
    "                ptr2 += 1\n",
    "        while(ptr1 < len(posting1)):\n",
    "            answer_docID.append(posting1[ptr1])\n",
    "            ptr1 += 1\n",
    "        while(ptr2 < len(posting2_NOT)):\n",
    "            answer_docID.append(posting2_NOT[ptr2])\n",
    "            ptr2 += 1\n",
    "\n",
    "\n",
    "        num_docs_retreived = len(answer_docID)\n",
    "        doc_names_retreived = getDocsFromID(file_names, answer_docID)\n",
    "\n",
    "        if(verbose==True):\n",
    "            print(f\"Query OR NOT\\nNo. of documents retreived: {num_docs_retreived}\\nMinimum number of comparisons: {num_comparisons}\\nNames of retreived documents: {doc_names_retreived}\")\n",
    "        \n",
    "        return num_docs_retreived, num_comparisons, doc_names_retreived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index_postings, inverted_index_frequency = create_inverted_index(file_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a-team']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getDocsFromID(file_names, [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query AND NOT\n",
      "No. of documents retreived: 16\n",
      "Minimum number of comparisons: 848\n",
      "Names of retreived documents: ['aids.txt', 'bbc_vide.cat', 'drunk.txt', 'english', 'final-ex.txt', 'finalexm.hum', 'gd_tznew.txt', 'hoosier.txt', 'insanity.hum', 'lawskool.txt', 'lawyer.jok', 'quotes.jok', 'smackjok.hum', 'smartass.txt', 'wagon.hum', 'woodbugs.txt']\n"
     ]
    }
   ],
   "source": [
    "postings1 = retreive_postings(inverted_index_postings, 'piano')\n",
    "postings2 = retreive_postings(inverted_index_postings, 'hand')\n",
    "num_docs_AND, min_cmps_AND, doc_names_AND = query_AND_NOT(postings1, postings2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_query(posting1, posting2, op):\n",
    "    if(op == \"AND\"):\n",
    "        return query_AND(posting1, posting2)\n",
    "    elif(op == \"OR\"):\n",
    "        return query_OR(posting1, posting2)\n",
    "    elif(op == \"AND NOT\"):\n",
    "        return query_AND_NOT(posting1, posting2)\n",
    "    elif(op == \"OR NOT\"):\n",
    "        return query_OR_NOT(posting1, posting2)\n",
    "    else:\n",
    "        print(\"INVALID OPERATOR\")\n",
    "        return []\n",
    "\n",
    "def process_query(query_text, ops):\n",
    "    query_tokens = preprocess_query(query_text)\n",
    "    print(f\"Query tokens : {query_tokens}\\n\")\n",
    "    print(f\"Operators : \")\n",
    "    n_qtoks = len(query_tokens)\n",
    "    n_ops = n_qtoks - 1\n",
    "    # query_operato = copy.deepcopy(ops)\n",
    "    if(len(ops) > n_ops):\n",
    "        print(f\"Insufficient tokens in the query\\n\")\n",
    "        return []\n",
    "    elif(len(ops) < n_ops):\n",
    "        print(f\"Insufficient operators\")\n",
    "        return []\n",
    "    else:\n",
    "        if(n_ops == 1):\n",
    "            p1 = retreive_postings(query_tokens[0])\n",
    "            p2 = retreive_postings(query_tokens[1])\n",
    "            return perform_query(p1, p2, ops[0])\n",
    "        else:\n",
    "            p1 = retreive_postings(query_tokens[0])\n",
    "            p2 = retreive_postings(query_tokens[1])\n",
    "            ans = perform_query(p1, p2, ops[0])\n",
    "            qtok_ptr = 2\n",
    "            for i in range(1, len(ops)):\n",
    "                posting_tok = retreive_postings(query_tokens[qtok_ptr])\n",
    "                ans = perform_query(ans, posting_tok, ops[i])\n",
    "                qtok_ptr += 1\n",
    "            return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2370e07001db70a9c24f7e21173c51fbc4321340913a02830aed4885459fa0a0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
