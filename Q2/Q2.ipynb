{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sarthakj01/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/sarthakj01/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/sarthakj01/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/sarthakj01/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# from substitutions import appos\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import copy\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import string\n",
    "import numpy as np\n",
    "import contractions\n",
    "from nltk import RegexpTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../Humor,Hist,Media,Food'\n",
    "file_names = os.listdir(data_dir)\n",
    "file_paths = [(data_dir + '/' + fname) for fname in file_names]\n",
    "docID_to_doc = {}\n",
    "for i in range(len(file_names)):\n",
    "    docID_to_doc[i] = file_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(tok):\n",
    "    punctuations = string.punctuation\n",
    "    tok = ''.join(x for x in tok if x not in punctuations)\n",
    "    return tok\n",
    "def remove_space(tok):\n",
    "    tok = ''.join(x for x in tok if x != ' ')\n",
    "    return tok\n",
    "\n",
    "def preprocess_file(file_text):\n",
    "\n",
    "    ftext = file_text.lower()\n",
    "    # print(f\"Original Text : \\n{ftext}\\n\\n\")\n",
    "    # print(\"\\n---------------------------------------\\n\")\n",
    "    file_toks = word_tokenize(ftext)\n",
    "    # print(f\"After word_Tokenization : \\n{file_toks}\\n\\n\")\n",
    "    # print(\"---------------------------------------\")\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    file_toks = [tok for tok in file_toks if tok not in stop_words]\n",
    "    # print(f\"After removing stopwords : \\n{file_toks}\\n\\n\")\n",
    "    # print(\"\\n---------------------------------------\\n\")\n",
    "    toks_no_punct = []\n",
    "    for tok in file_toks:\n",
    "        ctok = remove_punct(tok)\n",
    "        # print(f\"-> before removal : {tok} || After removal : {ctok}\")\n",
    "        if(ctok != \"\"):\n",
    "            toks_no_punct.append(ctok)\n",
    "    # print(\"\\n---------------------------------------\\n\")\n",
    "    cleaned_toks = []\n",
    "    for tok in toks_no_punct:\n",
    "        ctok = remove_space(tok)\n",
    "        # print(f\"-> before removal : {tok} || After removal : {ctok}\")\n",
    "        if(ctok != \"\"):\n",
    "            cleaned_toks.append(ctok)\n",
    "    return cleaned_toks\n",
    "\n",
    "def cleanQuery(file_text):\n",
    "\n",
    "    ftext = file_text.lower()\n",
    "    # print(f\"Original Text : \\n{ftext}\\n\\n\")\n",
    "    # print(\"\\n---------------------------------------\\n\")\n",
    "    file_toks = word_tokenize(ftext)\n",
    "    # print(f\"After word_Tokenization : \\n{file_toks}\\n\\n\")\n",
    "    # print(\"---------------------------------------\")\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    file_toks = [tok for tok in file_toks if tok not in stop_words]\n",
    "    # print(f\"After removing stopwords : \\n{file_toks}\\n\\n\")\n",
    "    # print(\"\\n---------------------------------------\\n\")\n",
    "    toks_no_punct = []\n",
    "    for tok in file_toks:\n",
    "        ctok = remove_punct(tok)\n",
    "        # print(f\"-> before removal : {tok} || After removal : {ctok}\")\n",
    "        if(ctok != \"\"):\n",
    "            toks_no_punct.append(ctok)\n",
    "    # print(\"\\n---------------------------------------\\n\")\n",
    "    cleaned_toks = []\n",
    "    for tok in toks_no_punct:\n",
    "        ctok = remove_space(tok)\n",
    "        # print(f\"-> before removal : {tok} || After removal : {ctok}\")\n",
    "        if(ctok != \"\"):\n",
    "            cleaned_toks.append(ctok)\n",
    "    return cleaned_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(fpaths):\n",
    "    \n",
    "    file_tokens = []\n",
    "    for fpath in fpaths:\n",
    "        # print(fpath)\n",
    "        f = open(fpath, 'r', encoding='utf-8', errors='ignore')\n",
    "        ftxt_unprocessed = f.read()\n",
    "        ftoks = preprocess_file(ftxt_unprocessed)\n",
    "        file_tokens.append(ftoks)\n",
    "    return file_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocsFromID(file_names, doc_IDs):\n",
    "    doc_names = []\n",
    "    for doc_ID in doc_IDs:\n",
    "        doc_names.append(file_names[doc_ID])\n",
    "    return doc_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_toks = read_file(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_positional_index(file_toks):\n",
    "    pidx_dict = {}\n",
    "    for i in range(len(file_toks)):\n",
    "        for j in range(len(file_toks[i])):\n",
    "            tok = file_toks[i][j]\n",
    "            if(tok not in pidx_dict.keys()):\n",
    "                pidx_dict[tok] = {}\n",
    "                pidx_dict[tok][i] = [j]\n",
    "            else:\n",
    "                if(i in pidx_dict[tok].keys()):\n",
    "                    pidx_dict[tok][i].append(j)\n",
    "                else:\n",
    "                    pidx_dict[tok][i] = [j]\n",
    "    pidx_dict = dict(sorted(pidx_dict.items()))\n",
    "    positional_index = {}\n",
    "    terms = pidx_dict.keys()\n",
    "    for t in terms:\n",
    "        positional_index[t] = []\n",
    "        for docID in pidx_dict[t].keys():\n",
    "            term_doc_positions = copy.deepcopy(pidx_dict[t][docID])\n",
    "            term_doc_positions.sort()\n",
    "            positional_index[t].append([docID, term_doc_positions])        \n",
    "        positional_index[t].sort(key=lambda x: x[0])\n",
    "    return positional_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_index = create_positional_index(file_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_all_in_range(ptrs, ranges):\n",
    "    for i in range(len(ptrs)):\n",
    "        if(ptrs[i] >= ranges[i]):\n",
    "            return False\n",
    "    return True\n",
    "# def process_phrase_query(positional_index, query_toks):\n",
    "\n",
    "#     n_toks = len(query_toks)\n",
    "#     outer_pointers = [0] * n_toks\n",
    "#     outer_ranges = [len(positional_index[query_toks[i]]) for i in range(n_toks)]\n",
    "#     while(check_all_in_range(outer_pointers, outer_ranges)):\n",
    "#         all_equal = True\n",
    "#         for i in range(n_toks - 1):\n",
    "#             if(positional_index[query_toks[i]][outer_pointers[i]][0] != positional_index[query_toks[i + 1]][outer_pointers[i + 1]][0]):\n",
    "#                 all_equal = False\n",
    "#                 break\n",
    "#         if(all_equal == True):\n",
    "#             inner_pointers = [0] * n_toks\n",
    "#             inner_ranges = [len(positional_index[query_toks[i]][outer_pointers[i]][1]) for i in range(n_toks)]\n",
    "#             while(check_all_in_range(inner_pointers, inner_ranges)):\n",
    "#                 for i in range(n_toks - 1):\n",
    "                    \n",
    "def process_phrase_query(positional_index, query_text):\n",
    "\n",
    "    query_toks = cleanQuery(query_text)\n",
    "    n_toks = len(query_toks)\n",
    "    # print(query_toks)\n",
    "    outer_pointers = [0] * n_toks\n",
    "    # print(outer_pointers)\n",
    "    outer_ranges = [len(positional_index[query_toks[i]]) for i in range(n_toks)]\n",
    "    answer = []\n",
    "    while(check_all_in_range(outer_pointers, outer_ranges)):\n",
    "        all_equal = True\n",
    "        for i in range(n_toks - 1):\n",
    "            if(positional_index[query_toks[i]][outer_pointers[i]][0] != positional_index[query_toks[i + 1]][outer_pointers[i + 1]][0]):\n",
    "                all_equal = False\n",
    "                break\n",
    "        if(all_equal == False):\n",
    "            least_ptr = 0\n",
    "            for i in range(1, n_toks):\n",
    "                if(positional_index[query_toks[least_ptr]][outer_pointers[least_ptr]][0] > positional_index[query_toks[i]][outer_pointers[i]][0]):\n",
    "                    least_ptr = i\n",
    "            outer_pointers[least_ptr] += 1\n",
    "        if(all_equal == True):\n",
    "\n",
    "            doc_ID = positional_index[query_toks[0]][outer_pointers[0]][0]\n",
    "            # print(f\"Checking for docid : {doc_ID}\")\n",
    "            posting_lists = []\n",
    "            for i in range(n_toks):\n",
    "                posting_lists.append(positional_index[query_toks[i]][outer_pointers[i]][1])\n",
    "            # print(posting_lists)\n",
    "            j = 1\n",
    "            inner_pointers = [0] * n_toks\n",
    "            inner_ranges = [len(posting_lists[i]) for i in range(n_toks)]\n",
    "            # print(f\"Initial Inner pointers : {inner_pointers}\")\n",
    "            # print(f\"Initial Inner ranges : {inner_ranges}\\n-------------------\\n\")\n",
    "            flag = True\n",
    "            while(flag == True and check_all_in_range(inner_pointers, inner_ranges)):\n",
    "                # print(f\"Current inner pointers : {inner_pointers}\")\n",
    "                cntr = 0\n",
    "                for i in range(1, n_toks):\n",
    "                    if(posting_lists[i][inner_pointers[i]] == posting_lists[i - 1][inner_pointers[i - 1]] + 1):\n",
    "                        cntr += 1\n",
    "                \n",
    "                \n",
    "                if(cntr == n_toks - 1):\n",
    "                    # print(f\"--M--\\nFound a match using innerPointers : {inner_pointers}\\n--MM--\")\n",
    "                    flag = False\n",
    "                    answer.append(doc_ID)\n",
    "                    break\n",
    "                else:\n",
    "                    while(inner_pointers[j] < inner_ranges[j] and (posting_lists[j][inner_pointers[j]] <= posting_lists[j - 1][inner_pointers[j - 1]])):\n",
    "                        print(j)\n",
    "                        inner_pointers[j] += 1\n",
    "                    if(inner_pointers[j] < inner_ranges[j]):\n",
    "                        \n",
    "                        if(posting_lists[j][inner_pointers[j]] == posting_lists[j - 1][inner_pointers[j - 1]] + 1):\n",
    "                            j += 1\n",
    "                        else:\n",
    "                            j = 1\n",
    "                            inner_pointers[0] += 1\n",
    "            # print(\"\\n-------------------\\n\")\n",
    "            # print(answer)\n",
    "\n",
    "            # doc = positional_index[query_toks[0]][outer_pointers[0]][0]\n",
    "            # j = 1\n",
    "            # flag = True\n",
    "            # inner_pointers = [0] * n_toks\n",
    "            # inner_ranges = [len(positional_index[query_toks[i]][outer_pointers[i]][1]) for i in range(n_toks)]\n",
    "            # print(inner_ranges)\n",
    "            # # print(inner_ranges)\n",
    "            # while(flag == True and check_all_in_range(inner_pointers, inner_ranges)):\n",
    "            #     print(inner_pointers)\n",
    "                \n",
    "            #     cntr = 0\n",
    "            #     for i in range(1, n_toks):\n",
    "            #         # print(positional_index[query_toks[i]][outer_pointers[i]][1][inner_pointers[i]])\n",
    "            #         if(positional_index[query_toks[i]][outer_pointers[i]][1][inner_pointers[i]] == positional_index[query_toks[i - 1]][outer_pointers[i - 1]][1][inner_pointers[i - 1]] + 1):\n",
    "            #             cntr += 1\n",
    "            #     if(cntr == n_toks - 1):\n",
    "            #         flag = False\n",
    "            #         answer.append(positional_index[query_toks[0]][outer_pointers[0]][0])\n",
    "            #         break\n",
    "     \n",
    "            #     while((positional_index[query_toks[j]][outer_pointers[j]][1][inner_pointers[j]] <= positional_index[query_toks[j - 1]][outer_pointers[j - 1]][1][inner_pointers[j - 1]]) and inner_pointers[j] < inner_ranges[j]):\n",
    "            #         inner_pointers[j] += 1\n",
    "            #     if(inner_pointers[j] < inner_ranges[j]):\n",
    "            #         if(positional_index[query_toks[j]][outer_pointers[j]][1][inner_pointers[j]] == positional_index[query_toks[j - 1]][outer_pointers[j - 1]][1][inner_pointers[j - 1]] + 1):\n",
    "            #             j += 1\n",
    "            #         else:\n",
    "            #             j = 1\n",
    "            #             inner_pointers[0] += 1\n",
    "                \n",
    "            for i in range(n_toks):\n",
    "                outer_pointers[i] += 1\n",
    "    print(answer)\n",
    "    doc_names = getDocsFromID(file_names, answer)\n",
    "    print(doc_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[21, [85]], [595, [973]], [623, [201]], [942, [168, 174, 4295]]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_index['constitutional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[21, [86]], [579, [922]]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_index['impediment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "[122, 334, 763]\n",
      "['abbott.txt', 'wrdnws2.txt', 'quotes.txt']\n"
     ]
    }
   ],
   "source": [
    "# human mechanism so completely between\n",
    "x = process_phrase_query(positional_index, \"human mechanism so completely between\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print('between' in stopwords.words('english'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2370e07001db70a9c24f7e21173c51fbc4321340913a02830aed4885459fa0a0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
