{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Samyak\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import copy\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../Humor,Hist,Media,Food'\n",
    "file_names = os.listdir(data_dir) #reading the data directory to list all the files\n",
    "file_paths = [(data_dir + '/' + fname) for fname in file_names] #forming file paths\n",
    "docID_to_doc_mapping = {} #forming docID to doc name mapping\n",
    "for i in range(len(file_names)):\n",
    "    docID_to_doc_mapping[i] = file_names[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Part (for file texts and query text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(tok):\n",
    "    '''\n",
    "        Removing punctations from tokens\n",
    "    '''\n",
    "    punctuations = string.punctuation\n",
    "    tok = ''.join(x for x in tok if x not in punctuations)\n",
    "    return tok\n",
    "def remove_space(tok):\n",
    "    '''\n",
    "        Removing blank space toks\n",
    "    '''\n",
    "    tok = ''.join(x for x in tok if x != ' ')\n",
    "    return tok\n",
    "\n",
    "def preprocess_file(file_text):\n",
    "    '''\n",
    "        This function preprocesses the file text.\n",
    "        Input: file_text in string form represting the text of a file\n",
    "        Returns: cleaned_toks, word tokens present in the file after preprocessing\n",
    "    '''\n",
    "\n",
    "    #converting the text to lowercase\n",
    "    ftext = file_text.lower()\n",
    "\n",
    "    #performing word tokenization\n",
    "    file_toks = word_tokenize(ftext)\n",
    "\n",
    "    #removing the stopwords from tokens\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    file_toks = [tok for tok in file_toks if tok not in stop_words]\n",
    "\n",
    "    #removing punctuation marks from tokens\n",
    "    toks_no_punct = []\n",
    "    for tok in file_toks:\n",
    "        ctok = remove_punct(tok)\n",
    "        if(ctok != \"\"):\n",
    "            toks_no_punct.append(ctok)\n",
    "    \n",
    "    #Removing blank space tokens\n",
    "    cleaned_toks = []\n",
    "    for tok in toks_no_punct:\n",
    "        ctok = remove_space(tok)\n",
    "        if(ctok != \"\"):\n",
    "            cleaned_toks.append(ctok)\n",
    "\n",
    "    return cleaned_toks\n",
    "\n",
    "def cleanQuery(query_text):\n",
    "    '''\n",
    "        Preprocessing the query text\n",
    "        Input: query_text, string of the phrase query text\n",
    "        Returns: cleaned_toks, an array containg the preprocessed query tokens\n",
    "    '''\n",
    "\n",
    "    #We perform the same preprocessing steps on the query as we did for the file text\n",
    "\n",
    "    #converting the text to lowercase\n",
    "    qtext = query_text.lower()\n",
    "    \n",
    "    #performing word tokenization\n",
    "    query_toks = word_tokenize(qtext)\n",
    "    \n",
    "    #removing the stopwords from tokens\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    query_toks = [tok for tok in query_toks if tok not in stop_words]\n",
    "    \n",
    "    #removing punctuation marks from tokens\n",
    "    toks_no_punct = []\n",
    "    for tok in query_toks:\n",
    "        ctok = remove_punct(tok)\n",
    "        if(ctok != \"\"):\n",
    "            toks_no_punct.append(ctok)\n",
    "    \n",
    "    #Removing blank space tokens\n",
    "    cleaned_toks = []\n",
    "    for tok in toks_no_punct:\n",
    "        ctok = remove_space(tok)\n",
    "        if(ctok != \"\"):\n",
    "            cleaned_toks.append(ctok)\n",
    "    \n",
    "    return cleaned_toks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the files and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(fpaths):\n",
    "    '''\n",
    "        Reads the files and preprocess every file's text to form word tokens for every file.\n",
    "        Returns a 2-D list containing word tokens for every file\n",
    "    '''\n",
    "    file_tokens = []\n",
    "    for fpath in fpaths:\n",
    "        f = open(fpath, 'r', encoding='utf-8', errors='ignore') #open the file\n",
    "        ftxt_unprocessed = f.read() #read the text of the file\n",
    "        ftoks = preprocess_file(ftxt_unprocessed) #preprocessing the text to form word tokens\n",
    "        file_tokens.append(ftoks)\n",
    "    return file_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocsFromID(docID_to_doc, doc_IDs):\n",
    "    '''\n",
    "        Given a list of document IDs, it outputs the document names corresponding to thos IDs.\n",
    "        Input: docID_to_docs (mapping between docID -> doc_name), docIDs - list of input document IDs\n",
    "        Returns: doc_names - list of doc_names corresponding to document IDs in doc_IDs\n",
    "    '''\n",
    "    doc_names = []\n",
    "    for doc_ID in doc_IDs:\n",
    "        doc_names.append(docID_to_doc[doc_ID])\n",
    "    return doc_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_toks = read_file(file_paths) #extracting the tokens from each and every file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Positional Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_positional_index(file_toks):\n",
    "    '''\n",
    "        This function build the positional index. It takes in the word tokens of each file as input and returns two dictionaries: \n",
    "        positional_index - corresponding to the posting and positions details for all the terms, pidx_freq - corresponding to the document frequency \n",
    "        for each term (no. of documents containing the term).\n",
    "    '''\n",
    "\n",
    "    pidx_dict = {} #this is dictionary to store the postings and positions details for the terms\n",
    "    pidx_freq = {} #this is dictionary to store the frequency values for the terms\n",
    "\n",
    "    #Iterate over all the files\n",
    "    for i in range(len(file_toks)):\n",
    "        #For each file, iterate over all the file tokens\n",
    "        for j in range(len(file_toks[i])):\n",
    "            tok = file_toks[i][j] #the j_th token in the i_th file\n",
    "            if(tok not in pidx_dict.keys()): #if the token is not yet present as a term in the index\n",
    "                pidx_dict[tok] = {} #create a new entry for the term and a dictionary for the term to store the document_ID and positions\n",
    "                pidx_dict[tok][i] = [j] #in the term's dictionary add the position 'j' in the positions list of the i_th document\n",
    "            else: # else if the token is already present as a term in the index\n",
    "                if(i in pidx_dict[tok].keys()): #if the i_th document is already present in the term's corresponding dictionary\n",
    "                    pidx_dict[tok][i].append(j) #then append the j_th position to the i_th document's positions list\n",
    "                else:\n",
    "                    pidx_dict[tok][i] = [j] # add the i_th document to the term's dictionary and initialize the new positions list for the document with the position j\n",
    "    \n",
    "    pidx_dict = dict(sorted(pidx_dict.items())) #alphabetically sort the index wrt the terms\n",
    "    \n",
    "    #Converting the intenal dictionary (the dictionary corresponding to each term which has document IDs and positions list) to a nested list.\n",
    "    positional_index = {}\n",
    "    terms = pidx_dict.keys()\n",
    "    for t in terms:\n",
    "        positional_index[t] = []\n",
    "        for docID in pidx_dict[t].keys():\n",
    "            term_doc_positions = copy.deepcopy(pidx_dict[t][docID])\n",
    "            term_doc_positions.sort() #sorting the positions corresponding to each document_ID included for each term\n",
    "            positional_index[t].append([docID, term_doc_positions])        \n",
    "        positional_index[t].sort(key=lambda x: x[0]) #sort a term's corresponding list wrt to document IDs.\n",
    "\n",
    "    #building the term frequency dictionary\n",
    "    for t in positional_index.keys():\n",
    "        pidx_freq[t] = len(positional_index[t])\n",
    "\n",
    "    return positional_index, pidx_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_postings(pidx, term):\n",
    "    '''\n",
    "        Given a term, retreive its posting list.\n",
    "        Input: pidx - positional index, term\n",
    "        Returns: [] if term not in index, posting list for the term otherwise\n",
    "    '''\n",
    "    all_terms = pidx.keys()\n",
    "    if(term not in all_terms):\n",
    "        return []\n",
    "    else:\n",
    "        return pidx[term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequency(pidx_freq, term):\n",
    "    '''\n",
    "        Given a term, retreive its frequency value.\n",
    "        Input: pidx_freq - positional index frequency array, term\n",
    "        Returns: 0 if term not in index, frequency value for the term otherwise\n",
    "    '''\n",
    "    all_terms = pidx_freq.keys()\n",
    "    if(term not in all_terms):\n",
    "        return 0\n",
    "    else:\n",
    "        return pidx_freq[term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_index_info(pidx, pidx_freq, term):\n",
    "    '''\n",
    "        Given a term, retreive both its posting list & frequency value.\n",
    "        Input: pidx - positional index, pidx_freq - positional index frequency array, term\n",
    "        Returns: [], 0 if term not in index; posting, frequency value for the term otherwise\n",
    "    '''\n",
    "    if(term not in pidx.keys()):\n",
    "        print(\"Term not found in index\")\n",
    "        return [], 0\n",
    "    else:\n",
    "        return pidx[term], pidx_freq[term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_index, idx_frequency_array = create_positional_index(file_toks) #creating the positional index\n",
    "#positional_index stores the information related to posting lists i.e- docID and position of term in that document.\n",
    "#idx_frequency_array stores information related to frequency of terms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phrase Query Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_all_in_range(ptrs, ranges):\n",
    "    '''\n",
    "        Helper function for checking if all the pointers are in respective ranges\n",
    "    '''\n",
    "    for i in range(len(ptrs)):\n",
    "        if(ptrs[i] >= ranges[i]):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def process_phrase_query(positional_index, query_toks):\n",
    "    '''\n",
    "        This function processes the phrase query. It takes in the positional_index and query_toks(tokens) as the input \n",
    "        and determines if all tokens are present in the terms list, then performs phrase query search otherwise returns 0, [].\n",
    "        Returns: num_docs - number of documents retreived, doc_names - names of retreived documents\n",
    "    '''\n",
    "    terms = positional_index.keys()\n",
    "    for tok in query_toks:\n",
    "        if(tok not in terms): #query token is not in the index terms\n",
    "            print(\"A query term is not present in index.\")\n",
    "            return 0, []\n",
    "    if(len(query_toks) == 1):\n",
    "        tok_postings = positional_index[query_toks[0]]\n",
    "        num_docs = len(tok_postings)\n",
    "        doc_IDs = []\n",
    "        for i in range(num_docs):\n",
    "            doc_IDs.append(tok_postings[i][0])\n",
    "        doc_names = getDocsFromID(docID_to_doc_mapping, doc_IDs)\n",
    "        return num_docs, doc_names\n",
    "    num_docs, doc_names = perform_phrase_query(positional_index, query_toks) #If all query tokens present in index terms, perform phrase query searching\n",
    "    return num_docs, doc_names\n",
    "\n",
    "def perform_phrase_query(positional_index, query_toks):\n",
    "    '''\n",
    "        This function performs phrase query searching and has the algorithm for it. Given the positional_index and query_toks (query tokens)\n",
    "        as input, the function returns number of documents retreived and their names for a given phrase query search.\n",
    "    '''\n",
    "    n_toks = len(query_toks) #number of tokens in the phrase query\n",
    "    outer_pointers = [0] * n_toks #constructing n_toks number of outer pointers using which we iterate over the document IDs.\n",
    "    outer_ranges = [len(positional_index[query_toks[i]]) for i in range(n_toks)] #ranges for the outer pointers for each query token term\n",
    "    answer = [] #array that holds our final answer\n",
    "    while(check_all_in_range(outer_pointers, outer_ranges)): #Check condition for while loop ensures all outer_pointers in range\n",
    "\n",
    "        all_equal = True\n",
    "        #This loop checks if all the outer pointers point to the same document ID. If not so, set flag all_equal to False and break out of the loop.\n",
    "        for i in range(n_toks - 1):\n",
    "            if(positional_index[query_toks[i]][outer_pointers[i]][0] != positional_index[query_toks[i + 1]][outer_pointers[i + 1]][0]):\n",
    "                all_equal = False\n",
    "                break\n",
    "        \n",
    "        if(all_equal == False): #If all outer pointers do not point to same document ID\n",
    "            #Through this loop we find the outer pointer that points to the least magnitude document ID and finally after the loop increment it by one.\n",
    "            least_ptr = 0\n",
    "            for i in range(1, n_toks):\n",
    "                if(positional_index[query_toks[least_ptr]][outer_pointers[least_ptr]][0] > positional_index[query_toks[i]][outer_pointers[i]][0]):\n",
    "                    least_ptr = i\n",
    "            outer_pointers[least_ptr] += 1 #increment that outer pointer which points to the lowest magnitude document ID by one.\n",
    "        \n",
    "        if(all_equal == True): #If all outer pointers point to same document ID\n",
    "            doc_ID = positional_index[query_toks[0]][outer_pointers[0]][0] #the value of document ID which all outer pointer point to\n",
    "            posting_positions_lists = [] #this will be a two dimensional list which contains the term positions list for each query token for the common document ID to which all outer pointers point\n",
    "            for i in range(n_toks):\n",
    "                posting_positions_lists.append(positional_index[query_toks[i]][outer_pointers[i]][1]) #appending the term positions list for the i_th query term/token corresponding common document_ID pointed to by the outer pointer\n",
    "            \n",
    "            j = 1 \n",
    "            inner_pointers = [0] * n_toks #constructing n_toks number of inner pointers using which we iterate over the positions list for each query token for the corresponding common document_ID pointed to by the outer pointer.\n",
    "            inner_ranges = [len(posting_positions_lists[i]) for i in range(n_toks)] #ranges for the outer pointers for each query token term\n",
    "            flag = True\n",
    "            while(flag == True and check_all_in_range(inner_pointers, inner_ranges)): #Check condition for while loop ensures all inner_pointers in range\n",
    "                \n",
    "                #This loop is used to check if in the positions array the values pointed by the inner pointers are consecutive starting from the position value (value pointed by inner_pointer) of the 0_th index query token (i.e first query token).\n",
    "                cntr = 0\n",
    "                for i in range(1, n_toks):\n",
    "                    if(posting_positions_lists[i][inner_pointers[i]] == posting_positions_lists[i - 1][inner_pointers[i - 1]] + 1): #IF the position value pointed by consecutive inner pointer is consecutive, increment cntr by 1\n",
    "                        cntr += 1\n",
    "                \n",
    "                if(cntr == n_toks - 1): #if all the values are consecutive i.e - the positions array the values pointed by the inner pointers are consecutive starting from the position value (value pointed by inner_pointer) of the 0_th index query token (i.e first query token).\n",
    "                    #This means the current document ID is a valid answer\n",
    "                    flag = False\n",
    "                    answer.append(doc_ID) #append the current document ID to our answer and break\n",
    "                    break\n",
    "                else:\n",
    "                    #While the position value pointed by the j_th inner pointer is less or equal to that pointed by (j-1)_th inner pointer, keep incrementing the j_th inner pointer (because we want that the position value of j_th pointer should be equal to value of (j-1)th plus one)\n",
    "                    while(inner_pointers[j] < inner_ranges[j] and (posting_positions_lists[j][inner_pointers[j]] <= posting_positions_lists[j - 1][inner_pointers[j - 1]])):\n",
    "                        inner_pointers[j] += 1\n",
    "                    #If after all the incrementing, the j_th pointer is in range then compare the value of j_th inner pointer with (j-1)_th\n",
    "                    if(inner_pointers[j] < inner_ranges[j]):\n",
    "                        \n",
    "                        if(posting_positions_lists[j][inner_pointers[j]] == posting_positions_lists[j - 1][inner_pointers[j - 1]] + 1):\n",
    "                            #If the position value pointed by j_th inner pointer is equal to (j-1)_th's value plus one, then these two tokens(j_th and (j-1)_th) are present at valid & consecutive positions, thus we increment j by one and move on to the next query token\n",
    "                            j += 1\n",
    "                        else:\n",
    "                            #If the  position value pointed by j_th inner pointer is not equal to (j-1)_th's value plus one, means they are not currently consecutive terms thus we discard this current sequence of positions starting from the position value pointed by the 0_th inner pointer (first token's inner pointer).\n",
    "                            j = 1\n",
    "                            inner_pointers[0] += 1 # We will increment the 0_th inner pointer by one to again start with a new position sequence and check\n",
    "\n",
    "            #The case when the document IDs pointed by all the outer pointers are equal, then increment all the outer pointers by one.\n",
    "            for i in range(n_toks):\n",
    "                outer_pointers[i] += 1\n",
    "    \n",
    "    #Now we have our answer\n",
    "    num_docs = len(answer) #number of documents retreived\n",
    "    doc_names = getDocsFromID(docID_to_doc_mapping, answer) #names of documents retreived\n",
    "\n",
    "    return num_docs, doc_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input Phrase Query: different countries in europe\n",
      "Query tokens : ['different', 'countries', 'europe']\n",
      "\n",
      "Number of documents retreived : 1\n",
      "Names of documents retreived : ['aboutada.txt']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_input = input(\"Enter the phrase query : \")\n",
    "query_toks = cleanQuery(query_input)\n",
    "print(f\"\\nInput Phrase Query: {query_input}\")\n",
    "print(f\"Query tokens : {query_toks}\\n\")\n",
    "if(len(query_toks) > 5):\n",
    "    print(\"Please enter query within length limit.\")\n",
    "elif(len(query_toks) == 0):\n",
    "    print(\"No query tokens to search after preprocessing\")\n",
    "else:\n",
    "    num_docs_retreived, doc_names_retreived = process_phrase_query(positional_index, query_toks)\n",
    "    print(f\"Number of documents retreived : {num_docs_retreived}\")\n",
    "    print(f\"Names of documents retreived : {doc_names_retreived}\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2370e07001db70a9c24f7e21173c51fbc4321340913a02830aed4885459fa0a0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
